### 第3章 提示词工程入门
#### 3.1 简介
第2章构建了一个非对称语义搜索系统，该系统利用LLM的强大功能，并使用基于LLM的嵌入引擎，基于自然语言高效检索相关文档。由于在大量文本上对LLM进行了预训练，该系统能够理解查询语句背后的含义，并检索准确的结果。

然而，构建一个基于LLM的有效的检索应用，需要的不仅仅是接入一个预先训练好的模型，然后获得检索结果——如果用户想了解LLM并获得更好的用户体验呢？用户可能还想依靠LLM的学习来提高检索能力，并创建一个端到端的基于LLM的应用。这就是需要提示词工程发挥作用的地方了。

#### 3.2 提示词工程
提示词工程是制作输入给LLM的过程，旨在将任务有效地传达给LLM，使其返回准确且有用的输出，如图3.1所示。提示词工程涉及用户如何构建LLM的输入，以引导LLM获得所需的输出。这是一项需要理解语言的细微差别、所从事的具体领域，以及所用LLM的能力和局限性的技能。

![image](https://github.com/user-attachments/assets/ee495e9f-304d-45c7-a00b-7f10a047aa52)


在本章中，我们将深入探索提示词工程的精髓，探索制作有效提示的技术和最佳实践，以获得准确且相关的输出。涵盖的主题包括为不同类型的任务构建提示的技巧，针对特定领域微调模型的方法，以及评估LLM输出的质量的标准。在本章结束时，读者将具备创建基于LLM的强大应用程序所需的技能和知识，以及充分利用这些前沿模型的潜力。

##### 3.2.1 LLM的对齐
为了理解为什么提示词工程对LLM应用程序开发至关重要，首先要理解LLM是如何训练的，以及它们是如何与人类输入相匹配的。在语言模型中，对齐是指模型如何理解和响应输入的提示，这些输入的提示“符合”用户（至少“符合”负责对齐LLM的人）的期望。在标准语言建模过程中，模型被训练是根据前一个词的上下文预测下一个词或词序列。然而，这种方法本身不足以让模型回答特定的指令或提示，这可能会限制它在某些应用程序中的应用。

如果语言模型未能与输入提示对齐，则提示词工程将面临挑战，因为模型可能生成与预期不符、不相关或不正确的响应。然而一些先进的语言模型已经融入了额外的对齐特征，例如Anthropic的Constitutional AI驱动的强化学习，或OpenAI的GPT系列中的人类反馈强化学习，这些创新性的对齐技术可以将明确的指令和反馈纳入模型的训练过程，从而提高模型理解和响应特定提示的能力，使其在问答或语言翻译等应用中更加高效和实用。如图3.2所示，即使像GPT-3这样的现代LLM也需要对齐技术才能表现出用户想要的行为。2020年发布的原始GPT-3模型是一个纯粹的自回归语言模型；它试图完成语句的空白部分，并对自由发挥给出错误信息不作约束。2022年1月，GPT-3的第一个对齐版本发布（InstructGPT），这个版本能够以更简洁准确的方式回答问题，展现了对齐技术带来的显著改进。

![image](https://github.com/user-attachments/assets/c9c26658-f95f-4e24-a18e-df883a5fa713)


本章重点介绍的语言模型，不仅经过了自回归语言建模任务的训练，还经过了回答指导提示的对齐。这些模型的开发目标是提高它们理解和响应特定指令或任务的能力。这些模型包括GPT-3和ChatGPT（来自OpenAI的闭源模型）、FLAN-T5（来自谷歌的开源模型）和Cohere的命令系列，这些模型已经使用大量数据和技术（如迁移学习和微调）进行了训练，以更有效地生成对指令提示的响应。通过这一探索，我们将了解使用这些模型对应的自然语言模型产品和其特性的初始样貌，并更深入地了解如何充分利用语言模型对齐的全部功能。

##### 3.2.2 LLM提问
语言模型指令对齐的提示词工程设计的首要规则是对提问内容有清晰的表达。当用户给LLM一个任务时，要确保这个任务能够清晰地传达给LLM。这一点对于LLM能轻松完成的简单任务来说尤其重要。

在要求GPT-3纠正句子语法时，只需给出简单的指令“纠正这个句子的语法”就可以得到清晰准确的回复。提示词也应该清楚地指出要纠正的短语。

如图3.3所示，使用LLM回答人类问题的最佳方式是直接对LLM提问。

![image](https://github.com/user-attachments/assets/919cc3b1-ba8c-450f-ad06-4ab90565d202)


为了使LLM的回复更可信，我们可以通过添加前缀来更清晰地约束任务的输入和输出。下面看另一个简单的例子——让GPT-3将一个句子从英语翻译成土耳其语。

采用提示词对LLM提问由以下三个要素组成：
- 直接给LLM任务指令：“从英语翻译成土耳其语”属于提示词的第一部分，LLM可以分辨出这是指令，然后继续读入其他指令内容。
- 我们想要翻译的英语短语前面有“English:”作前缀，这样要翻译的语言类型变得更加明确。
- 为了约束LLM回答的结果，我们添加前缀“Turkish:”。

这三个要素都是提示词的组成部分，用来约束LLM回答内容的边界。如果给GPT-3清晰的提示，它将能够识别出所要求的任务并正确填写答案。如图3.4所示，采用提示词对大模型提问更详细版本有三个组成部分：一套清晰简洁的指令，输入前缀，即一个提升大模型理解力的标签，输出前缀后跟一个冒号，没有其他空格。

![image](https://github.com/user-attachments/assets/6d1a3c36-c3e3-4e4c-8a79-97229c26463d)


我们可以做进一步的拓展，如何让GPT-3纠正语法时输出多个选项，并将结果以编号列表的方式格式化输出。如图3.5所示，给出清晰直接的提示的关键一步是告诉LLM如何构建输出。在这个例子中，要求GPT-3以编号列表的形式给出语法正确的多个LLM回复版本。

说到提示词工程，有一条经验法则非常实用：有疑问时，只需直接对LLM提问。提供清晰直接的提示词对于从LLM获得最准确和最有用的输出至关重要。

##### 3.2.3 小样本学习
当需要更深入理解更复杂的任务时，给LLM提供几个相关的例子，可以在很大程度上帮助LLM产生准确且一致的输出。小样本学习是一种强大的技术，它包含为LLM提供任务的几个相关例子，以帮助LLM理解问题的背景和细微差别。

小样本学习一直是LLM领域的一个主要研究重点。GPT-3的创建者是认可小样本学习的潜力的，这可以从GPT-3论文的标题“语言模型是小样本学习者”中得到证明。

对于需要特定语调、语法或风格的任务，以及使用特定语言的领域，小样本学习尤为有用。图3.6展示了要求GPT-3将评论分类为主观或非主观的示例；这是一个二分类任务。如图3.6所示，图中上面的两个例子展示了LLM如何仅从几个例子中直观地获得任务的答案；下面两个示例显示了相同的提示结构，但没有任何示例（称为“零样本”），所以似乎无法回答用户希望它们如何操作。在图中可以看到，因为LLM可以回顾一些示例并进行直观判断，小样本示例更有可能产生预期的结果。

小样本学习为用户与机器学习算法的交互开辟了新的可能性。通过这种技术，用户可以在不提供明确指令的情况下，让机器学习算法理解用户的任务，使其更加直观和友好。这一突破性的能力为开发各种基于机器学习算法的应用，从聊天机器人到语言翻译工具，铺平了道路。

![image](https://github.com/user-attachments/assets/aeceeccb-4f69-4ba0-a68d-5b5c0bdd3965)


##### 3.2.4 结构化输出
LLM可以生成各种格式的文本，但有时这种变化太过多样，可能会给与其他系统的协同工作和集成带来挑战。以特定的方式构建输出结构，使其更容易与其他系统协同工作并集成到其他系统中，这可能是有帮助的。在前面我们让GPT-3以编号列表的形式给出答案时，就看到了这种结构。也可以让LLM以JSON（JavaScript对象符号）等结构化数据格式输出，如图3.7所示。图3.7简单地要求GPT-3以JSON格式返回一个响应，图中的上部分确实会生成一个有效的JSON，但其中的键也是土耳其语，这可能不是用户想要的。用户可以在指令中，通过提供一个一次性示例（图中的下部分），使LLM以用户要求的精确JSON格式输出翻译。

![image](https://github.com/user-attachments/assets/81f9794c-815d-4d78-90a2-ac25b645c5cd)


通过以结构化格式生成LLM输出，开发人员可以更便捷地提取特定信息，并将其传递给其他服务。此外，采用结构化格式有助于确保输出的一致性，降低在使用模型过程中出现错误或不一致的风险。

##### 3.2.5 人物角色提示词
提示中的特定单词选择会极大地影响模型的输出，也可能导致截然不同的结果。例如，添加或删除一个单词可能会导致LLM转移其注意力或改变其对任务的解释。在某些情况下，这可能会导致不正确或不相关的响应；在其他情况下，它可能会产生所需的准确输出。

为了解释这些变化，研究人员和从业者经常为LLM创建不同的“角色”，代表模型可以根据提示采用不同的风格或声音。这些角色可以基于特定主题、流派，甚至是虚构人物，旨在引发LLM的特殊类型的反应（图3.8）。通过利用角色，LLM开发人员可以更好地控制模型的输出，系统的最终用户可以获得更独特和量身定制的体验。

![image](https://github.com/user-attachments/assets/a9b2c3dd-f190-4e83-9f4f-c30927ef0458)


图3.8中，从左上角开始，可看到一个基线提示词，要求GPT-3作为商店服务员进行回应。用户可以通过要求它以“激动”的方式甚至是海盗的方式回应来注入更多个性。还可以通过要求LLM以粗鲁的方式来滥用这个系统。任何想要使用LLM的开发人员都应该意识到，无论是有意还是无意，这些类型的输出都是可能的。在第5章，我们将探索可以帮助减轻这种行为的高级输出验证技术。

人物角色可能并不总是用于积极的目的。就像任何工具或技术一样，有些人可能会滥用LLM，通过向LLM提供促进仇恨言论或其他有害内容的提示，个人可以生成延续有害思想并强化负面刻板印象的文本。LLM的开发者倾向于采取措施来减轻这种潜在的滥用，例如实施内容过滤器，并与人工审核员合作审查模型的输出。同时，使用LLM的个人在使用这些模型时也必须承担责任，并考虑自己的行为（或LLM代表其采取的行为）对他人可能产生的潜在影响。

#### 3.3 跨模型提示词工程
提示在很大程度上取决于语言模型的架构和训练，这意味着适用于一种模型的提示可能不适用于另一种模型。例如，ChatGPT、GPT-3（与ChatGPT不同）、T5和Cohere命令系列中的模型都具有不同的底层架构、预训练数据源和训练方法，这反过来又会影响使用它们时提示的有效性。虽然一些提示可以在模型之间转移，但其他提示可能需要对齐或重新设计，才能与特定模型配合使用。

本节将讲解如何跨模型使用提示，并深入考虑每个模型的独特功能和局限性。我们的目的是开发有效的提示词，这些提示词可以精准地指导语言模型生成所需的输出。

##### 3.3.1 ChatGPT
一些LLM可以接受不止一个“提示”。通过与对话过程（例如ChatGPT）对齐的模型可以接受系统的提示词和多个“用户”和“助手”的提示词。如图3.9所示，ChatGPT接受一个整体系统提示以及任何数量的用户和助手提示，以模拟正在进行的对话。系统提示词旨在作为对话的一般指令，通常包括要遵循的总体规则和角色。用户和助手的提示词则分别代表了用户和LLM之间的消息交互。对于自己选择查看的任何LLM，务必查看其文档，了解如何构建输入提示词的详细信息。

![image](https://github.com/user-attachments/assets/d3418dce-67fe-4c08-bdaa-c9604fedc71c)


##### 3.3.2 Cohere
我们已经在本章看到了Cohere的命令系列模型，作为OpenAI的替代方案，它们表明提示并不总是可以从一个模型简单地移植到另一个模型。相反，通常需要稍微改变提示，以便另一个LLM完成其工作。

假设我们要求OpenAI和Cohere将英语翻译成土耳其语（图3.10），OpenAI的GPT-3可以在没有太多指引的情况下接受翻译指令，而Cohere模型需要更多地构建提示词。这并不意味着Cohere比GPT-3差，它只是意味着用户需要考虑如何为给定的LLM构建独属于该LLM的提示词。

##### 3.3.3 开源提示词工程
讨论提示词工程时，如果不提及GPT-J和FLAN-T5等开源模型是不公平的。在使用它们时，提示词工程充分利用其预训练和微调的关键步骤（第4章会涉及这个主题）。这些模型可以像闭源模型一样生成高质量的文本输出。然而，与闭源模型不同，开源模型提供更大的灵活性，以及对提示词工程的控制，使开发人员能够在微调过程中根据特定用例定制提示和输出。

例如，从事医疗聊天机器人开发的人员可能希望创建侧重于医学术语和概念的提示，而从事语言翻译模型开发的人员可能希望创建强调语法和语义的提示。

![image](https://github.com/user-attachments/assets/af83dea6-3a45-4e0f-9627-3d916a9e9f8c)


通过开源模型，开发人员可以灵活地根据特定用例微调提示，从而产生更准确更相关的文本输出。

开源模型中提示词工程的另一个优势是能够与其他开发人员和研究人员合作。由于开源模型拥有庞大而活跃的用户和贡献者社区，这使得开发人员可以轻松分享他们的提示词工程策略，接收来自社区的反馈，并共同努力改进模型的整体性能。这种协作式的提示词工程方法可以在自然语言处理研究中带来更快的进展和更重大的突破。

记住开源模型是如何进行预训练和微调（如果有的话）是有好处的。例如，GPT-J是一种自回归语言模型，因此我们期望小样本提示等技术比直接询问指导性提示的效果更好。相比之下，FLAN-T5是专门针对指导性提示词进行微调的，因此虽然小样本学习仍然存在，但用户也可以依靠简单询问来实现。如图3.11所示，开源模型在训练方式和期望提示方面可能有很大差异。GPT-J没有指令对齐，很难回答直接指令（左下）。相比之下，FLAN-T5模型做过指令对齐，确实知道如何接受指令（右下）。这两个模型都能从小样本学习直观地进行学习，但FLAN-T5似乎在主观任务上遇到了困难。也许它是一个很好的待微调模型——很快会在后面的章节中介绍。 
