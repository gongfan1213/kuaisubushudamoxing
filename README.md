### 目录
**第1部分 大模型介绍**
- **第1章 大模型概述** —— 页码1
    - 1.1 什么是大模型 —— 页码2
        - 1.1.1 大模型的定义 —— 页码3
        - 1.1.2 大模型的关键特征 —— 页码5
        - 1.1.3 大模型是如何工作的 —— 页码9
    - 1.2 当前流行的大模型 —— 页码17
        - 1.2.1 BERT —— 页码17
        - 1.2.2 GPT-3和ChatGPT —— 页码17
        - 1.2.3 T5 —— 页码18
    - 1.3 垂直领域大模型 —— 页码19
    - 1.4 大模型的应用 —— 页码19
        - 1.4.1 经典的NLP任务 —— 页码20
        - 1.4.2 自由文本生成 —— 页码22
        - 1.4.3 信息检索/神经语意搜索 —— 页码23
        - 1.4.4 聊天机器人 —— 页码24
    - 1.5 本章小结 —— 页码25

- **第2章 大模型语义检索** —— 页码26
    - 2.1 简介 —— 页码26
    - 2.2 语义检索的任务 —— 页码27
    - 2.3 非对称语义检索方案概述 —— 页码28
    - 2.4 组件 —— 页码29
        - 2.4.1 文本嵌入器 —— 页码29
        - 2.4.2 文档分块 —— 页码33
        - 2.4.3 向量数据库 —— 页码39
        - 2.4.4 Pinecone —— 页码39
        - 2.4.5 开源替代方案 —— 页码39
        - 2.4.6 检索结果重排 —— 页码40
        - 2.4.7 API —— 页码42
    - 2.5 完整方案 —— 页码44
    - 2.6 闭源组件的成本 —— 页码45
    - 2.7 本章小结 —— 页码45

- **第3章 提示词工程入门** —— 页码46
    - 3.1 简介 —— 页码46
    - 3.2 提示词工程 —— 页码46
        - 3.2.1 LLM的对齐 —— 页码47
        - 3.2.2 LLM提问 —— 页码48
        - 3.2.3 小样本学习 —— 页码50
        - 3.2.4 结构化输出 —— 页码51
        - 3.2.5 人物角色提示词 —— 页码52
    - 3.3 跨模型提示词工程 —— 页码53
        - 3.3.1 ChatGPT —— 页码53
        - 3.3.2 Cohere —— 页码53
        - 3.3.3 开源提示词工程 —— 页码54
    - 3.4 采用ChatGPT构建问答机器人 —— 页码54
    - 3.5 本章小结 —— 页码56

**第2部分 充分挖掘大模型的潜力**
- **第4章 通过定制化微调优化大模型** —— 页码61
    - 4.1 简介 —— 页码63
    - 4.2 迁移学习与微调入门 —— 页码63
        - 4.2.1 微调过程的解释 —— 页码64
        - 4.2.2 闭源预训练模型作为基础模型 —— 页码64
    - 4.3 OpenAI微调API概览 —— 页码66
        - 4.3.1 GPT-3微调API —— 页码67
        - 4.3.2 案例学习：亚马逊评论情感分类 —— 页码67
        - 4.3.3 数据指南和最佳实践 —— 页码67
    - 4.4 使用OpenAI CLI实现自定义数据微调 —— 页码68
    - 4.5 设置OpenAI CLI —— 页码68
    - 4.6 LLM微调实践 —— 页码71
        - 4.6.1 采用量化指标评测大模型 —— 页码72
        - 4.6.2 定性评估技术 —— 页码73
        - 4.6.3 将微调的GPT-3模型集成到应用程序中 —— 页码75
        - 4.6.4 案例学习：亚马逊评论分类 —— 页码77
    - 4.7 本章小结 —— 页码78

- **第5章 高级提示工程** —— 页码79
    - 5.1 提示注入攻击 —— 页码79
    - 5.2 输入/输出验证 —— 页码80
    - 5.3 批处理提示 —— 页码84
    - 5.4 提示链 —— 页码85
        - 5.4.1 提示链作为防御提示注入的手段 —— 页码87
        - 5.4.2 使用提示链来防止提示填充 —— 页码88
        - 5.4.3 使用提示链来安全地使用多模态LLM —— 页码89
    - 5.5 思维链提示 —— 页码91
    - 5.6 重新审视小样本学习 —— 页码93
    - 5.7 测试和迭代快速开发 —— 页码102
    - 5.8 本章小结 —— 页码103

- **第6章 定制嵌入层和模型架构** —— 页码104
    - 6.1 案例研究：构建一个推荐系统 —— 页码104
        - 6.1.1 定义问题和数据 —— 页码104
        - 6.1.2 推荐系统的定义 —— 页码106
        - 6.1.3 基于万条用户行为数据构建推荐系统 —— 页码108
        - 6.1.4 生成自定义字段来对比项目的相似性 —— 页码110
        - 6.1.5 采用基础词向量构建基线 —— 页码112
        - 6.1.6 准备微调数据 —— 页码112
        - 6.1.7 使用Sentence Transformers微调开源嵌入器 —— 页码116
        - 6.1.8 微调效果总结 —— 页码117
    - 6.2 本章小结 —— 页码120

**第3部分 大模型的高级使用**
- **第7章 超越基础模型** —— 页码121
    - 7.1 案例研究：视觉问答 —— 页码121
        - 7.1.1 模型简介：DistilBERT、视觉转换器和GPT-2 —— 页码122
        - 7.1.2 隐藏状态投影和融合 —— 页码125
        - 7.1.3 交叉注意力是什么以及为什么至关重要 —— 页码125
        - 7.1.4 定制多模式联运模型 —— 页码128
        - 7.1.5 数据：视觉问答 —— 页码131
        - 7.1.6 VQA训练迭代 —— 页码132
        - 7.1.7 结果总结 —— 页码133
    - 7.2 案例研究：从反馈中强化学习 —— 页码135
        - 7.2.1 FLAN-T5模型 —— 页码137
        - 7.2.2 奖励模型：情感和语法正确性 —— 页码137
        - 7.2.3 Transformer强化学习 —— 页码139
        - 7.2.4 RLF训练循环 —— 页码140
        - 7.2.5 结果总结 —— 页码143
    - 7.3 本章小结 —— 页码143

- **第8章 开源大模型的高级微调方法** —— 页码144
    - 8.1 案例研究：采用BERT对动漫进行多标签分类 —— 页码145
        - 8.1.1 采用Jaccard相似分来评估动漫标题多标签分类的效果 —— 页码145
        - 8.1.2 简单的微调大模型训练流程 —— 页码145
        - 8.1.3 通用的开源大模型微调技巧 —— 页码147
        - 8.1.4 结果总结 —— 页码148
    - 8.2 采用GPT-2生成LaTeX —— 页码155
        - 8.2.1 开源大模型的提示词工程 —— 页码155
        - 8.2.2 结果总结 —— 页码157
    - 8.3 Sinan尝试做出聪明而优美的回应：SAWYER —— 页码158
        - 8.3.1 有监督指令微调 —— 页码160
        - 8.3.2 奖励模型的训练 —— 页码161
        - 8.3.3 从（期望的）人类反馈中进行强化学习 —— 页码163
        - 8.3.4 结果总结 —— 页码167
    - 8.4 日新月异的微调世界 —— 页码167
    - 8.5 本章小结 —— 页码171

- **第9章 将LLM应用于生产** —— 页码172
    - 9.1 闭源LLM应用于生产 —— 页码173
    - 9.2 开源LLM应用于生产 —— 页码173
        - 9.2.1 将LLM应用于推理 —— 页码174
        - 9.2.2 互操作性 —— 页码174
        - 9.2.3 模型量化 —— 页码174
        - 9.2.4 模型剪枝 —— 页码175
        - 9.2.5 知识蒸馏 —— 页码175
        - 9.2.6 大模型的成本预估 —— 页码176
        - 9.2.7 模型推送到Hugging Face仓库 —— 页码183
    - 9.3 本章小结 —— 页码187
        - 9.3.1 欢迎向社区贡献代码 —— 页码187
        - 9.3.2 继续加油 —— 页码187

**第4部分 附录**
- **附录A LLM常见问题解答** —— 页码189
- **附录B LLM术语表** —— 页码192
- **附录C LLM应用架构** —— 页码197 
