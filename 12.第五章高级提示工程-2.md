### 第5章 高级提示工程
- 研究和引用相关行业统计数据和趋势，以支持该计划。
- 列出组织中需要批准该计划的关键人员。
- 用至少两种解决方案解决每个疑虑和担忧。
- 将计划控制在500字以内。

这可能是LLM来说无法一次完成的工作量。

当笔者在GPT - 3的playground中多次运行这个提示（除了最大长度之外的所有默认参数，以允许更长的内容）时，看到了许多问题。主要问题是该模型通常拒绝完成营销计划之外的任何任务，而营销计划通常甚至不包括笔者要求的所有项目。LLM通常不会列出关键人物，更不用说它们的关注点和解决这些关注点的方法了。计划本身通常超过600个单词，因此模型甚至无法遵循这一基本指令。

这并不是说营销计划本身是不可接受的，它有点泛泛而谈，但涵盖了笔者要求它解决的大部分关键点。这里的问题是：当对LLM要求太多时，它通常会选择要解决的任务，而忽略其他任务。

在极端情况下，当用户用过多的信息填满LLM的输入词元限制时，就会出现提示填充现象，用户希望LLM能够自己“弄清楚”时，可能会导致不正确或不完整的响应，或产生事实幻觉。举一个达到词元限制的例子，假设希望LLM输出一个SQL语句来查询数据库，考虑到数据库的结构和自然语言查询，如果有一个包含许多表和字段的大型数据库，那么这个请求很快就会达到输入限制。

可以采取一些策略来避免提示填充的问题。首先，最重要的是在提示中简明扼要，并只包含LLM所需的信息。这使得LLM能够专注于手头的具体任务，并产生更准确的结果，以解决所有期望的问题。此外，可以使用提示链，将多任务工作流程分解为多个提示。例如，可以用一个提示生成营销计划，然后使用该计划作为输入，要求LLM识别关键人员，等等。图5.9所示为链式提示的潜在工作流程：一个提示用于生成计划，另一个提示生成利益相关者和关注点，最后一个提示确定关注点的方法。

提示生成营销计划，然后使用该计划作为输入，要求LLM识别关键人员，等等。

提示填充也会对GPT的性能和效率产生负面影响，因为模型可能需要更长的时间来处理混乱或过于复杂的提示，并生成输出。通过提供简洁且结构良好的提示，可以帮助GPT更有效、更高效地执行。

现在已经探索了快速填充的危险，并看到了避免危险的方法，下面把注意力转向一个重要的安全和隐私问题：提示注入。

#### 5.4.3 使用提示链来安全地使用多模态LLM

![image](https://github.com/user-attachments/assets/15b9cc5d-ec1f-4cb2-a0c8-a79f1d82e62b)


假设想要构建一个311式的系统，人们可以在其中提交照片来报告他们附近的问题。可以将几个LLM连接在一起，每个LLM都有特定的角色，以创建一个全面的解决方案。
- **LLM - 1（图像字幕）**：这种多模态模型专门为提交的照片生成准确的字幕，用于处理图像并提供其内容的文本描述。
- **LLM - 2（分类）**：这个纯文本模型采用LLM - 1生成的标题，将问题分类为几个预定义选项之一，如“坑洼”“路灯损坏”或“涂鸦”。
- **LLM - 3（后续问题）**：基于LLM - 2确定的类别，LLM - 3（纯文本LLM）生成相关的后续问题，以收集有关该问题的更多信息，确保采取适当的行动。
- **LLM - 4（视觉问答）**：这种多模态模型与LLM - 3协同工作，使用提交的图像回答后续问题。它将图像中的视觉信息与LLM - 3的文本输入相结合，提供准确的答案以及每个答案的置信度评分。使系统能够优先处理需要立即关注的问题，或将置信度评分较低的问题升级给操作员进行进一步评估。

图5.10为多模态提示链，从左上角的用户提交图像开始，使用四个LLM（三个开源模型和Cohere）来接收图像、描述图像、分类图像、生成后续问题，并以给定的置信度回答这些问题。

此示例的完整代码可以在本书的代码库中找到。

关于提示链，下面来看看迄今为止在应用方面最有用的进步之一——思维链。

![image](https://github.com/user-attachments/assets/6f0fbb39-22eb-4d91-bb6e-842573588070)


#### 5.5 思维链提示
思维链提示是一种迫使LLM通过一系列步骤进行推理的方法，从而产生更结构化、更透明和更精确的输出。其目标是将复杂的任务分解为更小且相互关联的子任务，使LLM能够逐步解决每个子任务。这不仅有助于模型“聚焦”于问题的特定方面，而且鼓励生成中间输出，更容易识别和调试过程中的潜在问题。

思维链提示的另一个显著优势是提高了LLM生成响应的可解释性和透明度。通过了解模型的推理过程，用户可以更好地理解和判断最终输出是如何得到的，这提高了人们对模型决策能力的信任。

**示例：基本算术**
最近出现的LLM，如ChatGPT和GPT - 4，即使没有提示，比它们的前辈更有可能输出思维链。图5.11显示了GPT - 3和ChatGPT中完全相同的提示。


![image](https://github.com/user-attachments/assets/a055e5da-89ba-4e60-83ae-e58031637e7f)


**Question**：A store sells packs of pencils. There are 12 pencils in each pack. If Mary buys 3 packs and then gives away 8 pencils, how many pencils does she have left?
A) 28 pencils
B) 32 pencils
C) 36 pencils
D) 20 pencils

**Answer**：
- **GPT - 3**：自己不会尝试推理，直接给出答案D) 20 pencils。
- **当被要求推理时，GPT - 3得到了正确答案**
  - **Reasoning**：Mary has 3 packs of pencils, each containing 12 pencils. This means she has 36 pencils in total. She then gives away 8 pencils, leaving her with 28 pencils.
  - **Answer**：A) 28 pencils
- **ChatGPT和GPT - 4**：没有被要求推理，但进行了推理，虽然格式上不工整，但很快会看起来更合理。
  - **推理过程**：Mary initially has 3 packs of pencils, with 12 pencils in each pack, for a total of 3 × 12 = <<3*12=36>>36 pencils. After giving away 8 pencils, she is left with 36 - 8 = <<36 - 8=28>>28 pencils.
  - **Answer**：A) 28 pencils

一些模型经过专门训练，可以一步步地推理问题，包括GPT - 3.5和GPT - 4，但并非所有模型都是如此。图5.11证明了GPT - 3.5（ChatGPT）不需要被明确告知来推理，否则它自己不会这样做。GPT - 3（达芬奇）则需要明确要求通过一系列思维链来推理问题，一般来说，将复杂任务分解为易于消化的子任务是思维链提示的绝佳选择。

图5.11（上）证明，对GPT - 3来说，一道有多个选项的基本算术题太难了。图5.11（中）通过在提示语末尾添加“一步一步推理”，让GPT - 3思考问题时使用思维链提示，模型得到了正确答案。图5.11（下）ChatGPT和GPT - 4不需要被告知如何推理问题，因为它们已经对齐，可以主动对思维链进行思考。

#### 5.6 重新审视小样本学习
下面重新学习一下小样本学习的概念，这种技术使LLM能够以极少的训练数据快速适应新任务。在第3章中讲解了小样本学习的例子。随着基于Transformer的LLM技术的不断发展，越来越多的人将其应用于自己的架构中，小样本学习已经成为一种关键的方法，可以充分利用这些最先进的模型，使它们能够高效学习并执行比LLM最初承诺更广泛的任务。

笔者想在小样本学习方面更进一步，看看能否在特别具有挑战性的领域——数学——提高LLM的性能。

**示例：使用LLM进行小学算术**


尽管大模型具有令人印象深刻的能力，但它们往往很难以与人类相同的准确性和一致性处理复杂的数学问题。这个例子的目标是通过利用小样本学习和一些基本的提示工程技术，增强LLM理解、推理和解决相对复杂的数学应用题的能力。

这个例子将使用名为GSM8K的开源数据集，这是一个由8500个多种语言的小学算术应用题组成的数据集。该数据集的目标是支持需要多步推理的基础数学问题的问答任务。图5.12展示了来自训练集的GSM8K数据点的示例。它展示了一个问题和一系列如何逐步解决问题的思路，在定界符“####”之后得出最终答案。注意使用的是主子集；这个数据集的一个子集叫作socratic，它具有相同的格式，但其思路遵循苏格拉底方法。

![image](https://github.com/user-attachments/assets/5acbf3f4-5ea8-453b-af36-72a4eefbea65)


```
{
    "question": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?",
    "answer": "Natalia sold 48/2 = <<48/2=24>>24 clips in May. Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May. #### 72"
}
```

请注意GSM8K数据集使用包含用于方程的<<>>词元的方法，就像ChatGPT和GPT - 4一样。这是因为这些LLM部分使用具有类似符号的类似数据集进行训练。

这意味着这些模型应该已经擅长这个问题了，对吧？这就是这个例子的重点。

假设目标是让LLM在这个任务上尽可能表现优秀。本章将从最基本的提示开始——只是让LLM解决任务。

当然，由于希望尽可能公平地对待LLM，因此还包括一个明确的指令，清楚地说明要做什么，甚至提供期望的答案格式，以便可以在最后轻松解析。可以在playground中将其可视化，如图5.13所示。图5.13只是让ChatGPT和DaVinci用明确的指令和遵循的格式解决一个算术问题，但两个模型都回答错了。

![image](https://github.com/user-attachments/assets/36042bbf-3ab0-493a-8a9a-49ebf00eb732)


**用户**：

Answer the arithmetic problem in the following format:

Question: {an arithmetic question}

Answer: {the final answer as a number}

####
Question: The Easter egg hunt team hid 100 eggs. The Smith twins each found 30 eggs. All the other eggs except 10 were found by their friends. How many eggs did the friends find?

**助手**：
- **ChatGPT**：Answer: 40 eggs (100 - 30 - 30 - 10 = 40)
- **DaVinci**：Answer: 70 （甚至没有尝试推理答案）

图5.14给出了提示基线的基线准确度（通过给出完全正确的答案模型定义），即只需给出明确的指令和格式化要求，即可获得以下四个LLM：
- 聊天机器人（GPT - 3.5 - turbo）。
- 达芬奇（文本 - 达芬奇 - 003）。
- 连贯（命令 - xlarge - nightly）。
- 谷歌的大型Flan - T5。

下面通过测试思维链是否提高了模型的准确性，开始提高准确性的探索。

在本章前面已经看到了使用思维链的例子：在回答问题之前要求LLM展示其工作，似乎可以提高其准确性。现在将更加严格：下文将定义一些测试提示，并从给定的GSM8K测试数据集中选取几百个项目运行。程序清单5.2加载数据集并设置前两个提示。


![image](https://github.com/user-attachments/assets/03b079ea-c5e6-4679-bf65-a118ce99c5e8)


**程序清单5.2：加载GSM8K数据集并定义前两个提示**
```python
# 从datasets库中导入load_dataset函数
from datasets import load_dataset

# 使用main配置加载gsm8k数据集
gsm_dataset = load_dataset("gsm8k", "main")

# 从数据集的train子集中输出第1个问题
print(gsm_dataset['train']['question'][0])
print()

# 从数据集的train子集中输出对应的第1个答案
print(gsm_dataset['train']['answer'][0])
...
Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
Janet sells 16 - 3 - 4 = <<16 - 3 - 4=9>>9 duck eggs a day.
She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer's market.
#### 18
...
```

新提示（图5.15）要求LLM在给出最终答案之前通过推理得出答案。将这个变体与基准进行比较，将揭示第一个问题的答案：是否要将思维链包含在提示内？答案可能是“当然”，但值得测试，主要是因为包含思维链意味着在上下文窗口中包含更多词元。正如一次又一次看到的那样，更多的词元意味着更多的钱——因此，如果这一系列思维链没有带来显著的结果，那么可能根本不值得将其包括在内。

![image](https://github.com/user-attachments/assets/4adec679-3bdd-415f-ad93-54337329fb26)


**图5.15**

**用户**：

Answer the arithmetic problem in the following format:

Question: {an arithmetic question}

Reasoning: {thinking through step by step on how to solve the problem}

Answer: {the final answer as a number}

####

Question: The Easter egg hunt team hid 100 eggs. The Smith twins each found 30 eggs. All the other eggs except 10 were found by their friends. How many eggs did the friends find?



**助手**：

If the Smith twins found 30 eggs each, then they found a total of 60 eggs (30 + 30 = 60). If all the other eggs except 10 were found by their friends, then the friends found 100 - 60 - 10 = 30 eggs.

Answer: 30 eggs



图5.15中第一个提示变体扩展了基线提示，只是给LLM空间来首先推理得出答案。ChatGPT现在对这个例子给出了正确答案。

![image](https://github.com/user-attachments/assets/5d4d3e5c-effd-498b-9d5a-73c270ccd02f)


**程序清单5.3显示了通过测试数据集运行这些提示的示例**

```python
# 定义一个函数来格式化GSM的k - shot示例
def format_k_shot_gsm(examples, cot = True):
    if cot:
        # 如果cot = True,在提示中包含推理
        return '\n###\n'.join(
            [f"Question: {e['question']}\nReasoning: {e['answer'].split('####')[0].strip()}\nAnswer: {e['answer'].split('#### ')[-1]}' for e in examples]
        )
    else:
        # 如果cot = False, 在提示中不包含推理
        return '\n###\n'.join(
            [f"Question: {e['question']}\nAnswer: {e['answer'].split('#### ')[-1]}' for e in examples]
        )

# 定义test_k_shot函数,测试使用k - shot学习的模型
def test_k_shot(
    k, gsm_datapoint, verbose = False, how = 'closest', cot = True,
    options = ['curie', 'cohere', 'chatgpt', 'davinci', 'base - flan - t4', 'large - flan - t5']
):
    results = {}
    query_emb = model.encode(gsm_datapoint['question'])
   ...
    # 对GSM测试集开始迭代
    # 初始化一个空字典来存储结果
    closest_results = {}
    # 用不同的k - shot数据循环
    for k in tqdm([0, 1, 3, 5, 7]):
        closest_results[f'Closest K={k}'] = []
        # 用不同的GSM样本数据集循环
        for i, gsm in enumerate(tqdm(gsm_sample)):
            try:
                # 用当前数据点测试k - shot学习并保存结果
                closest_results[f'Closest K={k}'].append(
                    test_k_shot(
                        k, gsm, verbose = False, how = 'closest',
                        options = ['large - flan - t5', 'cohere', 'chatgpt', 'davinci']
                    )
                )
            except Exception as e:
                error += 1
                print(f'Error: {error}. {e}. i={i}. K={k}')
```

**第一个结果如图5.16所示，比较了四个LLM之间前两个提示选择的准确性**

- **不适用思维链询问**：5.5节测试的基线提示，有明确的指令集和格式。

- **使用思维链询问**：实际上是同一个提示，但同时也给出LLM空间来先推理答案。

![image](https://github.com/user-attachments/assets/be71f1a5-4807-4742-a61a-db17f6b1999a)


图5.16以图5.13所示的格式，向四个模型询问一些算术问题，提供改进的基线。ChatGPT似乎在这项任务中表现最好（这并不奇怪）。

思维链貌似带来了期望的准确性的显著提高。因此，第一个问题的答案已经明确：是否在提示中加入一系列的思维链？是。

接下来测试一下如果给出一些在上下文中解决问题的例子，LLM是否会很好地响应，这些例子是否只会让LLM更加困惑。

在图5.16中，与没有思维链（左侧的条形图）相比，使LLM产生思维链（右侧的条形图）已经极大地促进了所有模型。

**用小样本促进LLM**


接下来要回答的问题是：是否要在提示中包含小样本示例？同样，可能会假设答案为“是”


。但是例子等于更多的词元，因此值得在数据集上进行再次测试，以测试更多提示变体：

 - **只提问（K = 0）**：表现最好的提示（到目前为止）。
 - **随机3例**：从训练集中随机选取3个例子，并在例子中包含思维链，以帮助LLM理解如何通过问题推理。

图5.17显示了新提示变体的示例，以及该变体在测试集中的表现。结果似乎很明确，包括这些随机示例 + 思维链（CoT）确实很有希望。这似乎回答了疑问：是否要包含少量的示例？是。

令人惊讶的是，我们正在取得进步。再问两个额外的问题。

(1) **示例重要吗？重新审视语义搜索**

想要一个思维链，也想要样本，样本真的重要吗？在前面的例子中只是从训练集中随机抓取了3个示例，并将其包含在提示中。但是，如果我们更聪明一点呢？接下来笔者将从自己的书中抽取一页内容，使用开源双编码器实现原型语义搜索。通过这种方法，当向LLM提出一个数学问题时，在上下文中包含的例子是训练集中语义最相似的问题。



程序清单5.4显示了如何通过编码GSM8K的所有训练样本来实现这个原型。可以使用这些嵌入在小样本学习中只包含语义相似的例子。

![image](https://github.com/user-attachments/assets/338eb289-491f-4347-94b0-f2fbaa702d38)


**程序清单5.4：对在GSM8K训练集中的问题进行编码以实现动态检索**
```python
from sentence_transformers import SentenceTransformer
from random import sample
from sentence_transformers import util

# 加载预训练的SentenceTransformer模型
model = SentenceTransformer('sentence - transformers/multi - qa - mpnet - base - cos - v1')

# 从GSM数据集中获取问题
docs = gsm_dataset['train']['question']

# 使用SentenceTransformer模型对问题进行编码
doc_emb = model.encode(docs, batch_size = 32, show_progress_bar = True)
``` 
