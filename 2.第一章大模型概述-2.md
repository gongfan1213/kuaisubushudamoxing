### 第1章 大模型概述
#### 1.1.3 大模型是如何工作的

LLM的表现效果仅是合格还是最佳，取决于其预训练和微调的方式。用户需要快速了解LLM是如何进行预训练的，以了解它擅长什么，不擅长什么，以及用户是否需要用自定义数据来更新它的权重参数。

1. **预训练**

市场上的每个LLM都经过了大量文本数据和特定语言建模相关任务的预训练。在预训练期间，LLM尝试学习和理解通用语言与单词之间的关系。每个LLM都经过不同语料库和不同任务的训练。

例如，BERT最初是在以下两个公开可用的文本语料库上进行预训练（图1.6）。

- **英文维基百科**：免费在线百科全书维基百科英文版的文章集合。包含一系列主题和多种写作风格，使其成为英语文本（约25亿单词）的多样化和代表性样本。

- **BookCorpus**：大量的小说和非小说类书籍。通过从网络上抓取书籍文本而创建，包括从浪漫到悬疑，再到科幻及历史题材的各种类型。语料库中的书籍最小长度为2000个单词，并且由验证过身份的作者用英语撰写（总共约8亿单词）。

![image](https://github.com/user-attachments/assets/2c45aa0e-3e56-448b-8107-bff23e1613be)


BERT在这两个特定的语言建模任务上进行了预训练（图1.7）。

- **掩码语言建模（MLM）任务（自编码任务）**：帮助BERT识别单个句子内的词元交互。

- **下一句预测（NSP）任务**：帮助BERT理解句子之间的词元是如何相互作用的。

![image](https://github.com/user-attachments/assets/c041abf2-9a63-48d0-9106-0cb858a4a482)



对这些语料库进行预训练，BERT（主要通过自注意力机制）能从中学到丰富的语言特征和上下文关系。使用大型、多样化的语料库已经成为NLP研究的常见做法，因为它已被证明可以提高模型在下游任务中的性能。

如图1.6所示，BERT最初是在英文维基百科和BookCorpus上进行预训练的。而最近的LLM是在比它大数千倍的数据集上训练的。

如图1.7所示，BERT在两个任务上进行了预训练：通过自编码语言建模任务（又称为“掩码语言建模”任务）学习单个词的嵌入表示；通过“下一句预测”任务学习整个文本序列的嵌入表示。


**注意**

LLM的预训练过程会随着时间的推移而不断发展，因为研究人员会找到更好的训练LLM的方法，并逐步淘汰那些没有多大帮助的方法。例如，在谷歌发布BERT的第一年，使用了NSP预训练任务，Facebook AI的BERT变体RoBERTa证明BERT不需要NSP任务来匹配，从而在几个领域击败了原始BERT模型。


最终使用的LLM，可能会与其他LLM进行不同的预训练，这就是LLM彼此不同的原因。一些LLM在专有数据源上进行训练，包括OpenAI的GPT系列模型，这样会使这些公司比竞争对手更具优势。

本书不会经常回顾预训练的思路，因为它不是“快速入门指南”中“快速”的部分。尽管如此，了解这些模型是如何进行预训练的仍然是有价值的，因为这种预训练能够应用迁移学习，并能够达到想要的最优效果。


2. **迁移学习**

迁移学习是机器学习技术的一种，利用从一个任务中获得的知识来提高另一个相关任务的性能。LLM的迁移学习包括在一个文本数据语料库中对LLM进行预训练，然后通过使用特定任务数据更新模型的参数，对LLM进行微调，以用于特定的“下游”任务，如文本分类或文本生成。

迁移学习背后的思想是预训练的模型已经学习了大量关于语言和单词之间关系的信息，这些信息可以作为起点，以提高新任务的性能。迁移学习允许LLM针对特定任务进行微调，所需数据要小得多。如果从头开始训练模型，则不需要特定任务的数据。这大大减少了训练LLM所需的时间和资源。图1.8是这种关系的可视化表示。一般的迁移学习流程包括在通用数据集上对通用自监督任务进行模型预训练，然后在特定的任务数据集上针对手头的具体任务模型进行微调。

![image](https://github.com/user-attachments/assets/ae354c76-d95d-473d-8024-01be8c56daa9)


3. **微调**

一旦LLM经过预训练，就可以针对特定任务进行微调。微调涉及在较小的特定任务数据集上训练LLM，以调整其参数，使其适应特定任务。这使得LLM能够利用其预训练的语言知识来提高在特定任务上的准确性。微调已被证明可以极大地提高特定领域和特定任务的性能，并使LLM能够快速适应各种NLP应用。

图1.9展示了后面章节中微调模型的基本流程。无论是开源还是闭源，流程或多或少都是一样的：

（1）定义要微调的模型以及全部微调参数（例如学习率）。

（2）聚合一些训练数据（格式和其他特征，取决于当前更新的模型）。 

（3）计算损失（误差的度量）和梯度（如何改变模型以最小化误差的信息）。 

（4）通过反向传播来更新模型——这是一种更新模型参数以最小化误差的机制。 

![image](https://github.com/user-attachments/assets/f4c532d3-ccaf-445b-9553-5afe6008edf8)


**注意**
除了笔者提到的非常具体的高级练习之外，读者不需要Hugging Face账户或密钥来阅读和使用本书中的任何代码。

4. **注意力**
Transformer的原始论文的标题是“Attention Is All You Need”（注意力就是你需要的全部）。注意力机制是深度学习模型中使用的一种机制（不仅仅只是Transformer），允许模型动态地“聚焦”输入的不同部分，从而提高性能和结果准确性。在注意力普及之前，大多数神经网络对所有输入进行平等处理，模型根据输入的固定表示进行预测。现代的LLM依赖注意力机制，可以动态地聚焦输入序列的不同部分，从而在预测时权衡每部分的重要性。

简而言之，LLM在大型语料库上进行预训练，有时会在较小的数据集上针对特定任务进行微调。回想一下，在训练LLM时，其中一个因素是作为语言模型，Transformer的有效性在于其高度并行性，可实现更快的训练和高效的文本处理。真正使Transformer区别于其他深度学习架构的，是其使用注意力捕获大范围词元之间的依赖关系的能力。换句话说，注意力机制是LLM基于Transformer的关键组成部分，使LLM能够有效地在训练循环和任务之间保留信息（即迁移学习），同时能够轻松处理冗长的文本样本。

注意力被认为是帮助LLM学习（或至少是认识）内部世界和人类可识别规则的最重要的因素。斯坦福大学在2019年进行的一项研究表明，BERT中的某些注意力计算与语法和语法规则的概念相对应。例如，研究人员注意到，BERT能够从其预训练中以极高的准确率注意到动词的直接宾语、名词的限定词和介词的宾语。这些关系在图1.10中可以可视化的方式呈现。

![image](https://github.com/user-attachments/assets/370214d1-1543-409f-bdcb-04d51f50cecc)


图1.10中的研究对LLM进行了调查，发现LLM似乎能够识别未被明确告知的语法规则。

其他研究揭示了LLM能够通过预训练和微调学习那些其他类型的“规则”。一个例子是由哈佛大学研究人员领导的一系列实验，这些实验探索了LLM学习一组规则的能力，这些规则用于合成任务，如奥赛罗游戏（图1.11）。他们发现有证据表明，LLM能够通过训练历史移动数据来理解游戏规则。

![image](https://github.com/user-attachments/assets/76d74645-efed-4be6-9071-559037a15ecf)


图1.11中，LLM可能能够学习关于世界的各种知识，无论是游戏的规则和策略，还是人类语言的规则。

然而，对于学习任何规则的LLM，它都必须将人们认为的文本转换为机器可读的内容。这是通过嵌入过程完成的。

![image](https://github.com/user-attachments/assets/46230589-6ecd-4761-a037-78e6efa8fdcb)


5. **嵌入**

嵌入是在高维空间中对单词、短语或词元的数学表示。在NLP中，嵌入用于表示单词、短语或词元，以捕捉它们的语义含义以及与其他单词的关系。可能有多种类型的嵌入，包括对句子中词的位置进行编码的位置嵌入，以及对词的语义进行编码的词嵌入（图1.12）。

图1.12是关于BERT如何使用三层的嵌入模型嵌入给定文本的示例。一旦文本被词元化，每个词元都会被赋予一个嵌入，然后将每个词元的嵌入和位置嵌入相加，因此每个词元在计算注意力之前都有一个初始嵌入。除非它们有更实际的目的，否则人们不会过多关注LLM嵌入的各个层，但了解这些部分以及它们的内涵是有益的。

LLM根据预训练学习不同词元的嵌入，并可以在微调期间进一步更新这些嵌入。

6. **词元化**

如前所述，词元化涉及将文本分解为最小的理解单元——词元。词元化过程是句子按照语义切割成小片断，并以嵌入方式参与注意力机制的计算过程，这也是大模型训练的重要环节之一。词元组成了LLM的静态词汇表，但并不总是代表整个单词。例如，词元可以代表标点符号、单个字符，如果一个单词不为LLM所熟知，甚至可能是一个子词。几乎所有的LLM都有对特殊词元的定义。 

例如BERT模型具有特殊的[CLS]词元，BERT会自动讲其作为每个输入的第一个词元注入，旨在表示整个输入序列的编码语义含义。
