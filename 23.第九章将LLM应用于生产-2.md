### 务的模型在性能方面与教师模型的相似性,并展示了模型在内存使用和速度方面的差异性。

图9.4中BERT large-uncased模型比特定任务DistilBert模型的速度快4~6倍,内存效率更高,但效果稍差。

### 9.2.6 大模型的成本预估

在使用开源模型的情况下,成本预估包括托管和运行模型所需的计算和存储资源。

- **计算成本**: 包括模型运行的计算机(虚拟机或专用硬件)的成本。计算机的CPU、GPU、内存和网络功能,以及运行时间等因素也需考虑在内。

- **存储成本**: 包括存储模型的权重和偏差,以及模型推理所需的数据成本。这些成本取决于型号和数据的大小、存储类型(如SSD与HDD)。如果存储多个模型版本,存储成本需要累计。 

- **扩展成本**: 如果用户打算服务大量请求,则可能需要使用负载平衡和自动扩展解决方案,这会带来额外的成本。 

- **维护成本**: 与监控和维护部署相关的成本,例如日志记录、警报、调试和更新模型。


准确预测这些成本需要全面了解应用程序的需求、所选云提供商的定价结构和模型的资源需求。通常情况下,明智的做法是利用云服务提供的成本估算工具,执行小规模测试以收集指标,或咨询云解决方案架构师以获得更准确的预测。

### 9.2.7 模型推送到Hugging Face仓库
本章使用了Hugging Face的内置模型,最终考虑通过Hugging Face平台向世界共享自己的开源、微调模型,旨在为社区提供更广泛的模型可视化及其易用性。如果倾向于使用Hugging Face作为存储库,则需要遵循以下步骤。
- **(1)准备模型**

在推送模型之前,请确保对其进行了适当微调,并将其保存为与Hugging Face兼容的格式。为此,可以使用Hugging Face Transformers库中的`save_pretrained()`函数(见程序清单9.5)。
```python
# 程序清单9.5:将模型和标记器保存到硬盘
from transformers import BertModel, BertTokenizer
# 假设用户有微调后的模型和标记器
model = BertModel.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# 保存模型和标记器
model.save_pretrained("<your-path>/my-fine-tuned-model")
tokenizer.save_pretrained("<your-path>/my-fine-tuned-model")
```
- **(2)考虑授权**

将模型上传到存储库时,必须为其指定许可证。许可证将告诉用户能否对代码做出修改。主流的许可证包括Apache 2.0、MIT和GNU GPL v3。应该在模型存储库中包含许可证文件。下面是三个许可证的详细信息。
    - **Apache 2.0**: Apache 2.0许可证允许用户自由使用、复制、分发、展示和公开,包括制作衍生作品。条件是任何发行版都应该包括原始Apache 2.0许可证的副本,说明所做的任何更改,并包括一个NOTICE文件(如果存在)。此外,尽管该许可证允许使用专利权利要求,但它并不提供出资人对专利权的公开授予。 
    - **MIT**: MIT许可证是一种开放源代码许可证,它允许在专有软件中重新使用被许可软件,但是要求所有被许可软件的副本都必须包含MIT许可证的副本。用户可以自由地使用、复制、修改、合并、发布、分发、再许可和/或销售软件副本,只需在这些副本中包含必要的版权和许可声明。 
    - **GNU GPL v3**: GNU通用公共许可证(GPL)是一种版权许可证,它要求分发或发布的任何作品,只要全部或部分包含,或衍生自该程序或其任何部分,都必须根据GNU GPL v3的条款向所有第三方免费提供整体许可证。此许可证确保收到作品副本的所有用户可以自由使用、修改和分发原作品。不过,它要求任何修改也必须根据相同的条款进行许可,而MIT或Apache许可则不要求如此。 
- **(3)书写模型名片**
模型名片是模型的主要文档。它提供有关模型用途、功能、限制和功能的信息。模型名片的基本组件包括以下各项。
    - **模型描述**: 关于模型做什么以及如何训练的详细信息。 
    - **数据集详细信息**: 用于训练和验证模型的数据信息。 
    - **评估结果**: 关于模型在各种任务中的性能的详细信息。 
    - **用法示例**: 显示如何使用模型的代码段。 
    - **限制和偏见**: 模型中的任何已知限制或偏见。 

模型名片(名为README.md的标记文件)应位于模型的根目录中。Hugging Face Trainer也提供了一种使用`trainer.create_model_card()`方法自动创建模型名片的方法。模型提供者需要向这个自动生成的说明文件中添加更多内容,否则说明文件将仅包括模型名称和最终度量等基本信息。
- **(4)将模型推送到存储库**
Hugging Face Transformers库具有`push_to_hub`功能,允许用户方便地将其模型直接上传到Hugging Face Model hub。程序清单9.6是该功能的使用示例。
```python
# 程序清单9.6:将模型和标记器推送到Hugging Face库
from transformers import BertModel, BertTokenizer
# 假设用户有微调后的模型和标记器
model = BertModel.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# 将模型和标记器存储到目录中
model.save_pretrained("my-fine-tuned-model")
tokenizer.save_pretrained("my-fine-tuned-model")
# 把模型推送到Hub库
model.push_to_hub("my-fine-tuned-model")
tokenizer.push_to_hub("my-fine-tuned-model")
```
该脚本验证用户的Hugging Face凭据,将微调的模型和标记器保存到目录中,然后将它们推送到Hub库。`push_to_hub`方法采用模型存储库的名称作为参数。

用户还可以通过使用Hugging Face提供的命令行`hugging face CLI login`单独登录,或者使用`huggingface_hub`客户端以编程方式与Hub交互,以在本地保存凭据。请注意,本例假设已经在Hugging Face模型仓库中创建了一个名为“my - fine - tuned - model”的存储库。如果存储库不存在,则需要先创建它,或者在调用`push_to_Hub`时使用`repository_name`参数。

将模型推送到仓库之前,不要忘记在模型目录中编写一个良好的模型名片(README.md文件),这将与模型和标记器一起自动上传,并为用户提供如何使用模型以及其性能、限制等方面的说明书。Hugging Face有一些较新的工具可以帮助用户编写信息丰富的模型名片,Hugging Face也提供了大量如何使用这些工具的文档。 
- **(5)使用Hugging Face推理终端部署模型**
在将模型推送到Hugging Face存储库后,可以使用Hugging Face提供的推理终端进行轻松部署,可以支持专用的、完全自主可控的基础架构。该服务支持创建生产级别的API,而无须用户处理容器、GPU或任何MLOP。Hugging Face推理终端支持以现收现付的方式运行所使用的原始计算能力,有助于降低生产成本。

图9.5展示了基于DistilBERT的序列分类器在Hugging Face推理终端上使用部署时的一个截图,该序列分类器每月仅花费约80美元。程序清单9.7显示了使用该端点处理请求的示例。

![image](https://github.com/user-attachments/assets/1fbd9cd6-534d-42e2-9b5e-7bfc31cbc1d5)


图9.5在Hugging Face推理终端上部署一个简单的二元分类器,该分类器接收一段文本并输出两个类(“有毒”和“无毒”)的概率。

```python
# 程序清单9.7:使用Hugging Face inference endpoint来分类文本
import requests, json
# 用自己的URL替换Hugging Face inference endpoint的URL
url = "https://d2q5h5r3a1pkorfp.us-east-1.aws.endpoints.huggingface.cloud"
# 用实际的Hugging Face API key替换'HF_API_KEY'
headers = {
    "Authorization": f"Bearer {HF_API_KEY}",
    "Content-Type": "application/json",
}
# 在HTTP请求中发送的数据
# 将top_k参数设置为None,以获得所有可能的分类
data = {
    "inputs": "You're such a noob get off this game.",
    "parameters": {'top_k': None}
}
# 用头和数据为Hugging Face API制作一个POST请求
response = requests.post(url, headers = headers, data = json.dumps(data))
# 输出服务器的回复
print(response.json())
# [{'label': 'Toxic','score': 0.67}, {'label': 'Non-Toxic','score': 0.33}]
```
将ML模型部署到云是一个独立的主题。显然,这里的讲解忽略了关于MLOps流程、监控仪表板和持续训练管道的工作。即使如此,这应该足以让用户开始使用部署的模型。

### 9.3 本章小结
正如莎士比亚所言,离别可以是如此甜蜜的悲伤,让我们通过LLM结束阅读之旅。现在应该停下来回顾一下本书的内容。从提示工程的复杂性,探索令人兴奋的语义搜索领域,为LLM奠定提升准确性的知识基础,并为定制应用微调它们,到利用蒸馏和指令对齐的能力,已经涉及了许多使用这些优秀模型的方式,利用它们的能力,我们与技术的互动更具吸引力并以人为中心。

- **9.3.1 欢迎向社区贡献代码**

您编写的每一行代码,都使我们在让技术更好地理解和反馈人类需求的路上更近了一步。虽然挑战是巨大的,但潜在的回报更大,您的每一步探索都对社区的集体知识库有帮助。


您的好奇心和创造力,以及从本书中获得的技术技能,将成为您的指南。在您继续摸索和推进LLM能力边界时,为您带来指导。 

- **9.3.2 继续加油**

当您冒险前进时,请保持好奇心,保持创造力,保持友善。记住,您的工作会影响到其他人,请确保以善良和公平的方式影响他人。LLM的前景广阔而神秘,等待着像您这样的探险家来照亮前路。所以,本书献给你们——下一代大模型的先驱。让我们一起快乐编码。 
