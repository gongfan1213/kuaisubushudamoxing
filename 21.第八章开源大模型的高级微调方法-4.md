图 8.12

在图8.12中，奖励模型将接受对各种LLM查询的回复，这些LLM由人类评分，并学会区分查询的回复中哪些是首选，哪些不是首选。

奖励过程可以被视为一个简单的分类任务：给定一个问题和两个答案，对哪个答案是首选进行分类。然而，标准的分类指标仅奖励系统选择正确的答案，而这里更值得关注的是连续的奖励程度。这个任务会借鉴OpenAI的经验，为这些标记的答案定义一个自定义损失函数。

当对模型进行微调时，通常需要自定义损失函数。根据经验，损失函数的选择取决于具体任务，而不是适用于所使用的模型。损失函数是模型在训练过程中优化的方向。该函数量化模型预测与实际数据之间的差异，引导模型的学习朝着预期的结果发展。因此，当可用的通用损失函数无法有效捕捉特定任务的细微差别时，创建自定义损失函数就变得很有必要。

自定义损失函数时需要清楚地了解任务的目标和数据集的性质。这需要了解模型如何学习，以及如何以有意义和有效的方式将预测与实际目标进行比较。此外，考虑损失函数的复杂性和可解释性之间的平衡至关重要。虽然复杂的函数可能会更好地捕捉任务的复杂性，但也可能会使训练更加艰难，结果更难以解释。

一个基本的原则是，必须确保自定义损失函数是可微分的，也就是说，它必须在任何地方都有导数。提出这一要求是因为在这些模型中学习是通过梯度下降完成的，而这就需要计算损失函数的导数。

对于奖励模型，将基于负对数似然损失自定义一个损失函数。这个特定的损失函数对于涉及概率和排名的任务特别重要。在这种情况下，我们不仅关注模型是否能够正确预测，而且关注它对预测的置信度。负对数似然是一种惩罚模型的方法，该模型对错误预测过于自信或对正确预测不够自信进行惩罚。

因此，负对数似然概率描述了模型对其预测的置信，促使模型对数据有更细致的理解。它鼓励模型将更高的概率分配给首选结果，将较低的概率分配给次选结果。这种机制使它在训练模型对回复进行排序或任何其他相对偏好重要的场景中特别有效。

下面定义如图8.13所示的成对对数似然损失。该函数将接受一个问题以及一组由人类给出的具有分数的回复，并训练模型选择得分较高的回复。

此函数类似于OpenAI在2022年3月发表的一篇论文（https://arxiv.org/abs/2203.02155）中定义的原始InstructGPT损失函数，但这里增加了乘以分数差平方的步骤，以帮助从较少的数据中学习更多。程序清单8.10显示了在Python代码中为Trainer类定义的自定义损失函数。

程序清单8.10：自定义奖励对数损失

```js
1. Reward of preferred - Reward of nonpreferred (Rk - Rj) = 0.53
a. Higher is better (higher difference means we prefer the preferred
2. score_diff = actual score difference = 6 - 2 = 4
a. The higher this number, the more I want the rewards to be far from each
other
3. Multiply by actual score difference squared (Rk - Rj) * score_diff**2 = 8.48
a. Higher is better and now if the responses are greatly different, this
number is much higher
4. Apply sigmoid function to the difference
a. This represents a model's estimated probability that the preferred
response should indeed be preferred over the less preferred one
b. Higher is better
5. Take the log of the value
a. This does a lot but primarily penalizes wrong predictions more harshly
b. Higher is better
6. Take the negative value
a. Lower is better
Final Loss = -log(sigmoid (reward_of_preferred - reward_of_non_preferred) *
(score_diff**2))
= 0.0002
```

![image](https://github.com/user-attachments/assets/fed2d503-ea33-4218-82dc-55856ab56140)


```
# 子类化Hugging Face Trainer类来定制损失计算

class RewardTrainer(Trainer):
    # 重写compute_loss函数，以定义如何计算特殊任务的损失
    def compute_loss(self, model, inputs, return_outputs = False):

# 利用模型对喜欢的回复y_j计算奖励，输入IDs和注意力掩码由输入提供
rewards_j = model(input_ids = inputs["input_ids_j"], attention_mask = 
inputs["attention_mask_j"])[0]
# 类似地，为不喜欢的回复y_k计算奖励
rewards_k = model(input_ids = inputs["input_ids_k"], attention_mask = 
inputs["attention_mask_k"])[0]
# 使用负对数似然函数计算损失
loss = - nn.functional.logsigmoid((rewards_j - rewards_k) * torch.pow(torch.
tensor(inputs['score_diff'], device = rewards_j.device), 2)).mean()
# 如果将输出（y_j和y_k的奖励）和损失一起返回
if return_outputs:
    return loss, {"rewards_j": rewards_j, "rewards_k": rewards_k}
# 否则，只返回损失
return loss
```

奖励模型能够准确地为首选回复分配奖励，这对于强化学习的下一步至关重要。这需要定义两个模型，一个用于理解提问和回复的语义，另一个实现如何分别对首选和非首选的回复进行奖励和惩罚。

现在可以定义强化学习的训练循环，类似第7章中的相关做法。

## 8.3.3 从（期望的）人类反馈中进行强化学习
当在第7章尝试让FLAN - T5模型创建更多语法正确且中性的总结时，开始了从反馈中探索强化学习的话题。对于当前的例子，不会偏离这个框架太多。从技术上讲，这个循环的结构更简单，不会像第7章那样结合两个奖励模型，而只是使用自定义奖励模型。图8.14描述了这个强化学习的循环过程。

强化库（TRL）考虑奖励系统的奖励和与原始模型的差异来进行更新

GPT - 2对一批问题的回复

“人类反馈”奖励模型为指令 - 反应提供数值奖励

将生成的文本与原始LLM（在完成任何更新之前）生成的文本进行比较，以确保回复不会过于发散

![image](https://github.com/user-attachments/assets/01b7bf82-bf1f-4a32-bcb5-3500c88b4e94)


要获取完整的代码，请查看本书的代码库。鉴于它与第7章中的RL代码几乎完全相同，此处跳过了重复部分。
## 8.3.4 结果总结

这里并没有在模型的每一个环节展示其结果。因为在现实中，必然是先实现和搭建模型管线，才能看到模型运行的结果，因此在实现每一个环节之前，了解这个环节的实现过程是很有必要的。在这里给出实现过程，如果每个独立的组件都能表现良好，它应该获得想要的结果：一个相对合理的指令微调模型。图8.15描述了SAWYER的各部分是如何学习的。

第1步：监督指令微调的损失函数值从90多下降到2.1

第2步：奖励模型训练仅用了一个周期，损失就从~68降至~17（左），并且（选择首选回复的）准确度略低于98%（右）。图中展示的是2个周期的训练结果，不过在第一个周期后就使用了该模型

第3步：强化学习。该图显示了2个周期后给出的奖励，并最终在1250步的断点采纳了该模型，因为此处具有最高的奖励

![image](https://github.com/user-attachments/assets/23b5de3b-d0e7-4010-ba5b-e6627b96800d)


总地来说，考虑到该任务中包含了自定义损失和自定义RLF循环，SAWYER似乎已经准备好回答一些问题，所以进一步实用它做一些尝试。图8.16展示了模型的几次运行情况。

图8.16中SAWYER表现良好。这里要求它为虚构人物写一个背景故事（图8.16（a）），并重写句子“求职是一个缓慢且乏味的过程”（图8.16（b））。与Vanilla GPT - 2和GPT - 2+监督相比，SAWYER（监督+强化）虽然没有强化，但表现良好。

在试用SAWYER时，很容易发现奖励模型的表现明显不如预期。图8.17中给出了一些突出的例子。

图8.17中，当提问“上面”的反义词是什么时，SAWYER确实给出了正确的答案，但更简洁的答案被给予了负奖励（图8.17（a））。当笔者问谷歌是什么（图8.17（b））时，由于某种原因，没有使用强化学习的版本给出了看似正确的答案，但被给予了非常负面的奖励。

SAWYER准备好取代GPT - 4了吗？没有。SAWYER准备好作为通用问答人工智能投入生产了吗？没有。有可能采用小型开源模型，并创造性地做些什么吗？是的。图8.18显示了SAWYER的一些明显失败的案例。

![image](https://github.com/user-attachments/assets/1fd2e6bf-fc20-4a3d-ae3d-65f6ba6f6fbd)


图8.18中，尽管没有强化学习的版本可以告诉我普林斯顿大学的位置（图8.18（a）），但SAWYER却无法告诉我普林斯顿大学的位置。当询问德国现任总理是谁时，它也说了一些疯狂的话（图8.18（b））。请注意，实际正确答案的奖励都是负数，这是对奖励模型的另一个打击。


这里对“谁是德国现任总理”问题提出两点意见。第一点是人工智能是否得到了答案。在撰写本书时，奥拉夫·朔尔茨是现任总理，这突出了大模型训练数据的截止时间局限性。一个典型而又容易被忽略的问题是，大模型可能涉及政治的敏感问题，如“大模型可能会讨论希特勒”，大模型对于这种问题的回复也是不足为奇的。这是一个典型的例子，说明LLM可能会出现意料之外的输出。

![image](https://github.com/user-attachments/assets/39575fdb-523d-43bd-958f-bd1afb97a7a4)


这种潜在的风险可能源于GPT - 2的预训练数据，包括从各种来源（如Reddit）抓取的大量信息。Reddit虽然是一个丰富多样的信息来源，但也包含容易误导和虚假的信息。这些数据可能在预训练期间被模型吸收，从而导致模型会输出具有风险性的结果。

这些类型与预期的偏差突显了严谨的模型训练和验证的必要性，同时也强调了监控用于预训练的输入数据质量的重要性，以及对模型输出进行持续验证和测试的必要性。

![image](https://github.com/user-attachments/assets/e90a196e-7e0d-41e5-b8ac-9ffc6a30584d)


![image](https://github.com/user-attachments/assets/7b51b126-b392-4ba4-a880-7156df7540c2)


# 8.4 日新月异的微调世界

值得注意的是，在本书探索微调LLM的领域时，微调方面的创新永远不会停止，新的微调方法会不断出现，每个方法都为改进和优化本例中的模型和训练流水线提供了新的机会。

例如，近年来，一种吸引LLM工程师注意力的有趣技术是PEFT LoRA。这种方法巧妙地将以下两种策略结合在一起。

参数高效微调（Parameter - Efficient Fine - Tuning，PEFT）通过冻结大部分预训练的权重并仅在侧面添加少量额外权重，大大减少了LLM中可调参数的数量。

低秩自适应（Low - Rank Adaptation，LoRA）通过将PEFT的补充权重分解为紧凑的低秩矩阵，进一步减小附加权重。

PEFT和LoRA结合的优势在于在不牺牲太多性能的情况下，实现更灵活和优化的LLM微调，大大减少训练时间和内存需求。

本书代码库中提供一个PEFT LoRA示例，后续可能进一步对这个例子做升级。值得注意的是，本章讲述的策略是大模型优化的基础性原则，已经足够强大了。新策略通常只是优化现有的过程，调整相对较少，充分利用了前面章节中讲解的内容。尽管PEFT和LoRA提供了提高效率的途径，但微调LLM的基础性原则在很大程度上是保持不变的。

# 8.5 本章小结

本章检验了开源LLM的众多应用和改进方法，深入研究了它们的优缺点，强调了潜在优化方向，同时讲解了从模型的微调到模型在现实世界的应用，以及展示了一系列语境中LLM的多种能力和可扩展性。

在BERT微调来实现分类的案例中，突出即使是简单的任务也可以通过冻结、梯度累积和语义采样等技术进行优化。对这些技巧进行细致的权衡可以提高模型的性能。

当微调这些模型时，微调模型的可控和可自定义的空间是巨大的，可使它们适应各种任务和不同领域。

在LaTeX公式生成案例中再次验证，LLM在经过良好调优后，即使在数学符号等领域，也能生成有意义且符合上下文的输出。

在SAWYER案例中可以发现，对于参数量较小的大模型，如1.2亿规模的大模型一样可以提供不错的效果，尽管存在一些古怪之处。该系统在多个任务上令人惊讶的熟练程度证明了LLM的巨大潜力和微调策略的价值。然而，不符合预期的甚至错误的输出也在提示大家，在改进这些大模型时存在的挑战，以及充分验证和测试这些大模型的重要性。

本章深入讲解开源LLM的复杂性，同时讲解了它们令人难以置信的灵活性，以及被广泛地应用、微调和部署这些模型需要考虑的众多因素。

虽然旅程充满了挑战，但可以让读者的学习收获颇丰，并开辟了改进的途径，让大家对LLM的未来充满乐观的期待。第9章将讲解如何与世界分享自己的优秀工作，这样受益者就不会仅仅局限为我们自己。 
