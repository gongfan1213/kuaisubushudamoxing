元注入，旨在表示整个输入序列的编码语义含义。

读者可能熟悉传统NLP中使用的停用词删除、词干提取和截断等技术。这些技术对于LLM来说既不会使用，也不是必要的。LLM旨在处理人类语言的固有复杂性和可变性，包括使用the和an等停用词，以及时态和拼写错误等单词形式的变化。使用这些技术处理输入文本并送入LLM，可能会减少上下文信息并改变文本的原始含义，从而损害模型的性能。

词元化的过程可能涉及一些类似首字母大小写的预处理步骤。区分两种大小写：无大小写和大小写。在无词性标注中，所有词元都是小写的，并且通常会去掉字母的重音。在词性词元化中，保留词元的大小写。大小写的选择会影响模型的性能，因为大小写可以提供有关词元含义的重要信息。如图1.13所示，无词性词元和有词性词元的选择取决于任务。文本分类等简单任务通常更倾向于无词性词元，而命名实体识别等从词性中获取意义的任务则更倾向于有词性词元。

![image](https://github.com/user-attachments/assets/7efb3f7a-a291-4fbb-bf37-c50b0b759963)


注意

即使大小写词元的概念也带有一些偏见，具体取决于模型。处理大小写，即实现小写和去除重音，通常是西式的预处理步骤。笔者说土耳其语，所以知道变音符号（例如，笔者姓氏中的Ö）很重要，实际上可以帮助LLM理解土耳其语中的单词。任何没有在各种语料库上充分训练的语言模型都可能难以解析和利用这些语境。

图1.14展示了词元化的一个例子，即LLM如何处理词汇表外（OOV）的短语。OOV短语只是LLM无法识别为词元的短语/单词，所以在BERT中，词元化方案将把笔者的名字（Sinan）在大多数LLM中都不是一个词元，所以在BERT中，词元化方案将把笔者的名字分成两个词元（假设不区分大小写）：

• Sin：笔者名字的第一部分。

• ##an：一个特殊的子词词元，与单词an不同，仅用作分割未知单词的手段。

![image](https://github.com/user-attachments/assets/e697ffb9-0f04-4b1b-a433-fa64a6dc4b6f)


如图1.14所示，每个LLM都必须处理以前从未见过的单词。如果我们考虑为什么有这个限制，了解LLM如何对文本进行词元化就很重要。在BERT中，“子词”用前面的“##”表示，表明它们是单个单词的一部分，而不是新单词的开头。这里的词元“##an”与单词an完全不同。

一些LLM限制了用户一次可以输入的词元数量。如果我们在意这个限制，LLM如何对文本进行词元化就非常重要。

到目前为止，已经讲解了很多关于语言建模的内容，即预测短语中缺失的词元或下一个词元。然而，现代LLM还可以借鉴其他人工智能领域，使模型更高效，更与人类语言对齐——这意味着人工智能正在按照人类的期望进行。换句话说，与人类语言对齐的LLM有一个与人类目标相匹配的目标。

### 7. 超越语言建模：对齐+RLHF

语言模型的对齐是指模型对符合用户期望的输入进行提示的反应程度。标准的语言模型根据之前的上下文预测下一个单词，但这可能会限制它们对特定指令或提示的有效性。研究人员正在提出可扩展且高性能的方法来将语言模型与用户意图对齐。一种广泛使用的对齐语言模型的方法，是将强化学习（RL）纳入训练循环。

人类反馈强化学习（RLHF）是一种流行的对齐预训练LLM的方法，它利用人类反馈来增强模型性能。允许LLM从相对较小、高质量的批量人类反馈中学习其自身输出，从而克服传统监督学习的一些局限性。人类反馈强化学习在ChatGPT等现代LLM中取得了显著进步。这是使用强化学习进行对齐的一种方法，同时还出现了其他对齐方法，如具有AI反馈的强化学习（如constitutional AI）。将在后面的章节中详细讲解与强化学习结合的一些流行的LLM。

现在将介绍在本书中使用的一些流行的LLM。

# 1.2 当前流行的大模型

GPT、BERT和T5是OpenAI和谷歌公司分别开发的三种流行的LLM。尽管这些模型都以Transformer为共同祖先，但它们的架构差异很大。Transformer家族中其他广泛使用的LLM变体还包括RoBERTa、BART（我们之前使用它进行了一些文本分类）和ELECTRA。

## 1.2.1 BERT

如图1.15所示，BERT是首批LLM之一，在涉及大量文本快速处理的许多NLP任务中仍然很受欢迎。BERT是一种自编码模型，使用注意力机制来构建句子的双向表示。这种方法使其成为句子分类和词元分类任务的理想选择。

![image](https://github.com/user-attachments/assets/bb2808da-ce9b-4a26-ada4-675426f8c7dd)


BERT使用Transformer的编码器，忽略解码器，相对于其他一次生成一个词元的较慢的LLM，它非常擅长快速处理/理解大量文本。因此，当不需要编写自由文本时，BERT衍生的架构最适合快速处理和分析大型语料库。

BERT本身并不对文本进行分类或总结文档，但它通常被用作下游NLP任务的预训练模型。BERT已成为NLP社区中广泛使用和高度重视的LLM，为开发更高级的语言模型铺平了道路。

## 1.2.2 GPT-3和ChatGPT

如图1.16所示，GPT模型家族擅长生成与用户意图一致的自由文本。GPT与BERT相反，是一种自回归模型，它使用注意力机制来预测序列中下一个基于先前词元的词元。GPT算法家族（包括ChatGPT和GPT-3）主要用于文本生成，并以其生成自然、类似人类的文本能力而闻名。

GPT依赖于Transformer的解码器部分，忽略了编码器，因此它非常擅长一次生成一个词元的文本。基于GPT的模型适合在较大的上下文窗口中生成文本。还可以用于处理/理解文本。GPT派生的架构非常适合需要自由编写文本的应用程序。

![image](https://github.com/user-attachments/assets/61d51f22-9713-4ceb-9d48-e5cf9b6d3ba5)


## 1.2.3 T5

T5是一个纯粹的编码器/解码器Transformer模型，旨在执行多种NLP任务，从文本分类到文本摘要和文本生成都可以使用。事实上，它是第一个能够拥有如此功能的流行模型。在T5之前，像BERT和GPT-2这样的LLM通常必须使用词元数据进行微调，然后才能用来执行这些特定的任务。

T5同时使用了Transformer的编码器和解码器，因此在处理和生成文本方面都非常通用。基于T5的模型可以执行从文本分类到文本生成等范围很广的NLP任务，因为它们能够使用编码器构建输入文本的表示，并使用解码器生成文本（图1.17）。T5是首批在无须任何微调的情况下同时解决多个任务的LLM之一。源自T5的架构非常适合“既需要处理和理解文本，又需要自由生成文本”的应用。

![image](https://github.com/user-attachments/assets/7aa50bb8-1087-472a-8e14-c164ce5d1a40)


T5无须微调即可执行多项任务的能力推动了其他多功能LLM的开发，这些LLM可以高效准确地执行多项任务，几乎不需要微调或简单微调。与T5同时发布的GPT-3也拥有这种能力。

这三个LLM——BERT、GPT和T5——功能非常强大，可用于各种NLP任务，如文本分类、文本生成、机器翻译和情感分析等。这些LLM及其变体将是本书的主要焦点。

## 1.3 垂直领域大模型

垂直领域的大模型（LLM）是在特定学科领域（如生物学或金融学）接受过训练的LLM。与通用LLM不同，这些模型旨在理解其训练领域内使用的特定语言和概念。

垂直领域LLM的一个例子是BioGPT（图1.18），BioGPT是一种特定领域的Transformer模型，在大型生物医学文献上进行了预训练。BioGPT在生物医学领域的成功启发了其他特定领域的LLM，如SciBERT和BioBERT。该模型由人工智能医疗公司Owkin与Hugging Face合作开发，在200多万篇生物医学研究文章的数据集上进行了训练，使其在广泛的生物医学NLP任务（如命名实体识别、关系提取和问答）中非常有效。BioGPT的预训练将生物医学知识和垂直领域的专业术语编码到LLM中，可以在较小的数据集上进行微调，使其适应特定的生物医学任务，减少对大量词元数据的需求。


![image](https://github.com/user-attachments/assets/5b3e6e75-5320-4512-9e69-689c0287e967)


使用垂直领域的LLM的优势在于它们在垂直领域的文本集训练过。这种相对狭窄但大量的预训练使其能够更好地理解特定领域使用的语言和概念，从而提高该领域内包含的NLP任务的准确性和流畅性。相比之下，通用LLM可能难以有效地处理特定领域使用的语言和概念。

# 1.4 大模型的应用


正如已经看到的，LLM的应用范围非常广泛，研究人员至今仍在不断挖掘LLM的新应用。本书大致以以下三种方式使用LLM。

（1）使用预训练的LLM的基本能力处理和生成文本，而不需要对LLM的部分结构做进一步微调。

- 示例：使用预训练的BERT/GPT创建信息检索系统。

（2）微调预训练的LLM，以使用迁移学习执行非常具体的任务。

- 示例：微调T5，以创建特定领域/行业的文档摘要。

（3）让一个预训练过的LLM解决或者合理地推测它预训练过的任务。

- 示例：提示GPT-3写一篇博客文章。

- 示例：提示T5进行语言翻译。


这些方法以不同的方式使用LLM。虽然它们都利用了LLM的预训练，但只有第二种方法需要微调。下面来看看LLM的一些具体应用。

# 1.4.1 经典的NLP任务

LLM的绝大多数应用在常见的NLP任务（如分类和翻译）中提供了最佳结果。并不是说在Transformer和LLM出现之前没有解决这些任务，只是现在开发人员和从业者可以用相对较少的词元数据（由于Transformer在大型语料库上的高效预训练）来解决这些问题，并且具有更高的准确率。

## 1. 文本分类
文本分类任务为给定的文本片段分配一个标签。该任务通常用于情感分类，其目标是将一段文本分类为积极、消极或中性；或用于主题分类，其目标是将一段文本分类为一个或多个预定义的类别。像BERT这样的模型可以被微调，以使用相对较少的词元数据进行分类，如图1.19所示。

![image](https://github.com/user-attachments/assets/f9fc1ffe-190c-41b2-b6be-347f4e5410af)


图1.19是使用BERT实现快速准确的文本分类任务的架构。分类层通常作用于特殊的词元

![image](https://github.com/user-attachments/assets/d2186263-dd89-416c-bec7-12de16194c9e)
