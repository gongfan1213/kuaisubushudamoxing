### 第9章 将LLM应用于生产
随着LLM研究的不断成熟，将其部署到生产环境中变得越来越重要，由此可以与更多人共享自己的工作成果。本章重点讲解闭源和开源LLM部署时的不同策略的影响，并详细介绍模型服务管理的最佳实践、推理准备以及提高效率的方法，包括量化、剪枝和蒸馏等策略。

#### 9.1 闭源LLM应用于生产

对于闭源LLM，部署过程需要与提供该模型API服务的公司进行互动。因为其底层硬件和模型管理都被抽象化了，这种模型即服务的方法非常便捷。不过，这也需要对API密钥进行严格的管理。

1. **成本预测**

前面的章节中在某种程度上讨论了成本。简要回顾一下，闭源模型的成本预测主要涉及计算预期的API使用量，这通常是访问该模型的方式。这里的成本取决于提供者的计价方式，并且根据多种因素变化，包括以下内容。

- **API调用**：指应用程序对模型发出的请求数。供应商通常根据API调用的次数进行收费。
- **使用不同的模型**：同一家公司可能会以不同的价格提供不同的模型。例如，微调过的Ada模型会比标准Ada模型稍微贵一些。
- **模型/提示词版本**：如果模型提供者为不同版本的模型或提示词提供不同的定价，每个版本或提示词可能会有不同的费用。
准确预估这些成本需要对应用程序的需求和预期使用情况有清晰的了解。举例来说，进行连续、高容量的API调用的应用程序比低频、低容量调用的应用程序更高的成本。

2. **API密钥管理**

如果采用闭源LLM，会用到API，就可能需要管理一些API密钥。有几种较好管理API密钥的方式。首先，不要将密钥嵌入代码中，因为这样容易将其暴露在版本控制系统或意外共享的风险中。相反，应该使用环境变量或云安全密钥管理服务来存储密钥。

其次，为了最大程度地减少潜在的密钥泄露风险，建议定期更换API密钥。即便密钥不慎泄露，由于密钥只在短时间内有效，被盗用的窗口期也是有限的。

最后，建议使用具有最低权限的密钥。如果仅需要API密钥进行模型推理请求，则该密钥不应具有修改模型或访问其他云资源的权限。

#### 9.2 开源LLM应用于生产
部署开源LLM与闭源LLM的过程是不同的。部署开源LLM具有更多模型代码和代码部署的控制权。当然，这种控制权也带来了额外问题，一方面是模型推理部署难度增加，另一方面是推理速度一般降低，耗时一般会延长。

##### 9.2.1 将LLM应用于推理

虽然可以直接使用从生产中训练出来的原始模型，但是推理模型依然有很多优化空间。

一种通用且简易的方法是在PyTorch等深度学习框架中调用“.eval()”方法将模型转换为推理模式。eval模式禁用了一些较低级别的深度学习层，如Dropout和Batch Normalization层，它们在训练和推理期间的行为不同，从而使模型在推理期间具有确定性。程序清单9.1显示了如何通过简单的代码来执行“.eval()”调用。

**程序清单9.1：将模型设置为eval模式**
```python
trained_model = AutoModelForSequenceClassification.from_pretrained(
    f"genre - prediction",
    problem_type = "multi_label_classification",
).eval()
```
为了防止训练期间过拟合的层（如dropout层）在推理过程中处于启用状态，可以通过将一些激活值随机设置为0来实现。通过使用“.eval()”将模型设置为推理模式可以禁用这些层，可以确保模型的输出更具确定性，即在相同的输入下提供一致的预测结果。此外，禁用这些层还可以加快推理速度，并提升模型的透明性和可解释性。

##### 9.2.2 互操作性

模型具有互操作性（Interoperability）对开发人员是非常有益的，这意味着它们可以在不同的机器学习框架之间进行使用。实现这一目标的一种常见方法是使用ONNX（Open Neural Network Exchange，开放神经网络交换），这是一种开放的标准格式，用于表示机器学习模型。

ONNX提供一种机制，允许将模型从一个深度学习框架（如PyTorch）导出，然后导入到另一个深度学习框架（如TensorFlow）中进行推理。这种跨框架的兼容性对于跨环境或跨平台部署模型非常有用。它使得模型能够在各种框架之间无缝运行，为开发人员提供了更大的灵活性和选择性。程序清单9.2显示了使用Hugging Face的optimum包（一个用于使用加速运行（如ONNX runtime）构建和运行推理的实用程序包）的代码片段，以将序列分类模型加载到ONNX格式中。

**程序清单9.2：将基因预测模型转换为ONNX**
```python
#!pip安装优化
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
    f"genre - prediction - bert",
    from_transformers = True
)
```
假设在PyTorch中训练了一个模型，但希望在支持TensorFlow的平台上进行部署。在这种情况下，可以首先将模型转换为ONNX格式，然后将其转换为TensorFlow格式，而无须重新训练模型。这种转换过程可以帮助用户在不同的框架之间无缝地迁移模型，并在目标平台上进行推理，从而节省时间和资源。

##### 9.2.3 模型量化
量化（Quantization）是一种用于降低神经网络中权重和偏差精度的技术。可以使模型参数存储空间更小，推理速度更快，与此同时，会适当降低模型的性能。有几种不同类型的量化方法可供选择，包括动态量化（在运行时对权重进行量化）、静态量化（还涉及输入/输出值的缩放）以及量化感知训练，这几种训练方法在训练阶段考虑了量化误差。optimum包提供了一系列相关功能，用于支持量化模型的实现和处理。

##### 9.2.4 模型剪枝
模型剪枝是一种有助于减小LLM大小的技术。它通过去除神经网络中对模型输出贡献最小的权重来降低模型的复杂性。这种技术可以带来更快的推理速度和更小的内存占用，尤其适用于在资源受限的环境中部署模型。Hugging Face内置的optimum包提供了一系列相关功能，可帮助进行模型剪枝，从而优化模型的大小和性能。

##### 9.2.5 知识蒸馏

蒸馏是一种用于创建较小模型（学生模型）的过程，该模型试图模仿较大模型（教师模型）或模型集合的行为。蒸馏过程通过将教师模型的知识转移到学生模型中，产生一个更紧凑的模型，能够以更高效的方式运行。这对于在资源有限的环境中部署模型非常有益处。通过蒸馏，学生模型能够继承教师模型的知识和特征，从而在保持较高性能的同时，具有更小的模型尺寸和更高的运行效率。

1. **任务特定蒸馏和任务不特定蒸馏**

在本书的其他章提到过采用蒸馏的模型，例如，DistilBERT——BERT的蒸馏版本——为原始模型训练更快、更便宜的BERT替代方案。

通常采用蒸馏LLM的方法来获得计算复杂度更小的模型。假设有一个复杂的LLM，经过训练可以接受动漫描述作为输入，并输出相应的类型标签。现在希望训练一个更小、更高效的学生模型，使其能够生成类似的描述。有两种蒸馏方法可供选择，第一种是任务无关蒸馏（task-agnostic distillation），这种方法可以简单地使用标注数据从头开始训练学生模型（如DistilBERT），让它能够预测教师模型的输出，包括根据教师模型的输出和基本事实标签来调整学生模型的权重。通过这种方式，学生模型可以从教师模型中获得知识，并尽可能地生成类似的描述。第二种方法是针对特定任务的蒸馏（task-specific distillation）。在这种方法中，学生模型通过微调基本事实标签和教师模型的输出来适应特定任务。这样做的目的是通过提供多个知识来源来增强学生模型的性能。

图9.1展示了针对特定任务的蒸馏（顶部），通过在教师逻辑和任务数据上训练预训练的学生模型，将较大的微调教师模型提取为较小的学生模型。相反，任务无关蒸馏（底部）首先蒸馏未微调的模型，然后根据特定任务的数据对其进行微调。

这两种方法各有其优点，它们之间的选择取决于可用的计算资源、教师模型的复杂性和学生模型的性能要求等因素。下面将使用第8章中MyAnimeList动漫类型预测器执行特定任务蒸馏的例子。

2. **案例学习：蒸馏动漫类型预测模型**

在本例中，将使用Hugging Face的Trainer对象的自定义子类，以及两个新的超参数所需的自定义训练参数。程序清单9.3扩展了Trainer和TrainingArguments类，以支持知识蒸馏。该代码包含以下几个特点。
- **DistrictionTrainingArguments**：该类扩展了Transformers库的TrainingArguments类，添加了两个特定于知识蒸馏的额外超参数：α和温度。α是一个加权因子，用于控制原始任务损失（如分类任务的交叉熵损失）和蒸馏损失之间的平衡，而温度是一个超参数，用于控制模型输出的概率分布的“平滑”，较高的值会导致较平滑的分布。

![image](https://github.com/user-attachments/assets/164be860-b35d-41f8-b529-d8f691c3138e)


- **DistrictionTrainer**：该类扩展了Trainer类库，增加了一个新的论证教师模型，即学生模型从中学习的预训练的模型。

- **自定义损失计算**：在DistrictionTrainer的compute_loss函数中，总损失为学生原始损失和蒸馏损失的加权平均。蒸馏损失计算为学生模型和教师模型的软化输出分布之间的KL散度。

通过蒸馏，修改后的训练模型的类可以利用更大、更复杂的模型（教师）包含的知识来提高更小、更高效的模型（学生）的性能，即使学生模型已经针对特定任务进行了预训练和微调。


**程序清单9.3：定义蒸馏训练的参数和对象**
```python
from transformers import TrainingArguments, Trainer
import torch
import torch.nn as nn
import torch.nn.functional as F

# 定制TrainingArguments类，以增加指定蒸馏参数
class DistillationTrainingArgs(TrainingArguments):
    def __init__(self, *args, alpha = 0.5, temperature = 2.0, **kwargs):
        super().__init__(*args, **kwargs)

        # alpha是原始学生模型的权重
        # 更高的数值意味着对学生原始模型的更多关注
        self.alpha = alpha
        # 在计算分布损失前温度能缓和分布概率
        # 较高的数值可以使分布更统一，携带更多关于教师模型的信息
        self.temperature = temperature

# 定制Trainer类以执行知识蒸馏
class DistillationTrainer(Trainer):
    def __init__(self, *args, teacher_model = None, **kwargs):
        super().__init__(*args, **kwargs)

        self.teacher_model = teacher_model

        # 把教师模型移到与学生模型相同的设备中
        self._move_model_to_device(self.teacher, self.model.device)

        # 将教师模型设为eval模式，因为只用来作推理，不用作训练
        self.teacher.eval()

    def compute_loss(self, model, inputs, return_outputs = False):
        # 根据输入计算学生模型的输出
        outputs_student = model(**inputs)
        # 学生模型的原始损失(例如分类的交叉熵)
        student_loss = outputs_student.loss

        # 根据输入计算教师模型的输出
        # 教师模型不需要梯度，所以使用torch.no_grad避免不必要的计算
        with torch.no_grad():
            outputs_teacher = self.teacher(**inputs)

        # 检查学生模型和教师模型输出的尺寸
        assert outputs_student.logits.size() == outputs_teacher.logits.size()

        # KL散度损失函数，用于对比学生模型和教师模型缓和后的分布
        loss_function = nn.KLDivLoss(reduction = "batchmean")
        # 计算学生模型与教师模型之间的分布损失
        # 在计算损失之前，对学生模型的输出应用log_softmax，对教师模型的输出应用softmax
        # 这是由于对输入的对数概率和nn.KLDivLoss中的目标概率的期望
        loss_logits = (loss_function(
            F.log_softmax(outputs_student.logits / self.args.temperature, dim = -1),
            F.softmax(outputs_teacher.logits / self.args.temperature, dim = -1)) *
            (self.args.temperature ** 2))

        # 总损失是学生模型的原始损失和蒸馏损失的加权和
        loss = self.args.alpha * student_loss + (1. - self.args.alpha) * loss_logits

        # 依靠return_outputs参数，返回损失或损失+学生模型的输出
        return (loss, outputs_student) if return_outputs else loss
```

3. **温度系数的细节**

通常温度是一个超参数，用于控制概率分布的“平滑度”。在LLM中，温度系数被用来控制类似GPT的模型（“随机性”）。下面详述在知识蒸馏中温度的作用。

- **平滑分布**：Softmax函数用于将逻辑回归变换为概率分布。在应用Softmax函数之前，用logits除以温度能有效地“平滑”分布。更高的温度将使分布更加趋向均匀（即更接近所有类别的概率相等），而较低的温度将使其更趋向“峰值”（即最有可能的类别的概率较高，而其他类别的概率较低）。在蒸馏的背景下，较平滑的分布（较高的温度）携带了更多关于非最大类的相对概率的信息，这可以帮助学生模型更有效地从教师模型那里进行学习。

图9.2展示了温度对一组Softmax输出的影响。最左边的图图名为“原始Softmax Temp=1.0”，描述了使用默认温度1.0的Softmax概率。这些是类的原始Softmax值——例如，自回归语言建模时要预测的标题。中间的图“高温Softmax Temp=5.0”显示了温度设置为5.0的相对较高温度时的分布，这平滑了概率分布，使其看起来更加均匀。在一个语言建模示例中，这种效果使原本不太可能从原始数据中选择的词元更有可能被选择。对于人工智能产品，这种变化通常被描述为使LLM更具不确定性和“创造性”。最右侧的图形“低温Softmax Temp = 0.5”显示了温度设置为0.5的低温时Softmax功能的输出，产生了一个更“峰值”的分布，为最有可能的类分配了更高的概率，而其他类的概率明显更低。因此，该模型被认为更确定，也不那么“有创意”。

![image](https://github.com/user-attachments/assets/22757b59-34bc-47a5-9b69-c22c1b93f226)


- **损失函数中的温度平方项**：损失函数的KL散度部分包括一个温度平方项。此项可以视为蒸馏损失的缩放因子，它校正了逻辑回归除以温度引起的比例变化。如果没有这种校正，当温度较高时，反向传播过程中的梯度会更小，这可能会减慢训练速度。加入温度平方项后，无论温度值如何，梯度的尺度都更加一致。

- **除以损失函数中的温度**：如前所述，计算Softmax之前，用逻辑回归除以温度来平滑概率分布。这是分别针对损失函数中教师模型和学生模型的逻辑回归进行的。

温度用于控制在蒸馏过程中传递关于硬目标（如预测标签）和软目标（教师的预测）的知识之间的平衡。温度值需要谨慎选择，必要时可能需要在开发数据集上进行一些验证。

4. **执行蒸馏过程**

采用修改后的模型类函数进行训练比较简单。只需定义一个教师模型（BERT large-uncased模型）、一个学生模型（DistilBERT模型）以及词元器和数据标准器。此处词元化提要和词元IDs对于教师模型和学生模型是共享的。虽然可以将模型从一个词元空间转换到另一个词元空间，但这种方式比较复杂，这里选择更容易的方式。

程序清单9.4突出展示了训练所用的主要代码片段。

**程序清单9.4：执行蒸馏过程**

```python
# 定义教师模型
trained_model = AutoModelForSequenceClassification.from_pretrained(
    f"genre - prediction", problem_type = "multi_label_classification",
)
# 定义学生模型
student_model = AutoModelForSequenceClassification.from_pretrained(
    'distilbert - base - uncased',
    num_labels = len(unique_labels),
    id2label = id2label,
    label2id = label2id,
)

# 定义训练参数
training_args = DistillationTrainingArgs(
    output_dir = 'distilled - genre - prediction',
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    num_train_epochs = 10,
    logging_steps = 50,
    per_device_train_batch_size = 16,
    gradient_accumulation_steps = 4,
    per_device_eval_batch_size = 64,
    load_best_model_at_end = True,
    alpha = 0.5,
    temperature = 4.0,
    fp16 = True
)

distil_trainer = DistillationTrainer(
    student_model,
    training_args,
    teacher_model = trained_model,
    train_dataset = description_encoded_dataset["train"],
    eval_dataset = description_encoded_dataset["test"],
    data_collator = data_collator,
    tokenizer = tokenizer,
    compute_metrics = compute_metrics,
)

distil_trainer.train()
```
5. **蒸馏案例小结**
三种模型的对比结果如下。
- **教师模型**：采用BERT large-uncased模型，以预测流派为目标来训练模型。模型效果与之前的任务完全类似，因为使用了一个更大的模型，可以产生更好的结果。
- **任务不定蒸馏的学生模型**：采用BERT base-uncased


模型蒸馏出的BERT模型，采用训练教师模型相同的训练数据。

- **特定任务蒸馏的学生模型**：蒸馏的BERT模型，采用BERT base - uncased模型从教师模型进行知识蒸馏。与其他两个模型采用相同的数据，但从两个方面进行判别——其一是实际任务的损失，其二是教师和学生模型损失的差异（KL散度）。

图9.3展示了在10个周期内训练三个模型的Jaccard分数（越高越好的度量）。可以看到，特定任务的学生模型优于任务无关的学生模型，甚至比早期的教师模型表现更好。教师模型在Jaccard相似性方面仍然表现最好，但这不是唯一的衡量指标。

一般的预测任务的性能可能不是唯一关心的问题。图9.4突出展示了特定任

务模型的最好Jaccard分数、运行一组16项所花时间以及模型的内存使用情况。可以看到，虽然BERT large - uncased模型在Jaccard分数上略胜一筹，但特定任务DistilBert模型在运行时间和内存使用方面表现出色，这对于资源受限的环境或对响应时间有要求的应用程序来说至关重要。 

![image](https://github.com/user-attachments/assets/7a3c825c-34c1-4bfb-8fad-2780f4787615)

![image](https://github.com/user-attachments/assets/92288f60-3bc8-498b-9e52-dfc6039c5f55)


![image](https://github.com/user-attachments/assets/d044b9ba-6466-4089-bbf0-8b49a19fbcfc)





 
