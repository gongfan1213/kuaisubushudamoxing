```python
    return decoder_outputs
```

本章定义一个模型并针对交叉注意力进行适当调整,下面来看看增强引擎动力的数据。
### 7.1.5 数据: 视觉问答
数据集来自Visual QA网站(https://visualqa.org),一个关于图像的开放式问答数据集的网站,如图7.6所示。包含关于图像的开放式问答,这些问题由人工标注答案。旨在产生需要理解视觉、语言和常识才能回答的问题。


![image](https://github.com/user-attachments/assets/5bbdae65-87ad-417d-b7cc-872772a4d815)


下面为模型解析数据集,程序清单7.3编写了一个函数,用于解析视觉问答文件并创建一个数据集,可以将其与Hugging Face的Trainer对象一起使用。

**程序清单7.3: 解析视觉问答文件**
```python
# 从给定的注释和问题文件中加载VQA数据的函数
def load_vqa_data(annotations_file, questions_file, images_folder, start_at = None, end_at = None, max_images = None, max_questions = None):
    # 加载注释和问题的JSON文件
    with open(annotations_file, "r") as f:
        annotations_data = json.load(f)
    with open(questions_file, "r") as f:
        questions_data = json.load(f)

    data = []
    images_used = defaultdict(int)
    # 创建一个question_id与注释数据映射的字典
    annotations_dict = {annotation["question_id"]: annotation for annotation in annotations_data["annotations"]}

    # 在指定的范围内对问题进行迭代
    for question in tqdm(questions_data["questions"][start_at:end_at]):
        # 检查图像文件是否存在,是否达到max_questions题数限制

        # 把数据作为字典加入
        data.append(
            {
                "image_id": image_id,
                "question_id": question_id,
                "question": question["question"],
                "answer": decoder_tokenizer.bos_token + '' + annotation["multiple_choice_answer"] + decoder_tokenizer.eos_token,
                "all_answers": all_answers,
                "image": image,
            }
        )
        # 如果达到max_images的限制,跳出循环

    return data


# 加载训练和验证VQA数据
train_data = load_vqa_data(
    "v2_mscoco_train2014_annotations.json", "v2_OpenEnded_mscoco_train2014_questions.json", "train2014",
)
val_data = load_vqa_data(
    "v2_mscoco_val2014_annotations.json", "v2_OpenEnded_mscoco_val2014_questions.json", "val2014"
)

from datasets import Dataset

train_dataset = Dataset.from_dict({key: [item[key] for item in train_data] for key in train_data[0].keys()})

# 为了后面的返回,可以选择把数据集保存在硬盘
train_dataset.save_to_disk("vqa_train_dataset")

# 创建评估数据集
val_dataset = Dataset.from_dict({key: [item[key] for item in val_data] for key in val_data[0].keys()})

# 为了后面的返回,可以选择把数据集保存在硬盘
val_dataset.save_to_disk("vqa_val_dataset")
```
### 7.1.6 VQA训练迭代
在这个案例研究中的训练与之前章节中的训练没有什么不同。说实话,大部分艰苦的工作都是在数据解析环节完成的。通过Hugging Face的Trainer和TrainingArguments对象与自定义模型一起使用,训练过程可以简单地归结为验证损失。完整的代码可以在本书的代码库中找到,程序清单7.4中显示了一个代码片段。


**程序清单7.4: VQA的训练循环**
```python
# 定义模型的结构
DECODER_MODEL = 'gpt2'
TEXT_ENCODER_MODEL = 'distilbert - base - uncased'
IMAGE_ENCODER_MODEL = 'facebook/dino - vitb16' # 来自Facebook的一个ViT版本

# 用指定的结构初始化MultiModalModel
model = MultiModalModel(
    image_encoder_model = IMAGE_ENCODER_MODEL,
    text_encoder_model = TEXT_ENCODER_MODEL,
    decoder_model = DECODER_MODEL,
    freeze = 'nothing'
)

# 构建训练参数
training_args = TrainingArguments(
    output_dir = OUTPUT_DIR,
    optim = 'adamw_torch',
    num_train_epochs = 1,
    per_device_train_batch_size = 16,
    per_device_eval_batch_size = 16,
    gradient_accumulation_steps = 4,
    evaluation_strategy = "epoch",
    logging_dir = "./logs",
    logging_steps = 10,
    fp16 = device.type == 'cuda', # 这会节省有GPU功能的计算机的内存
    save_strategy = 'epoch'
)

# 用model、训练参数、数据集初始化Trainer
Trainer(
    model = model,
    args = training_args,
    train_dataset = train_dataset,
    eval_dataset = val_dataset,
    data_collator = data_collator
)
```
这个例子的完整代码很多,如前所述,强烈建议查看本书代码库,以获取完整的代码和注释。
### 7.1.7 结果总结
图7.7显示了一个图像样本,其中包含对新开发的VQA系统提出的一些问题。注意,其中一些回答不止一个词元,这是将LLM作为解码器而不是标准VQA系统那样输出单个词元的直接好处。


![image](https://github.com/user-attachments/assets/e23ad1c5-3633-4855-93e9-ddc59fd9962f)



图7.7中,VQA系统在回答关于图像的样本问题方面并不差,即使使用了相对较小的模型(就参数数量而言,特别是与当今最先进的系统相比)。每个百分比是GPT - 2在回答给定问题时生成的词元预测概率。显然,它有一些问题答错了。通过在更多数据上进行训练,可以进一步减少错误数量。

这只是数据的一个样本,并不能客观地反映性能。为了展示模型训练过程,如图7.8所示,仅仅一个训练周期后,VQA系统验证损失值就大幅下降。

模型远非完美。它需要更先进的训练策略和更多的训练数据,才能达到最优性能。即便如此,使用免费数据、免费模型和(主要是)免费计算能力(笔者自己的笔记本电脑),也可以产生一个不错的VQA系统。


![image](https://github.com/user-attachments/assets/143e4004-15b5-4f94-a96d-987f087efdce)


暂时抛开纯粹的语言建模和图像建模,接下来将探索利用该方法的近亲——强化学习来微调语言模型的新方法。
