### 第2章 大模型语义检索
#### 2.1 简介

第1章中讲解了大模型的内部运作机制以及现代LLM对文本分类、文本生成和机器翻译等经典NLP任务的冲击。语义检索（LLM的一个强大应用场景）也得到了越来越多的关注。

现在读者可能是快速掌握ChatGPT和GPT - 4对话的最佳时机。与此同时，我想向读者展示在这个新颖的Transformer架构之上还能构建的应用。像GPT等文本到文本的模型，其结构本身设计得非常精妙，在业界大模型最广泛的落地方案之一是依靠强大的LLM来实现文本生成的有效的嵌入表示。

文本嵌入是一种基于上下文语义的将单词或短语表示为机器可读的高维空间中数值向量的方法。其思想是，如果两个短语相似（在本章稍后更详细地讲解“相似”一词），那么表示这些短语的向量应该在通过某种度量后（如欧氏距离）紧密相连，反之亦然。图2.1显示了一个简单的语义检索的向量空间的示例。图2.1表示相似语义的向量应更加靠近，不同语义的向量应更加远离。在这种情况下，例如他们可能想要“一张老式魔术卡”。一个合格的语义检索系统应该以这样的方式进行查询：即使它们共享某些关键字，它最终只会靠近语义相似的结果（如“魔术卡”）并远离非相关内容（如“老式魔术工具包”）。当用户搜索想要购买的物品时，比如魔术卡交易卡，但他们可能只是搜索“一张老式魔术卡”。然后，检索系统应该将这个查询转化为嵌入向量表示，这样如果两个文本嵌入彼此相似，则表明用于生成它们的短语应该是相似的。

![image](https://github.com/user-attachments/assets/3606e168-e394-4ed8-be47-9783f1a726a0)


将文本转化为向量的过程可以看作是一种具有特殊意义的哈希映射。但我们不能将向量反向还原为文本。它是一种文本的表示方式，具有通过数值计算进行比较的特性。

基于LLM的文本嵌入使人们能够挖掘单词和短语的语义价值，而不仅仅是表面语法或拼写。人们可以利用丰富的语料数据，对LLM进行预训练和微调，以搭建各种功能的应用程序。

本章介绍如何使用LLM进行语义搜索，探讨如何使用LLM搭建强大的工具，实现信息检索和分析。利用本章知识，可以构建一个真实的语义检索系统，第3章将在GPT - 4的基础上构建一个聊天机器人。


#### 2.2 语义检索的任务

传统的搜索引擎通常会依据用户输入的内容，给用户一堆包含刚才输入的单词或字符排列的网站或链接。因此，如果用户输入“老式魔术卡”，搜索结果将返回标题/描述中包含这些词的组合。这是一种非常标准的搜索方式，但搜索结果并不总令人满意。例如，用户可能会得到老魔术工具包来帮助学习如何从帽子里拉出兔子。很有趣，但不是用户想要的。

用户在搜索引擎中输入的词语可能并不总是与检索结果出现的词语语义一致。如果查询的词语太过宽泛，将会检索出一系列无关的结果。这个现象不仅仅是检索结果在语句上的不一致；由于一些词具有多义性，相同的单词可能具有与搜索内容不同的含义。这就是语义搜索发挥作用的地方，正如前面提到的“魔术卡”描述的类似场景。

语义搜索系统可以理解用户的搜索词的含义以及语境与检索得到的文档的含义和语境是匹配的。语义搜索系统可以在数据库中找到相关结果，而不必依赖精确的关键字或n - gram匹配；相反，它依赖于预训练的LLM来理解搜索词和检索获得的文档的细微差别。如图2.2所示，基于关键词的传统搜索可能会将一个老式魔术工具包与实际想要的结果进行同等权重排序，而语义搜索系统可以理解我们正在搜索的实际含义。

![image](https://github.com/user-attachments/assets/41745b26-9ee1-4384-8b07-115eec9530ee)


非对称语义搜索的非对称部分是指输入查询的语义信息与搜索系统必须检索的文档/信息之间存在不平衡（一般指语义长度），其中一个比另一个短得多。例如，一个搜索系统试图将“魔术卡”与市场上的冗长项目描述段落进行匹配，这就是非对称搜索。三个字的搜索查询比段落的信息少得多，但仍然是我们必须比较的。

即使用户没有在搜索中使用完全正确的单词，非对称语义搜索系统也可以产生非常准确和相关的搜索结果。它们依赖于LLM的知识，而不是让用户去大海捞针。

当然，笔者大大简化了传统方法。在不切换到更复杂的LLM的情况下，仍然有许多方法可以提高搜索性能，不是只有纯语义搜索系统这种办法。它们不仅是“更好的搜索方式”这么简单。语义算法有其自身的不足，包括以下方面：

- 语义算法可能对文本中的微小变化过于敏感，例如大小写或标点符号的差异。

- 语义算法努力理解微妙的概念，如讽刺或反讽，这些概念依赖于当地的文化习俗。

- 与传统关键字匹配算法相比，语义算法在计算上可能更昂贵，尤其是在启动一个包含许多开源组件的自研系统时。

语义搜索系统在某些情况下可以成为一种有价值的工具，下面讲解如何构建解决方案。

#### 2.3 非对称语义检索方案概述

非对称语义搜索系统的总体流程遵循以下步骤。

1. **第一步：文档嵌入**

如图2.3所示，存储文档将包括对文档进行一些预处理，如文档嵌入，然后将文档存储在某些数据库中。

    - 收集需要做嵌入表示的文档，例如物品的描述段落。
    - 创建文本嵌入，对语义信息进行编码。
    - 将嵌入存储在数据库中，以便在用户触发查询后进行检索。

![image](https://github.com/user-attachments/assets/9fd86a64-01ef-498b-96f5-43769adb3247)


2. **第二步：检索文档**

如图2.4所示，在检索文档时，用户必须使用与文档相同的嵌入方案进行查询，将它们与之前存储的文档进行比较，然后返回最相似的文档。
    - 检索系统接收清洗过或预处理过的用户查询信息（例如用户搜索一个物品）。
    - 通过嵌入相似性（如欧氏距离）检索候选文档。
    - 必要时对候选文件进行重排。
    - 将最终搜索结果返回用户。

#### 2.4 组件
下面详细讲解每个组件，以方便读者了解下一步计划以及需要考虑的因素。

1. **2.4.1文本嵌入器**

语义搜索系统的核心都是文本嵌入器。该组件接收整个文本文档或单个单词或短语，并将其转换为向量。该向量对于该文本是唯一的，并且能捕获短语上下文的意义。


文本嵌入器的选择至关重要，因为它决定了文本向量表示的质量。用户有很多选择，可以使用开源和闭源的LLM作为嵌入器进行文本向量化。为了更快地起步，书中使用OpenAI的闭源Embeddings产品来达成目标。在后面的章节中，笔者将介绍一些开源选项。

OpenAI的Embeddings是一个强大的工具，可以便捷地提供高质量的向量，但它是一个闭源产品，这意味着用户对它的实现和潜在风险的控制有限。特别在使用闭源产品时，用户可能无法访问底层算法，这使得难以排查出现的任何问题。
    - **是什么让文本片段“相似”**

一旦将文本转换为向量，就必须找到一种数学表示来计算文本片段是否“相似”。余弦相似度是一种衡量两个事物相似度的方法。它着眼于两个向量之间的角度，并根据它们在方向上的接近程度给出分数。如果向量指向完全相同的方向，则余弦相似度为1。如果垂直（相距90°），则为0。如果指向相反的方向，则为 - 1。

图2.5是采用余弦相似度实现用户检索文档的示意图。在理想的语义搜索场景中，余弦相似度为用户提供了一种高效率的方法来大规模地比较文本片段的相似性，包括搜索词的所有文本都要转化为嵌入向量表示，然后计算它们之间的角度。角度越小，余弦相似度就越大，搜索的结果就相似。

也可以使用其他相似性度量，例如点积或欧氏距离。但是，OpenAI嵌入有一个特殊属性，它们的向量的幅度被归一化为1，这意味着：

        - 余弦相似度与点积相同。
        - 余弦相似度和欧氏距离将得到相同的排序。

归一化向量（所有向量的幅度均为1）很有用，因为可以使用简单的余弦计算来查看两个向量之间的距离，从而通过余弦相似度来查看两个短语在语义上的接近程度。

![image](https://github.com/user-attachments/assets/70e2b381-078c-40cc-94b6-c88ff0a0308e)


    - **采用OpenAI的嵌入器构建检索引擎**

如程序清单2.1所示，写几行代码就能获取OpenAI的嵌入。整个系统依赖于一种嵌入机制，该机制将语义相似的项目放置在彼此附近，使得当项目实际上相似时，余弦相似度较大。可以使用多种方法中的任何一种来创建这些嵌入，下面将使用OpenAI的嵌入引擎来完成这项工作。该引擎是OpenAI提供的多种嵌入机制的一种。我们使用最新的引擎，该引擎适用于大多数用例。

程序清单2.1：从OpenAI获取文本嵌入

```python
# 为要运行的指令导入必要的模型
import openai
from openai.embeddings_utils import get_embeddings, get_mebedding

# 使用存储在环境变量中的数据设置OpenAI的API关键字
'OPENAI_API_KEY'
openai.api_key = os.environ.get('OPENAI_API_KEY')

# 设置文本嵌入要使用的引擎
ENGINE = 'text-embedding-ada-002'

# 用指定的引擎为给定文本生成向量表示
embedded_text = get_embedding('I love to be vectorized',engine = ENGINE)

# 检查结果向量的长度，确保是期望的大小(1536)
len(embedded_text) == '1536'
```

OpenAI提供了几个可用于文本的嵌入引擎选项。每个引擎可能提供不同的准确性，并可以针对不同类型的文本数据进行优化。在写作本书时，代码中使用的引擎是最新版本的，也是OpenAI官方推荐使用的。

此外，还可以同时将多段文本传递给get_embeddings函数，该函数可以在单个API被调用来为所有文本生成嵌入。这会比逐个为单独的文本多次调用get_embedding更有效。稍后读者将看到该案例。
    - **开源嵌入替代方案**

OpenAI和其他公司提供了强大的文本嵌入产品，同时也有一些开源的文本嵌入替代品供读者选择。一个热门的选择是使用BERT的双编码器，这是一种基于深度学习的强大算法，已被证明可以在一系列自然语言处理任务中产生最先进的结果。读者可以在开源库中找到预训练的许多双编码器，包括Sentence Transformers库，它为各种自然语言处理任务提供预训练的模型，以使用户使用。

双编码器包括两个BERT模型：一个用于编码输入文本，另一个用于编码输出文本（图2.6）。这两个模型在大型文本数据语料库上同时进行训练，目标是最大化输入和输出文本对应的相似度。最终的嵌入学习到了输入和输出文本之间的语义关系。

![image](https://github.com/user-attachments/assets/8c5c34f2-de73-4113-8e18-38bf61e955f3)


程序清单2.2：使用预训练的双编码器Sentence Transformer包来生成文本嵌入
```python
# 输入SentenceTransformer
from sentence_transformers import SentenceTransformer

# 使用multi-qa-mpnet-base-cos-v1预训练模型初始化SentenceTransformer模型
pre-trained model
model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')

# 定义一串未生成嵌入的文档
docs = [
    "Around 9 million people live in London",
    "London is known for its financial district"
]

# 为文档生成嵌入向量
doc_emb = model.encode(
    docs,  # 文档
    batch_size = 32,  # 嵌入向量的大小
    show_progress_bar = True  # 展示进度条
)

# 形状为(2,768)的嵌入向量表示生成了2个嵌入，每个嵌入长度为768
doc_emb.shape  # == (2,768)
```

此代码创建了SentenceTransformer类的一个实例，该类使用预训练模型multi - qa - mpnet - base - cos - v1进行初始化。该模型专为多任务学习而设计，特别是用于问答和文本分类等任务。它使用非对称数据进行预训练，因此可以用于短句检索和长文档，并且能够很好地对它们进行比较。使用SentenceTransformer类中的encode函数为文档生成向量嵌入，并将生成的嵌入存储在doc_emb变量中。

不同的算法可能在不同类型的文本数据上表现各异，并且具有不同的向量维度。算法的选择会对结果嵌入的质量产生重大影响。此外，开源产品可能需要比闭源产品更多的定制和微调，但它们也提供了更大的灵活性和对嵌入过程的控制。有关使用开源双编码器嵌入文本的更多示例，请查看本书的代码部分。

# 2.4.2文档分块

一旦建立了文本嵌入向量检索引擎，就需要考虑大型文档的嵌入表示带来的挑战。将整个文档作为一个单一的向量明显是不切实际的，特别是当处理书籍或研究论文等长文档时。解决这个问题的一个方法是将文档分块，即将大型文档划分为更小、更易于管理的块进行嵌入表示。

## 最大词元窗口分块**
一种文档分块的方法是最大词元窗口分块。这是最容易实现的方法之一，将文档分成固定尺寸大小的块。例如，如果用户设置一个词元的最大窗口尺寸为500，用户希望每个块都略小于500个词元。创建大小大致相同的块也将有助于使系统更加一致。

这种方法的一个常见问题是，用户可能无意识地切断了块之间的一些重要文本的关联性，从而割裂了上下文。为了缓解这个问题，可以设置重叠窗口，指定重叠的词元数量，以便在块之间共享词元。尽管会引入一种冗余感，但在高精度和低延迟要求的场景中是可以接受的。

下面来看一个使用一些样例文本进行重叠窗口分块的例子（程序清单2.3）。首先读取一个大型文档，使用笔者最近写的一本有400多页的书进行测试。

程序清单2.3：读取整本书

```python
# 使用PyPDF2库读取PDF文件
import PyPDF2

# 以只读二进制模式打开PDF文件
with open('../data/pds2.pdf', 'rb') as file:

    # 创建一个PDF reader对象
    reader = PyPDF2.PdfReader(file)

    # 初始化一个空的字符串来保存文本
    principles_of_ds = ''

    # 循环PDF文件的每一页
    for page in tqdm(reader.pages):

        # 从页面中提取文本
        text = page.extract_text()
        # 找到打算提取文本的起点
        # 本例中，我们将提取以“]”开头的字符串
        principles_of_ds += '\n\n' + text[text.find(']') + 2:]

    # 从生成的字符串中去掉任何前导或尾随空格
    principles_of_ds = principles_of_ds.strip()
```

下面对文档进行分块，通过给定词元数目来得到块的数目（程序清单2.4）。

程序清单2.4：对有重叠和没有重叠的书进行分块

```python
# 受openAI公司启发，这是一个把文本分隔成有最大词元数量限制的函数
def overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5);
    max_tokens: tokens we want per chunk
    overlapping_factor: number of sentences to start each chunk with that overlapping with the previous chunk

    # 根据标点分隔文本
    sentences = re.split(r'[.!?]',text)

    # 得到每个句子的词元数量
    n_tokens = [len(tokenizer.encode(''+ sentence)) for sentence in sentences]

    chunks, tokens_so_far, chunk = [],0,[]

    # 通过循环计算每一个词元组中的句子和词元
    for sentence, token in zip(sentences, n_tokens):

        # 如果当前块的词元数量加上当前句子的词元数量大于词元的最大数量，把块加到块列表中并且重置
        # 到目前为止的块和词元
        if tokens_so_far + token > max_tokens:
            chunks.append(".".join(chunk) + ".")
            if overlapping_factor > 0:
                chunk = chunk[-overlapping_factor:]
                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])
            else:
                chunk = []
                tokens_so_far = 0
            # 如果当前句子中的词元数量超过的最大词元数量的限制，执行下一句
            if token > max_tokens:
                continue
            # 否则把句子加到块中并且把词元的数目加到总数中
            chunk.append(sentence)
            tokens_so_far += token + 1

    return chunks

split = overlapping_chunks(principles_of_ds,overlapping_factor = 0)
avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)
print(f'non - overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')
non - overlapping chunking approach has 286 documents with average length 474.1 tokens

# 每块有5个重叠的句子
split = overlapping_chunks(principles_of_ds,overlapping_factor = 5)
avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)
print(f'overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')
overlapping chunking approach has 391 documents with average length 485.4 tokens
```
通过使用有重叠文档分块方法对文本分块，文本块的数量会有所增加，但是一般重叠句子个数不会很多，相比无重叠分块，总体词元的数量不会


有显著差异。重叠系数越高，引入系统的冗余就越多。然而，最大词元窗口的方法没有考虑到文档的自然结构，这可能导致一个完整信息被分割为具有重叠信息的两个或多个块，从而导致检索系统输出混淆性内容。
## 2.查找自定义分隔符
为了优化文档分块效果，用户可以自定义自然分隔符来分割文档，如PDF中的分页符或段落之间的换行符。对于给定的文档，依据自定义的分隔符可以识别文本中的自然空白，并使用这种分隔方式来创建更有意义的文本单元，将文本单元转化为向量嵌入来最终表示整个文档的向量嵌入。如图2.7所示，最大词元窗口 
