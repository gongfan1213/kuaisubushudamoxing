### 7.2 案例研究: 从反馈中强化学习

本书中将反复强调语言模型的卓越能力。通常处理的是相对客观的任务,如分类。当任务更主观时,如语义检索和动漫推荐,不得不花一些时间来定义一个客观的定量指标来指导模型的微调和整体系统性能。一般来说,定义什么是"好"的输出文本可能具有挑战性,因为它通常是主观的,并且依赖于任务或背景。不同的应用程序可能需要不同的"好"属性,如讲故事的创新性、总结的可读性或代码片段的代码功能。

当微调LLM时,必须设计一个损失函数来指导训练。但是设计一个能够捕捉这些主观属性的损失函数似乎很棘手,而且大多数语言模型仍然使用简单的下一个词元预测损失(自回归语言建模)来训练,如交叉熵。至于输出评估,一些指标被设计为更好地捕捉人类偏好,如BLEU或ROUGE;然而,这些指标仍然存在局限性,因为它们使用非常简单的规则和启发式方法将生成的文本与参考文本进行比较。可以使用嵌入相似性来比较输出与真实序列,但这种方法只考虑语义信息,除此之外还需要比较其他因素,例如,可能要考虑文本的风格。

但是,如果使用实时反馈(人工或自动)来评估生成的文本并作为性能指标,甚至作为损失函数来优化模型,那该怎么办? 这就是来自反馈的强化学习(RLF)——RLHF用于人工反馈,RLAIF用于人工智能反馈——发挥作用的地方。通过采用强化学习方法,RLF可以使用实时反馈直接优化语言模型,使在通用文本数据语

### 快速部署大模型：LLM策略与实践（基于ChatGPT等大

料库上训练的模型与细致入微的人类价值观更加一致。

ChatGPT是RLHF的首批重要应用之一。虽然OpenAI对RLHF做出了令人印象深刻的解释，但它并没有涵盖所有内容，所以本节内容用来填补其中的空白。

训练过程包括以下三个核心步骤（图7.9）。


![image](https://github.com/user-attachments/assets/1ff63f1b-bbf9-424d-90c8-39b7dd09590c)


在大语料库上对LLM进行预训练，以学习语法、一般信息、特定任务等

↓

定义并尽可能训练一个奖励系统，该系统可以是真人、一个根据人类偏好调整的模型，或者一个完全的AI系统（例如另一个LLM）

↓

使用强化学习更新LLM，使用奖励系统作为信号
#### 图7.9


**步骤1：预训练语言模型**。预训练语言模型涉及在大量文本数据（如文章、书籍和网站，甚至是一个精心策划的数据集）上训练模型。在此阶段，模型学习为一般语料库或任务生成文本。这个过程帮助模型从文本数据中学习语法、句法以及某种程度的语义。预训练期间使用的目标函数通常是交叉熵损失，用来衡量预测的词元概率和真实的词元概率之间的差异。预训练使模型对语言有了基本的了解，以后可以针对特定任务进行微调。

**步骤2：定义（训练）奖励模型**。在预训练语言模型之后，下一步是定义一个可以使用的奖励模型。评估生成文本的质量。这涉及收集人类反馈，例如不同文本样本的排名或分数，可用于创建人类偏好数据集。奖励模型旨在捕捉这些偏好，并作为监督学习问题进行训练，其目标是学习一个函数，将生成的文本映射到奖励信号（标量值），根据人类反馈表示文本的质量。奖励模型作为人类评估的代理，在强化学习阶段用于指导微调过程。

**步骤3：使用强化学习微调语言模型**。在预训练语言模型和奖励模型就绪的情况下，最后一步是使用强化学习技术微调语言模型。在这个阶段，模型生成文本接收奖励模型的反馈，并根据奖励信号更新其参数。目标是优化语言模型，使生成的文本与人类偏好紧密结合。在这种情况下使用的流行强化学习算法包括Proximal Policy Optimization（PPO）和Trust Region Policy Optimization（TRPO）。使用强化学习进行微调使模型能够适应特定任务，并生成更好的、反映人类价值和偏好的文本。

图7.9中，基于强化学习的LLM训练的核心步骤包括对LLM进行预训练、定义并可能训练奖励模型，以及使用该奖励模型从步骤1开始更新LLM。

第8章将完整地执行此过程。为了设置这个相对复杂的流程，笔者将讲述一个更简单的版本。在这个版本中，将使用现成的预训练LLM（FLAN-T5），使用已定义和训练过的奖励模型，并真正专注于步骤3，进行强化学习训练。
#### 7.2.1 FLAN-T5模型

之前见过并使用过FLAN-T5（在图7.10中取自原始FLAN-T5论文的图像中可见），因此本节实际上只是复习。


![image](https://github.com/user-attachments/assets/1f4537fe-8265-4c8d-b5cb-ec9f9ec56ad2)


**指令微调**

Please answer the following question. What is the boiling point of Nitrogen?

**微调思考链**

Answer the following question by thinking step - by - step. The cafeteria had 23 apples. if they used 20 for lunch and bought 6 more, how many apples do they have?

**多任务指导微调(1.8K tasks)**

Or Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.

**语言模型**

-320.4°F

The cafeteria had 23 apples originally. They used 20 for lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9.

Geoffrey Hinton is a British - Canadian computer scientist born in 1947. George Washington died in 1799. So they could not have had a conversation together. So the answer is "no".

**Inference: generalization to unseen tasks**

#### 图7.10

FLAN-T5是一种经过指令微调的开源编码器-解码器架构，FLAN-T5实际上是一个纯Transformer模型，这意味着它内置了经过训练的交叉注意力层，并提供指令微调的好处（如GPT-3.5、ChatGPT和GPT-4）。本节将使用该模型的开源“小”版本。

在第8章将执行自己的指令微调版本。现在从谷歌人工智能那里借用这个已经进行了指令微调的LLM，并继续定义一个奖励模型。

#### 7.2.2 奖励模型：情感和语法正确性

奖励模型必须考虑LLM的输出（在例子中是一系列文本），并返回一个标量（单个数字）奖励，该奖励在数值上表示对输出的反馈。这种反馈可以来自实际的人（这将非常耗时），甚至可以来自另一个语言模型或一个更复杂的系统，对潜在的模型输出进行排名，然后将这些排名转换为奖励。只要为每个输出分配一个标量奖励，这两种方法都可以产生可行的奖励系统。

在第8章将做一些非常有趣的工作来定义自己的奖励模型。不过，本章将再次依靠其他人的辛勤工作，并使用以下预构建的LLM。

- 来自cardiffnlp/twitter-roberta-base-sentiment LLM的情绪分散：这个想法的目标是促进本质中立的摘要，因此这个模型的奖励将被定义为“中性”类的logit值（logit值可以是负的，这是首选）。
- 来自textattack/roberta-base-CoLA LLM的“语法分数”：希望总结在语法上正确，因此使用该模型的分数应该促进更容易阅读的总结。奖励将被定义为“语法正确”类的对数似然值。

请注意，通过选择这些分类器作为奖励系统的基础，默认信任它们的性能。笔者查看了它们在Hugging Face模型库中的描述，以了解它们是如何训练的，以及笔者可以找到哪些性能指标。总地来说，奖励系统在这个过程中起着重要作用——因此，如果它们与用户实际的奖励文本序列的方式不一致，将遇到一些麻烦。

**程序清单7.5：定义我们的奖励系统**
```python
from transformers import pipeline
# 初始化CoLA pipeline
tokenizer = AutoTokenizer.from_pretrained('textattack/roberta - base - CoLA')
model = AutoModelForSequenceClassification.from_pretrained("textattack/roberta - base - CoLA")
cola_pipeline = pipeline('text - classification', model = model, tokenizer = tokenizer)
# 初始化语义分析pipeline
sentiment_pipeline = pipeline('text - classification', 'cardiffnlp/twitter - roberta - base - sentiment')
# 得到一列文本的CoLA得分的函数
def get_cola_scores(texts):
    scores = []
    results = cola_pipeline(texts, function_to_apply = 'none', top_k = None)
    for result in results:
        for label in result:
            if label['label'] == 'LABEL_1': # 好的语法
                scores.append(label['score'])
    return scores
# 得到一列文本的语义得分的函数
def get_sentiment_scores(texts):
    scores = []
    results = sentiment_pipeline(texts, function_to_apply = 'none', top_k = None)
    for result in results:
        for label in result:
            if label['label'] == 'LABEL_1': # 中性情绪
                scores.append(label['score'])
    return scores
texts = [
    'The Eiffel Tower in Paris is the tallest structure in the world, with a height of 1,063 metres',
    'This is a bad book',
    'this is a bad books'
]
# 得到一列文本的CoAL得分和中立的语义得分
cola_scores = get_cola_scores(texts)
neutral_scores = get_sentiment_scores(texts)
# 使用zip把得分组合在一起
transposed_lists = zip(cola_scores, neutral_scores)
# 对每一个索引计算加权值
rewards = [1 * values[0] + 0.5 * values[1] for values in transposed_lists]
# 把奖励转换为一列张量
rewards = [torch.tensor([_]) for _ in rewards]
# 得分是[2.52644997, - 0.453404724, - 1.610627412]
```
#### 7.2.3 Transformer强化学习

Transformer强化学习（TRL）是一个开源库，可以用来训练Transformer模型的强化学习。这个库与最喜欢的Hugging Face的Transformers集成在一起。

TRL库支持GPT-2和GPT-Neo等纯解码器模型（第8章对此有更多介绍），以及FLAN-T5等序列到序列模型。所有模型都可以使用近端策略优化（PPO）进行优化。本书没有介绍PPO的内部工作原理，如果读者感兴趣，或者想了解更多应用过程，TRL库在GitHub上有许多例子。

图7.11显示了（目前）简化的RLF的训练过程。这是本书第一个来自反馈循环的强化学习，预训练LLM（FLAN-T5）从精选数据集和预构建的奖励系统中学习。在第8章将看到这个循环以更加定制化和严格的方式执行。

下面用代码来定义训练循环，以真正看到结果。

**数据来源（例如，新闻文章到总结）**，**抓取一批数据**


**FLAN-T5 LLM**，**输出总结并从人类或人工智能那里获得反馈（奖励）**

**优化LLM以获得更多奖励**，**通过PPO进行强化学习**

**我给这个总结打0.23分**

#### 图7.11


![image](https://github.com/user-attachments/assets/894aecee-6f17-4b56-b173-efedb46e84c7)


#### 7.2.4 RLF训练循环


RLF训练循环有以下几个步骤。

(1) 实例化两个版本的模型。

- “参考”模型，即原始FLAN-T5模型，永远不会更新。

- “当前”模型，将在每批数据后更新其参数。

(2) 从源中抓取一批数据（本例中是来自Hugging Face的新闻文章语料库）。

(3) 计算两个奖励模型的奖励，并将其汇总成一个标量（数字），作为两个奖励的加权和。

(4) 将奖励传递给TRL包，包括以下两项。

- 如何根据奖励系统对模型进行微调。

- 文本与参考模型生成的文本有多么不同，即两个输出之间的KL散度。不会深入探讨这个计算，但简单地说，它测量两个序列（本例为两段文本）之间的差异，目的是不让输出与原始模型的生成能力相差太远。
(5) TRL从这批数据中更新“当前”模型，将所有内容记录到报告系统中（笔者喜欢免费的权重和偏差平台），然后再从(1)开始。

图7.12显示了这种训练循环。图7.12中RLF训练循环有四个主要步骤：①LLM生成输出；②奖励系统分配一个标量奖励（正奖励表示良好，负奖励表示不良）；③TRL库在进行任何更新之前考虑奖励和分歧因素；④PPO策略更新LLM。

这个训练循环的代码片段出现在程序清单7.6中，整个训练循环在本书的代码库中有定义。

**当前的LLM为一批数据生成输出**

**考虑奖励模型中的奖励标量**

**将生成的文本与原始LLM（未更新之前）生成的文本进行比较，以确保响应不会过于发散**

**强化学习库（TRL）考虑了奖励系统的奖励和与原始模型的差异，以进行更新**

**textattack/roberta-base-CoLA**

**cardiffnlp/twitter-roberta-base-sentiment**

**FLAN-T5 LLM**

#### 图7.12

![image](https://github.com/user-attachments/assets/833c46b2-eb3e-4318-b048-c6af64a817c5)


**程序清单7.6：使用TRL定义RLF训练循环**

```python
from datasets import load_dataset
from tqdm.auto import tqdm
# 设置配置
config = PPOConfig(
    model_name = "google/flan - t5 - small",
    batch_size = 4,
    learning_rate = 2e - 5,
    remove_unused_columns = False,
    log_with = "wandb",
    gradient_accumulation_steps = 8,
)
# 为复制能力设置随机种子
np.random.seed(42)
# 加载模型和词元器
flan_t5_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)
flan_t5_model_ref = create_reference_model(flan_t5_model)
flan_t5_tokenizer = AutoTokenizer.from_pretrained(config.model_name)
# 加载数据集
dataset = load_dataset("argilla/news - summary")
# 预处理数据集
dataset = dataset.map(
    lambda x: {"input_ids": flan_t5_tokenizer.encode('summarize: ' + x["text"], return_tensors = "pt")},
    batched = False,
)
# 定义校准器函数
def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])
# 开始训练循环
for epoch in tqdm(range(2)):
    for batch in tqdm(ppo_trainer.dataloader):
        game_data = dict()
        # Prep the "summarize: " instruction that T5 works well with
        game_data["query"] = ['summarize: ' + b for b in batch["text"]]
        # 从GPT - 2模型获得回复
        input_tensors = [_.squeeze() for _ in batch["input_ids"]]
        response_tensors = []
        for query in input_tensors:
            response = ppo_trainer.generate(query.squeeze(), **generation_kwargs)
            response_tensors.append(response.squeeze())
        # 存储生成的回复
        game_data["response"] = [flan_t5_tokenizer.decode(r.squeeze(), skip_special_tokens = False) for r in response_tensors]
        # 从清洗过的回复(无特殊词元)中计算奖励
        game_data['clean_response'] = [flan_t5_tokenizer.decode(r.squeeze(), skip_special_tokens = True) for r in response_tensors]
        game_data['cola_scores'] = get_cola_scores(game_data["clean_response"])
        game_data['neutral_scores'] = get_sentiment_scores(game_data["clean_response"])
        rewards = game_data['neutral_scores']
        transposed_lists = zip(game_data['cola_scores'], game_data['neutral_scores'])
        # 对每个索引计算加权值
        rewards = [1 * values[0] + 0.5 * values[1] for values in transposed_lists]
        rewards = [torch.tensor([_]) for _ in rewards]
        # 运行PPO训练
        stats = ppo_trainer.step(input_tensors, response_tensors, rewards)
        # 记录统计结果(笔者使用加权和贝叶斯)
        stats['env/reward'] = np.mean([r.cpu().numpy() for r in rewards])
        ppo_trainer.log_stats(stats, game_data, rewards)
# 训练循环完成后，保存训练后的模型和词元器
flan_t5_model.save_pretrained("t5 - align")
flan_t5_tokenizer.save_pretrained("t5 - align")
```
看看它在两个训练周期之后的表现吧。
#### 7.2.5 结果总结

![image](https://github.com/user-attachments/assets/8372e4b8-6abe-4994-a761-b98e4e9098da)


图7.13显示了两个训练周期的奖励趋势。随着系统的进一步发展，给出了更多的奖励，这通常是一个好兆头。请注意，奖励开始时相对较高，表明FLAN-T5已经提供了相对中立和可读的响应，所以不应该期望生成的摘要发生较大的变化。
#### 图7.13 奖励趋势图（横坐标为步数，纵坐标为奖励，奖励值随着训练步数增加呈上升趋势，图经过平滑处理）

图7.13中随着训练的进行，系统给出了更多的奖励（该图经过平滑处理，以显示整体趋势）。

但是，这些经过调整的周期看起来是什么样子的呢？图7.14显示了RLF微调前后生成的摘要样本。

**原始的FLAN-T5模型喜欢用“废弃的”，倾向于带负面的情绪**

President Trump scrapped Obama-era program that protects from deportation immigrants brought illegally into the United States as children, delaying impleme ntation until March and giving a gridlocked Congress six months to decide t he fate of almost 800,000 young people. As the so - called Dreamers who have benef ited from the five-year-old pro gram were plunged into uncertainty, business and religious lea ders, mayors, governors, Democratic lawmakers, unions, civil liberties advo cates and even former Democratic President Barack Obama all condemned Trump's move.

**用增强学习微调过的FLAN-T5模型倾向于更中立的词“宣布”**

Trump announced his decision to end DACA, a political decision that protects from deportation immigrants brought illegally into the United States as children, delaying implementation until March and giving a gridlocked Congress six months to decide t he fate of almost 800,000 young people. As the so - called Dreamers who have benefited from the five-year-old program were plunged into uncertainty, business and religious leaders, mayors, governors, Democratic lawmakers, unions, civil liberties advo cates, and former Democratic Pre sident Barack Obama all condemned Trump's move.

![image](https://github.com/user-attachments/assets/e735e72b-e47d-45f9-9377-1132eca08d5c)


#### 图7.14

图7.14中微调后的模型在大多数摘要中几乎没有差别，但倾向于使用语法正确、易于阅读的中性词汇。

这是本书第一个对LLM进行非监督数据微调的例子。从未给FLAN-T5（文章、摘要）示例提供帮助，以学习如何总结文章，这很重要。FLAN-T5已经见过

关于总结的监督数据集，所以它应该已经知道该怎么做。想做的就是稍微调整响应，使响应与所定义的奖励指标更加一致。第8章提供了这个过程的更深入的例子，在这个过程中，用监督数据训练LLM，训练自己的奖励系统，并执行同样的TRL训练循环，得到更有趣的结果。

### 7.3 本章小结
像FLAN - T5、ChatGPT、GPT - 4、Cohere的命令系列、GPT - 2和BERT这样的基础模型是解决各种任务的绝佳起点。用有监督的词元数据微调模型来调整分类和嵌入，可以使我们走得更远，但有些任务要求创造性地使用微调。本章仅是抛砖引玉，接下来的两章将更深入地讲解修改模型和更创造性地使用数据的方法，甚至开始回答如何通过高效部署LLM以与世界分享惊人工作的问题。 

