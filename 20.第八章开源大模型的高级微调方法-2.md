### 第8章
在实践中，BERT中的冻结层类似于程序清单8.4所展示的样子。图8.5中还显示了一些冻结选项。这里尝试训练完全解冻的模型（选项1）和仅冻结部分层的模型（选项2），下面将展示实验结果。

**程序清单8.4：在BERT模型冻结除最后三层外的其他层+CLF层**

```python
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL,
    problem_type = "multi_label_classification",
    num_labels = len(unique_labels)
)
# 冻结除最后三个编码层之外的所有内容
for name, param in model.named_parameters():
    if 'distilbert.transformer.layer.4' in name:
        break
    param.requires_grad = False
```

#### 8.1.4 结果总结
两种训练过程（一是在不冻结任何层的情况下微调BERT，二是在最后三个编码层之前冻结所有内容）从同一起点开始训练，模型基本上是随机初始化，如F1、ROC/AUC、准确度和Jaccard相似分。

随着训练的进行，训练轨迹开始出现分歧。到最后一个训练周期，这些指标如下。

**训练损失**：两个模型都显示训练损失随着时间的推移而下降，表明模型正在成功地学习和提高其对训练数据的拟合度。然而，没有任何层被冻结的模型显示出略低的训练损失（0.1147对0.1452），表明对训练数据的学习更好。

**验证损失**：两个模型的验证损失也随着时间的推移而降低，表明模型对未见数据的泛化能力有所提高。没有任何层被冻结的模型获得了略微较低的验证损失（0.1452对0.1481），意味着如果目标是最小化验证损失，这是更好的选择。

**F1分数**：F1分数是精确度和召回率的平衡度量，对于没有任何层被冻结的模型，F1分数更高（0.5380对0.4886），表明该模型的精确度和召回率更高。

**ROC/AUC**：ROC/AUC在没有任何层被冻结的情况下也更高（0.7085对0.6768），表明总体分类性能更优。

**准确度**：没有层被冻结的模型也取得了略微更高的准确度得分（0.1533对0.1264），表明在更高的概率上能得到准确的预测。

**Jaccard相似分**：Jaccard相似分用于衡量预测标签和实际标签之间的相似性，对于没有任何层被冻结的模型，Jaccard分数更高（0.3680对0.3233），表明它预测的标签更接近实际标签。

未冻结的模型似乎比最后三层被冻结的模型性能更好。通过允许所有层进行微调，模型能够更好地适应任务的特殊性。然而，并非总是如此，具体也取决于任务和特定的数据集。在某些情况下，通过冻结初始层可以有效防止过拟，从而实现更好的泛化效果。这些策略之间的选择通常需要通过权衡，必须在基于特定任务和数据的背景下考虑。

值得注意的是，虽然未冻结模型的表现更好，但这是以更多的计算资源和时间为代价的。部分冻结模型的训练速度比未冻结模型快30%。根据具体用例，需要考虑性能和计算效率之间的权衡。有时效率指标的轻微下降是可以接受的，因为可以显著节省计算时间和资源，特别是在大型数据集或更复杂的模型中。图8.6突出了这些差异。

**部分冻结模型与未冻结模型之间的对比** （图8.6 ）


![image](https://github.com/user-attachments/assets/6030bc58-67c1-45c9-baf9-cba00db98c24)


可以用前面章节中使用的流水线来调用新训练的模型。程序清单8.5提供了相关代码。训练后的模型通常可以预测正确大部分的标签，并且很少有严重误判。

**程序清单8.5：使用基因预测器**
```python
# 从transformers库中导入必需的类
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
# 加载与模型相关的词元器
tokenizer = AutoTokenizer.from_pretrained(MODEL)
# 为后续分类加载预训练模型，设置问题类型为'multi_label_classification'
# '.eval()'方法用于设置评估模型
# 在评估模式，为了确保输出一致，所有神经元都会被用到
trained_model = AutoModelForSequenceClassification.from_pretrained(
    f"genre-prediction", problem_type = "multi_label_classification",
).eval()
# 为文本分类创建一个管道，这个管道将使用加载的模型和词元器
# 参数确保管道返回所有标签的得分，而不仅仅是最高得分
classifier = pipeline(
    "text - classification", model = trained_model, tokenizer = tokenizer,
    return_all_scores = True
)
# 使用分类器管道为给定文本做预测
prediction = classifier(texts)
# 为标签得分设置一个门槛，只有分数高于这个门槛的标签才被当作预测标签
THRESHOLD = 0.5
# 过滤掉分数低于门槛的标签
prediction = [[label for label in p if label['score'] > THRESHOLD] for p in
prediction]
# 打印每一个文本、预测标签的分数和实际标签
# 预测标签按分数降序排列
for _text, scores, label in zip(texts, prediction, labels):
    print(_text)
    print('--------------')
    for _score in sorted(scores, key = lambda x: x['score'], reverse = True):
        print(f'{_score["label"]}: {_score["score"] * 100:.2f}% ')
    print('actual labels: ', label)
    print('--------------')
```

### 8.2 采用GPT-2生成LaTeX
本章的第一个模型微调的例子是翻译任务。在选择该实验的语言时，选择了GPT-2可能不熟悉的语言。这种语言需要一种在模型的预训练阶段不经常遇到的语言，因为GPT-2在预训练阶段的数据源来自WebCrawl（一个来自Reddit链接的大型语料库）。此处选择LaTeX语言。

LaTeX是一种排版系统，是为制作科技文档而设计的。LaTeX不仅是一种标记语言，还是一种编程语言，用于排版复杂的数学公式和管理高质量的文本排版。它被广泛用于许多领域的科学文献的交流和出版，包括数学、物理学、计算机科学、统计学、经济学和政治学。

本微调任务的挑战包括两方面。首先，必须让GPT-2理解LaTeX，这与GPT-2最初训练使用的英语等自然语言有很大不同。其次，必须教会GPT-2将英语语句转换为LaTeX格式，这项任务不仅涉及语言翻译，还要求理解语句的上下文和语义。图8.7描述了该学习任务。 （图8.7 ）


![image](https://github.com/user-attachments/assets/9207d79a-8751-4485-9ff1-c01fbe38b332)


目前，在网络资源上并不能搜索到满足这种转换任务的数据集。因此，笔者自手动编写了50个简单的英语语句转换为LaTeX格式的样本。这是本书使用的最小数据集，但它将极大地帮助读者了解迁移学习在这里能发挥的作用。因为利用仅有的50个样本，需要依靠GPT-2对翻译任务的识别以及将其知识转移到这个任务的能力。

图8.7中的训练数据集是由笔者编写的50个英语转换为LaTeX的格式样本。在GPT-2预训练和迁移学习的帮助下，这些示例应该足以让GPT-2了解任务。

如图8.8所示，这里采用提示词工程来定义一个LaTeX转换任务，提示词技能包括使用清晰的指令和前缀来修饰引导模型，同时保持提示词的简洁性。 （图8.8 ）


![image](https://github.com/user-attachments/assets/15398c10-872b-43b2-b26a-61f4eb8cf21b)


#### 8.2.1 开源大模型的提示词工程
回想一下第3章和第5章关于提示词工程的内容，首先需要定义一个提示，将其输入模型，清楚地描述任务，并明确指示要做什么，就像ChatGPT或Cohere等模型一样。图8.8显示了最终确定的提示，其中包括一个明确的指令和明确的前缀，以描述模型在何处读取/写入响应。

朴素的想法是使用工程提示格式中的50个英语语句转换为LaTeX格式的样本，让GPT-2模型用自回归语言建模的标准定义损失（即下一个标记预测的交叉熵），一遍又一遍地读取它们（训练多个周期）。本质上，这属于一个分类任务，其中的标签是从词汇表中选择的标记。程序清单8.6显示了生成数据集的代码片段。

**程序清单8.6：为LaTeX建立客户数据集**
```python
data = pd.read_csv('../data/english_to_latex.csv')
# 增加提示
CONVERSION_PROMPT = 'Convert English to LaTeX\n'
CONVERSION_TOKEN = 'LaTeX:'
# “training prompt”是打算让GPT-2模型识别并学习的
training_examples = f'{CONVERSION_PROMPT}English:'+ data['English'] + '\n' +
CONVERSION_TOKEN +'' + data['LaTeX'].astype(str)
task_df = pd.DataFrame({'text': training_examples})
# 将包含LaTeX数据的pandas数据集转换为Hugging Face数据集
latex_data = Dataset.from_pandas(task_df)
def preprocess(examples):
    # 对语句进行词元化，需要时可以截断。此处不做填充，因为校准器将在后面的阶段
    # 动态处理
    return tokenizer(examples['text'], truncation = True)
# 对LaTeX数据集应用预处理函数，映射函数对数据集中的所有例子应用预处理函数，
# batched = True使函数可以在例子的不同批次上高效运行
latex_data = latex_data.map(preprocess, batched = True)
# 把预处理数据集分为训练集和测试集，train_test_split对例子进行随机划分，80%用
# 于训练，20%用于测试
latex_data = latex_data.train_test_split(train_size =.8)
```

定义好数据集后，定义模型和训练集。这里不再使用用于类型预测的AutoModelForSequenceClassification类，而是使用AutoModelForCausalLM来表示自回归语言建模的新任务。程序清单8.7显示了如何设置训练循环。

**程序清单8.7：使用GPT-2进行自回归语言建模**
```python
# 用于把例子整理成批
# 这是一个动态的过程，在训练期间处理
data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)
# 使用预训练版本初始化GPT-2模型
latex_gpt2 = AutoModelForCausalLM.from_pretrained(MODEL)
# 定义训练参数，包括输出目录、训练周期数、训练和评估的批大小、日志等级、评估策略
# 和存储策略
training_args = TrainingArguments(
    output_dir = "./english_to_latex",
    overwrite_output_dir = True,
    num_train_epochs = 5,
    per_device_train_batch_size = 1,
    per_device_eval_batch_size = 20,
    load_best_model_at_end = True,
    log_level = 'info',
    evaluation_strategy = 'epoch',
    save_strategy = 'epoch'
)
# 初始化Trainer，传入GPT-2模型、训练参数、数据集和数据校准器
trainer = Trainer(
    model = latex_gpt2,
    args = training_args,
    train_dataset = latex_data["train"],
    eval_dataset = latex_data["test"],
    data_collator = data_collator,
)
# 最后用测试数据集评估模型
trainer.evaluate()
```

#### 8.2.2 结果总结
虽然本例中的模型不一定是最优的LaTeX转换模型，但验证集中的误差已经下降很多。程序清单8.8显示了使用LaTeX转换模型的示例。

**程序清单8.8：使用GPT-2进行自回归语言建模**
```python
loaded_model = AutoModelForCausalLM.from_pretrained('./math_english_to_latex')
latex_generator = pipeline('text - generation', model = loaded_model, tokenizer =
tokenizer)
text_sample = 'g of x equals integral from 0 to 1 of x squared'
conversion_text_sample = f'{CONVERSION_PROMET}English: {text_sample}\n{CONVERSION_
TOKEN}'
print(latex_generator(
    conversion_text_sample, num_beams = 2, early_stopping = True, temperature = 0.7,
    max_new_tokens = 24
)[0]['generated_text'])
```
**输出示例**：
```
Convert English to LaTeX
English: g of x equals integral from 0 to 1 of x squared
LaTeX: g(x) = \int_{0}^{1} x^2 \,dx
```

GPT-2仅使用50个样本来训练任务，就能快速地学会LaTeX转换任务。如果在LaTeX转换的例子上进一步推进大模型的能力，会不会有更多的惊喜呢？

### 8.3 Sinan尝试做出聪明而优美的回应：SAWYER
可以说，本书的很多内容都是为了给出这一观点：在开源大模型的预训练参数中隐藏着巨大的信息，但往往需要一些微调才能真正有用。本例中已经看到GPT-2等预训练模型如何适应各种任务，以及微调如何从这些模型中产生额外的效果，类似的事情如OpenAI在2022年对GPT-3模型进行指令微调，从而引发了人们对人工智能的新一轮兴趣浪潮。

现在，是我们开始自己激动人心的旅程的时候了。

这里将使用曾经非常强大的GPT-2模型，一个“只有”大约1.2亿个参数的模型，以挖掘其潜力。本章专注于GPT-2而不是其更大的兄弟模型（如GPT-3）的原因有两方面，一方面模型并非越大越好，GPT-2可以在没有太多GPU资源时进行微调，另一方面GPT-3是闭源的模型。

我们将尝试类似于OpenAI在GPT-3、ChatGPT和其他模型上取得的成就。

本例中打算微调GPT-2，特别关注的是指令微调，定义一个奖励模型来模拟人类反馈（直接从人类获取反馈来修正模型会非常耗时且不切实际），并使用该奖励模型进行强化学习（RL），以引导模型随着时间的推移而改进，推动它产生更接近人类偏好的响应。

如图8.9所示，该计划包括三个步骤。 （图8.9 ）


![image](https://github.com/user-attachments/assets/6152474e-6e74-450d-83cc-efd83462b896)


**步骤（1）**：使用预先训练好的GPT-2，使其能理解回答问题的概念：首要目标是确保GPT-2模型能理解当前的任务，包括使其理解需要根据特定问题或者提示来生成答案。

**步骤（2）**：定义一个奖励模型，对符合人类偏好的问题答案给出高评分：一旦GPT-2明确了其任务，就需要建立一个能够评估其表现的体系。这就是奖励模型发挥作用的地方。它的设计目的是对符合人类偏好的答案进行更有利的评分。

**步骤（3）**：实施强化学习训练，以推动GPT-2给出符合人类偏好的回复：这一步是创建一个反馈机制，帮助GPT-2随着时间的推移而改进。将使用强化学习来提供这种反馈调优。通过推动模型提供更多符合人类偏好的回复，来不断改进和增强GPT-2的性能。

毫无疑问，这是一项具有挑战性的工作，但会让读者学习到很多知识。在这个实验结束时，预期的目标是逼近GPT-2的极限，并检验在限制条件下能提高多少性能。毕竟这就是数据科学的全部意义——学习、实验和突破可能的能力边界。

#### 8.3.1 有监督指令微调
第一步“有监督指令微调”与LaTeX案例几乎相同，因为这里将在一组新文档上微调开源大模型（在这种情况下使用GPT-2）。在LaTeX示例中，采用微调模型解决特定任务的关键点没有改变。其不同之处在于，不是定义单一的任务来完成（例如英语语句转LaTeX），而是向GPT-2提供来自开放指令OIG数据子集集中的单轮问答样本语料库。OIG是一个大型的开源指令数据集，目前包含大约4300万条指令。这里将使用这些示例中的10多条。其中一个示例如图8.10所示。 （图8.10 ）


![image](https://github.com/user-attachments/assets/fb6b003d-b5c6-4567-96fe-dc9d83b0456f)


图8.10中使用10万多条指令/响应对示例来微调GPT-2，以识别“输入一个问题，输出一个回复”的模式。

程序清单8.9包含这段代码的片段。读者看起来应该很熟悉，因为它与LaTeX微调代码类似。

**程序清单8.9：监督指导微调**

```python
from transformers import TrainingArguments, Trainer
# 初始化由Hugging Face提供的TrainingArguments对象
training_args = TrainingArguments(
    output_dir = "./sawyer_supervised_instruction",  # (checkpoints, logs etc.)
    overwrite_output_dir = True,  # 这个标志允许输出路径的内容(如果存在)被重写
    num_train_epochs = 1,  # 指定训练的期数
    per_device_train_batch_size = 2,  # 每个设备的训练批大小
    per_device_eval_batch_size = 4,  # 每个设备的评估批大小
    gradient_accumulation_steps = 16,  # 执行更新前累计的梯度步长，处理内存限制时有用
    load_best_model_at_end = True,  # 在每次评估时发现的最好模型是否加载
    evaluation_strategy = 'epoch',  # 每一期进行评估时定义
    save_strategy = 'epoch',  # 每一期存储检查点时定义
    report_to = "all",  # 按训练标尺传给谁，all代表所有可跟踪的系统
    available_tracking_systems (TensorBoard, WandB, etc.)
seed = seed,  # 确保复制能力的随机数的种子
    fp16 = True,  # 实现混合精度训练，得益于NVIDIA Volta系的张量计算核心GPU
)
# 初始化由Hugging Face提供的Trainer对象
trainer = Trainer(
    model = model,  # 被训练的模式
    args = training_args,  # 训练参数
    train_dataset = chip2_dataset['train'],  # 训练数据集
    eval_dataset = chip2_dataset['test'],  # 评估数据集
    data_collator = data_collator  # 用于将训练和评估期间的数据例子整理成批
)
# 为评估数据集评估模型
trainer.evaluate()
```
当有一个理解基本任务的模型后，需要进一步定义一个可以评估其效果优劣的模型。

### 8.3.2 奖励模型的训练
在微调了一个可以学习处理指令和生成回答的基本任务的模型之后，下一个挑战是定义一个可以有效地评估其效果的模型。在机器学习领域，这种模型被称为奖励模型。本节将讲解这种奖励模型的训练过程。

这一步将使用新的回复比较数据集，其中单个查询有多个回复，所有回复都由各种LLM给出。然后，人类对每个回复进行评分，分数为1 - 10，其中1表示最糟糕的回复，10表示最出色的回复。图8.11显示了其中一个对比的例子。 （图8.11 ）


![image](https://github.com/user-attachments/assets/466f3f81-029b-4977-b386-4ea51a65afb6)


从本质上来说，图8.11中的奖励数据集是很简单的：它将LLM对提问的回复进行比较，以定量给出LLM在提问/回复方面的有效程度。

有了这些人类标记的数据，可以进一步定义奖励模型的结构。其基本思想如图8.12所示，采用人类对问题的首选回复和非首选回复来区分，这些回复都输入到奖励模型LLM，这里采用的奖励模型为BERT模型，并让它学会区分什么是首选的，什么是非首选的，来作为对指令的响应。值得注意的是，这里并没有使用与微调中相同的指令。因为如果在这里使用相同的数据，系统将只看到来自单个数据集的数据。而本例的目的是使系统看到的数据更加多样化，以促进其回复未知查询的能力。 （图8.12 ）

在图8.12中，奖励模型将接受对各种LLM查询的回复，这些LLM由人类评分，并学会区分查询的回复中哪些是首选，哪些不是首选。


![image](https://github.com/user-attachments/assets/46828206-cef0-45c2-9b4f-0184f9c56af2)



奖励过程可以被视为一个简单的分类任务：给定一个问题和两个答案，对哪 
