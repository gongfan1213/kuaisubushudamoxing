### 第3部分 大模型的高级使用
### 第7章 超越基础模型

前几章重点讲解了使用或通过微调预训练模型（如BERT）处理各种自然语言和计算机视觉任务的方法。这些模型在基准测试中表现出最优性能，但它们仍然不足以解决更复杂或特定领域的任务，需要对这些任务有更深入的理解及处理能力。

本章讲解通过结合现有模型构建新型LLM架构的概念。通过组合不同的模型，可以创建一个更具优势的混合体系结构，该体系结构要么比单个模型性能更好，要么可以执行以前不可能完成的任务。

本章将构建一个多模态视觉问答系统，结合BERT的文本处理能力、视觉转换器的图像处理能力和开源GPT - 2的文本生成来解决视觉推理任务。本章还将探索强化学习领域，并讲解如何使用强化学习来微调预训练的LLM。让我们一探究竟吧。

#### 7.1 案例研究：视觉问答

视觉问答（VQA）是一项具有挑战性的任务，需要对图像和自然语言进行理解和推理（图7.1）。给定图像和文本形式的相关问题，目标是生成正确的文本回答。在第5章的提示链示例中讲解了一个使用预训练模型的VQA系统的简短示例，现在继续实践。

图7.1中，视觉问答（VQA）系统通常采用两种模式（类型）的数据——图像和文本，并返回适合人类阅读的问题答案。这张图片概述了解决这个问题最基本的方法之一：图像和文本由单独的编码器编码，最后一层预测一个单词作为答案。

本节将重点讲解通过使用现有的模型和技术来构建VQA+LLM系统。本章首先介绍用于此任务的基本模型：BERT、ViT和GPT - 2，然后探索这些模型的组合，以创建一个能够处理文本和图像输入并生成连贯文本输出的混合架构。


![image](https://github.com/user-attachments/assets/229af782-617a-45af-a6f3-20c34606ec7e)


本章还将演示如何使用专门为VQA任务设计的数据集来微调模型，会使用VQA v2.0数据集，其中包含大量关于图像的自然语言问题和相应的答案。将讲解如何准备该数据集并进行模型训练和评估，以及如何使用该数据集对模型进行微调。

#### 7.1.1 模型简介：DistilBERT、视觉转换器和GPT - 2

在构建的多模态系统中使用三个基本模型：DistilBERT、视觉转换器和GPT - 2。

这些模型虽然目前还没有被公认为是最优的模型，但仍然是强大的LLM，并已广泛用于各种自然语言处理和计算机视觉任务。同样值得注意的是，当考虑使用哪些LLM时，并不总是必须选择顶级的LLM，因为它们往往更大，使用起来更慢。有了正确的数据和正确的动机，可以使较小的LLM适用于特定任务。

1. **文本处理器：DistilBERT**

DistilBERT是流行的BERT模型的精简版本，针对速度和内存效率进行了优化。该预训练模型利用知识蒸馏功能将知识从较大的BERT模型转移到较小且更有效的模型。使其能够更快地运行，消耗更少的内存，同时保留了较大模型的大部分性能。

DistilBERT应事先掌握语言知识，这将有助于在训练期间进行迁移学习，使其能够高精度地理解自然语言文本。

2. **图像处理器：视觉转换器**

视觉转换器（ViT）是一种基于转换器的架构，专门为理解图像而设计。该模型使用自注意力机制从图像中提取相关特征。作为近年来流行的一种新模型，已被证明在各种计算机视觉任务中有效。

与BERT一样，ViT也在一个名为Imagenet的图像数据集上进行了预训练。因此，它还具有图像结构的先验知识，这在训练期间有所帮助，使得ViT能够非常精确地理解和提取图像中的相关特征。

使用ViT时，应该使用与模型在预训练中相同的图像预处理步骤，这样更容易学习新的图像集。但这并不是必需的，有利也有弊。

**重复使用相同的预处理步骤的好处如下**。

(1) **与预训练的一致性**：使用与预训练期间格式和分布相同的数据可以获得更好的性能和更快的收敛。

(2) **利用先验知识**：由于该模型已经在大型数据集上进行了预训练，已经学会了从图像中提取有意义的特征。使用相同的预处理步骤使模型能够有效地将这一先验知识应用于新的数据集。

(3) **改进的泛化能力**：如果预处理步骤与其预训练中的预处理步骤一致，则模型更有可能很好地泛化到新数据，因为模型已经看到了各种各样的图像结构和特征。

**重复使用相同的预处理步骤的缺点如下**。

(1) **灵活性有限**：重复使用相同的预处理步骤可能会限制模型适应新数据分布或新数据集特定特征的能力，这可能需要不同的预处理技术来获得最佳性能。

(2) **与新数据的不兼容性**：在某些情况下，新的数据集可能具有独特的属性或结构，这些属性或结构不适合预训练中的预处理步骤，如果预处理步骤没有相应的调整，可能会导致性能不佳。

(3) **对预训练数据的过度拟合**：过于依赖相同的预处理步骤可能会导致模型过度拟合预训练数据的特定特征，从而降低其泛化到新的和多样化的数据集的能力。

现在重新使用ViT图像预处理器对图像数据集进行处理。图7.2显示了预处理前的原始图像以及经过ViT标准预处理步骤后的图像。

图7.2中，视觉转换器（ViT）等图像系统通常必须将图像标准化为具有预定义标准化步骤的集合格式，以便尽可能公平一致地处理每幅图像。对于某些图像（如顶行倒下的树），图像预处理以牺牲图像的标准化为代价消除了背景。


![image](https://github.com/user-attachments/assets/5bb7480a-ef24-4645-85b1-eab0e62df56b)


3. **文本解码器GPT - 2**

GPT - 2是GPT - 3的前身，但更重要的是，它是一个开源的生成语言模型，在一个大型文本数据集上进行了预训练。GPT - 2在大约40GB的数据上进行了预训练，因此它也具有在训练期间有帮助的单词的先验知识，这也要归功于迁移学习。这三个模型——用于文本处理的DistilBERT、用于图像处理的ViT和用于文本解码的GPT - 2——的组合将为多模态系统提供基础，如图7.3所示。这些模型都有先验知识，我们将依靠迁移学习能力，使它们能够有效处理和生成高度准确和相关的输出，以完成复杂的自然语言和计算机视觉任务。

图7.3中，在VQA系统中，最终的单词元预测层可以用一个完全独立的语言模型（如开源GPT - 2）替换。将构建的VQA系统由三个基于Transformer的模型并行工作，来解决一个非常具有挑战性的任务。


![image](https://github.com/user-attachments/assets/ec1be948-a80a-4806-b1f5-813fff514208)


#### 7.1.2 隐藏状态投影和融合

当把文本和图像分别输入各自的模型（DistilBERT和ViT）时，它们会产生包含输入的有用特征表示的输出向量。然而，这些特征不一定具有相同的格式，它们可能具有不同的维度。

为了解决这种不匹配问题，使用线性投影层将文本和图像模型的输出向量投影到共享维度空间上，这能够有效地融合从文本和图像输入中提取的特征。共享维度空间将文本和图像特征进行组合（在本例中对二者进行平均）并输入解码器（GPT - 2）中，以生成连贯和相关的文本。

但是GPT - 2如何接收编码模型的这些输入呢？答案是被称为交叉注意力的注意力机制。

#### 7.1.3 交叉注意力是什么以及为什么至关重要

交叉注意力机制为多模态系统学习文本和图像输入之间的相互作用以及想要生成的输出。它是基础Transformer架构的关键组成部分，可以有效地将输入信息整合到输出中（序列到序列模型的标志）。交叉注意力实际上与自注意力非常相似，但是发生在两个不同的序列之间，而不是单个序列内。

在本例中，图像和文本编码器的查询组合序列将作为键和值输入，而输出序列将作为查询输入（文本生成GPT - 2）。

**注意力中的Query（查询）、Key（键）和Value（值）**。

注意力的三个内部组件——Query、Key和Value在本书之前的章节中并没有真正出现过，因为并没有真正需要理解它们为什么存在，相反，只是依赖于它们在数据中学习模式的能力。现在是时候仔细研究这些组件是如何相互作用的，以便能够完全理解交叉注意力是如何工作的。

在Transformers使用的自注意力机制中，Query、Key和Value组件对于确定每个输入词元相对于序列中其他词元的重要性至关重要。Query表示想要计算注意力权重的词元，Key和Value表示序列中的其他词元。注意力得分是计算Query和Key之间的点积，用归一化因子对其进行缩放，然后将其乘以Value，计算得到的加权值。

简而言之，Query用于从其他词元提取相关信息，这取决于注意力评分。Key有助于识别哪些词元与Query相关，而Value则提供相应的信息。三者关系如图7.4所示。

在交叉注意力中，Query、Key和Value矩阵的作用略有不同。在这种情况下，Query表示一种模态（例如文本）的输出，Key和Value表示另一种模态（例如图像）的输出。交叉注意力用于计算注意力得分，该得分决定了在处理另一种模态时，一种模态的输出被赋予的重要程度。

图7.4中，这两幅图像为输入“I like cats”中的单词like产生了缩放点积注意力值。基于Transformer的LLM的每个输入词元都有一个相关的Query、Key和Value。缩放点积注意力计算通过与Query词元（顶部）进行点积来为每个查询词元生成注意力得分。这些分数随后用于对Value词元进行适当的加权（底部），从而为输入中的每个词元产生一个最终向量，该向量现在知道输入中的其他词元以及应该关注它们的程度。在这种情况下，词元like应该将其22%的注意力放在词元I上，42%的注意力放在自己身上（词元需要关注自己，因为它们是序列的一部分），36%的注意力放在词元cats上。

在多模态系统中，交叉注意力计算的注意力权重，表示文本和图像输入之间的相关性（图7.5）。Query是文本模型的输出，而Key和Value是图像模型的输出。注意力通过计算Query和Key之间的点积，并通过归一化因子进行缩放。然后将得到的注意力权重乘以Value，以实现加权和，用于生成连贯和相关的文本响应。


![image](https://github.com/user-attachments/assets/0d2ba0c9-e7eb-4ee9-bdc2-c8fd8a01061a)


程序清单7.1显示了三个模型的隐藏状态。


![image](https://github.com/user-attachments/assets/dfe3ea95-a456-4cd6-9b25-04e13c4f6a27)


![image](https://github.com/user-attachments/assets/655fa302-db28-467c-8296-1d4b593f97e5)


**程序清单7.1：显示LLM的隐藏状态的维度**
```python
# 加载文本编码器模型,并打印配置的隐藏层大小(隐藏层单元的数量)
print(AutoModel.from_pretrained(TEXT_ENCODER_MODEL).config.hidden_size)

# 加载图像编码器模型(使用ViT架构),并打印配置的隐藏层大小
print(ViTModel.from_pretrained(IMAGE_ENCODER_MODEL).config.hidden_size)

# 加载解码器模型(CLM),并打印配置的隐藏层大小
print(AutoModelForCausalLM.from_pretrained(DECODER_MODEL).config.hidden_size)
# 768
# 768
# 768
```
在例子中，所有模型的隐藏状态维度都是相同的，因此理论上不需要投影任何内容。然而，使用投影层是一个很好的做法，这样模型就有一个可训练层，可以将文本/图像表示转化为对解码器更有意义的内容。


![image](https://github.com/user-attachments/assets/9a8991cf-2e67-4bc8-b005-76d4a554b1dc)


在训练过程中，交叉注意力参数必须随机化，并且需要在训练过程中学习。在训练过程中，模型会为相关特征分配更高的注意力权重，同时过滤掉不相关的特征。这样系统可以更好地理解文本和图像输入之间的关系，并生成与输入问题和图像更相关且准确的文本输出。有了交叉注意力、融合和模型的想法，就可以继续定义多模态架构。
#### 7.1.4 定制多模态联运模型
在涉及代码之前，笔者要指出的是，并非所有例子的代码都出现在本书页面中，但它们都保存在本书的代码库中。强烈建议同时使用这两种方法。


在创建新的PyTorch模块（正在做的）时，需要定义的主要方法是构造函数（init），它将实例化3个Transformer模型，并冻结某些层来加速训练（更多内容见第8章），以及前向方法——它将接收输入和标签，以生成输出和计算损失值（回想一下，损失等于误差——越低越好）。前向方法将接收以下输入。
- **input_ids**：一个包含文本词元输入ID的张量。这些ID由基于输入文本的词元器生成。该张量的形状为[batch_size,sequence_length]。 
- **attention_mask**：与input_ids形状相同的张量，指示哪些输入词元应被关注（值1），以及哪些输入词元应被忽略（值0）。它主要用于处理输入序列中的填充词元。 
- **decoder_input_ids**：一个包含解码器词元的输入ID的张量。这些ID由词元器根据目标文本生成，在训练期间用作解码器的提示。训练期间张量的形状为[batch_size,target_sequence_length]。在推理时，它只是一个开始词元，因此模型必须生成其余部分。 
- **image_features**：一个包含批量中每个样本的预处理图像特征的张量。张量的形状为[batch_size,num_features,feature_dimension]。 
- **labels**：一个包含目标文本标签的张量。张量的形状是[batch_size,target_sequence_length]。这些标签用于在训练期间计算损失，但在推理时并不存在。毕竟，如果有标签，那么就不需要这个模型了。 

程序清单7.2是从三个独立的基于Transformer的模型（BERT、ViT和GPT - 2）创建自定义模型所需的代码片段。完整的代码可以在本书的代码库中找到。


**程序清单7.2：一小段多模态模型**
```python
class MultiModalModel(nn.Module):
    # 冻结指定的编码器或解码器
    def freeze(self, freeze):
        # 在指定的组件中迭代并冻结它们的参数
        if freeze in ('encoders', 'all') or 'text_encoder' in freeze:
            for param in self.text_encoder.parameters():
                param.requires_grad = False
        if freeze in ('encoders', 'all') or 'image_encoder' in freeze:
            for param in self.image_encoder.parameters():
                param.requires_grad = False
        if freeze in ('decoder', 'all'):
            for name, param in self.decoder.named_parameters():
                if "crossattention" not in name:
                    param.requires_grad = False

    # 对输入文本进行编码,投影到解码器的隐藏空间中
    def encode_text(self, input_text, attention_mask):
        # 检查输入是否为NaN或无限数据
        self.check_input(input_text, "input_text")
        # 对输入文本进行编码,并且获得最后的隐藏状态
        text_encoded = self.text_encoder(input_text, attention_mask =attention_mask).last_hidden_state.mean(dim = 1)
        # 把编码后的文本投影到解码器的隐藏空间
        return self.text_projection(text_encoded)

    # 对输入图像进行编码并投影到解码器的隐藏空间
    def encode_image(self, input_image):
        self.check_input(input_image, "input_image")
        # 对输入图像进行编码,并获得最后的隐藏状态
        image_encoded = self.image_encoder(input_image).last_hidden_state.mean(dim = 1)
        # 把编码后的图像投影到解码器的隐藏空间
        return self.image_projection(image_encoded)

    # 前向过程: 对文本和图像进行编码,对编码后的特征进行融合(用GPT - 2模型解码)
    def forward(self, input_text, input_image, decoder_input_ids, attention_mask, labels = None):
        # 检查解码器的输入是否为NaN或无限数据
        self.check_input(decoder_input_ids, "decoder_input_ids")
        # 对文本和图像进行编码
        text_projected = self.encode_text(input_text, attention_mask)
        image_projected = self.encode_image(input_image)
        # 编码后的文本特征图像特征
        combined_features = (text_projected + image_projected) / 2
        # 设置解码器的补丁词元标签为 - 100
        if labels is not None:
            labels = torch.where(labels == decoder_tokenizer.pad_token_id, -100, labels)
        # 用GPT - 2模型解码
        decoder_outputs = self.decoder(
            input_ids = decoder_input_ids,
            labels = labels,
            encoder_hidden_states = combined_features.unsqueeze(1)
        )
``` 
