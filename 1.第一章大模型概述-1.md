### 第1部分 大模型介绍
#### 第1章 大模型概述
2017年，谷歌大脑团队推出了名为Transformer的高级人工智能（AI）深度学习模型。从那时起，Transformer就成为了学术界和工业界处理各种自然语言处理（NLP）任务的基准。近年来，很多人在无意识的情况下已经与Transformer模型有过互动，例如，谷歌使用BERT来增强其搜索引擎的功能，使其更好地理解用户的搜索意图，与此同时，OpenAI的GPT系列模型也因其生成类似人类产生的文本和图像而受到关注。

这些Transformer模型现在为GitHub的Copilot（由OpenAI与微软合作开发）等应用程序提供支持。该应用可以将评论和代码片段转换为功能齐全的源代码，甚至可以调用其他LLM（如程序清单1.1）来执行NLP任务。

**程序清单1.1：使用Copilot LLM从Facebook的BART LLM获取输出**

```python
from transformers import pipeline

def classify_text(email):
    """
    Use Facebook's BART model to classify an email into "spam" or "not spam"

    Args:
        email(str):The email to classify
    Returns:
        str:The classification of the email
    """
    # Copilot开始，这条注释之前的内容都是用来输入Copilot
    classifier = pipeline(
        'zero - shot - classification',model = 'facebook/bart - large - mnli')
    labels = ['spam', 'not spam']
    hypothesis_template = 'This email is {}.'

    results = classifier(
        email, labels, hypothesis_template = hypothesis_template)

    return results['labels'][0]
    # Copilot结束
```
在程序清单1.1中，笔者使用Copilot仅提供一个Python函数定义和一些注释，Copilot就可以生成符合笔者描述功能的可执行代码。这里没有精益求精，只是一个完全可用的Python函数，可以这样调用：
```python
classify_text('hi I am spam')  # 垃圾邮件
```
我们似乎被LLM包围了，但它在幕后到底在做什么？让我们来一探究竟。

### 1.1 什么是大模型
LLM是人工智能大模型，大多数情况下源自Transformer架构，旨在理解和生成人类语言、代码等。基于大量文本数据的训练后，大模型能够捕捉人类语言的复杂性和细微差别。LLM可以高度准确地、流畅地和有风格地执行各种与语言相关的任务，从简单的文本分类到文本生成。

在医疗保健行业，LLM被用于电子病历（EMR）处理、临床试验匹配和药物发现。在金融领域，LLM被用于欺诈检测、金融新闻的情感分析，甚至设计交易策略。LLM还可应用于客户服务自动化的聊天机器人和虚拟助手。由于其性能优越，使用灵活，基于Transformer的LLM正在成为各种行业和应用中越来越有价值的资产。

**注意**


在上下文中，将使用“理解”一词来表达很多的意思。通常指的是“自然语言理解”（NLU）——NLP的一个研究分支，专注于开发能够准确解释人类语言的算法和模型。正如我们将要看到的，NLU模型擅长分类、情感分析和命名实体识别等任务。然而，值得注意的是，虽然这些模型可以执行复杂的语言任务，但它们并不像人类那样能真正理解。

LLM和Transformer的成功得益于一些想法的结合。注意力、迁移学习和可扩展神经网络是深度学习和人工智能领域的重要技术，这些存在多年并被持续研究的技术为Transformer提供了基本框架，它们在同一时间的突破则催生了Transformer的出现。图1.1概述了过去几十年NLP的一些里程碑事件，这些事件共同促进了Transformer的发明。

图1.1展示了现代NLP简史，包括使用深度学习来解决语言建模问题的发展历程，大规模词嵌入模型（Word2vec），具有注意力的序列到序列模型（我们将在本章后面进行更深入讲解），以及2017年至今的Transformer。

![image](https://github.com/user-attachments/assets/099adeea-2f23-4fbc-83fa-d672c7e2f014)


Transformer架构本身就令人印象深刻。它可以实现以前最先进的NLP模型无法实现的高度并行和可扩展性，相比以前的NLP模型，Transformer可拓展到更大规模的训练数据集，同时还能缩短模型的训练时间。Transformer使用一种称为自注意力的特殊算法，允许序列中的每个词“关注”序列中的所有其他词（上下文），使其能够捕捉大范围内的依赖关系。当然，没有架构是完美的。Transformer仍然局限于一个输入上下文窗口，这代表了其在任何给定时刻可以处理的最大文本长度。

自2017年Transformer架构问世以来，利用该架构或将其部署进系统的软件生态呈爆炸式增长。名为Transformer的库及配套软件包使从业者能够使用、训练和共享模型，大大加快了模型的应用，目前已有数千个组织在使用（并且还在增加）。突然出现的流行LLM库（如Hugging Face）为大众提供了强大的开源模型。简而言之，使用和生产Transformer从未如此简单。

这就是本书的切入点。

本书的目标之一是指导读者如何使用、训练和优化各种LLM，以用于实际应用，同时让读者对模型的内部运作有足够的了解，知道如何在模型选择、数据预处理、微调参数等方面做出最佳决策。

本书的另一个目标是让软件开发人员、数据科学家、分析师和业余爱好者都能使用Transformer。为此，我们从最基础的开始，并由浅入深地了解LLM。

#### 1.1.1 大模型的定义

下面先谈谈使用LLM和Transformer解决的具体NLP任务，这是它们解决大量任务的能力基础。语言建模是NLP的一个子领域，涉及创建基于统计或者深度学习的模型，来预测指定词汇表（一组有限的已知词元）中一系列token（词元）的概率分布。通常有两种语言建模任务：自编码任务和自回归任务（图1.2）。

**注意**

词元是语义意义的最小单位，是通过将句子或文本分解成更小的单位而创建的；它是LLM的基本输入。词元既可以是单词，也可以是“子词”，正如将在本书中看到的那样。一些读者可能熟悉n - gram这个术语，它指的是N个连续的词元。

图1.2中，自编码和自回归语言建模任务都涉及填补缺失的词元，但只有自编码任务允许在缺失词元的两侧看到上下文。

![image](https://github.com/user-attachments/assets/05ec2627-ee48-4f29-8b30-a00972af99f0)


自编码语言模型让模型从已知词汇表中的单词来填充短语的缺失部分。

自回归语言模型要求模型从已知词汇中生成给定短语的下一个最可能的词。

自回归语言模型经过训练，可以根据短语中的前一个词元预测句子中的下一个词元。这些模型对应Transformer模型的解码器部分，其中掩码被应用于完整句子，以便注意力只能看到之前的词元。自回归模型是文本生成的理想选择。GPT就是这类模型的良好示例。

经过训练自编码语言模型，从输入的损坏文本中重建原始句子。这些模型对应于编码器。自编码语言模型创建整个句子的双向表征。它们可以针对各种任务（如文本生成）进行微调，但它们的主要应用是句子分类或词元分类。这种模型的典型例子是BERT。

总而言之，LLM是语言模型，可以是自回归、自编码或两者的组合。现代LLM通常基于Transformer架构（我们将在本书中使用），但也可以基于其他架构。LLM的核心特征是大的参数规模以及大型训练数据集，这些特性使它们能够在几乎不需要微调的情况下，仍能以较高的精度执行复杂的语言任务，如文本生成和分类。

表1.1显示了几个热门的LLM需要的磁盘大小、内存使用情况、参数数量和大致的训练数据的大小。请注意，这些大小是近似值，可能会因具体实现和使用的硬件而异。

**表1.1 热门的大模型比较**

| LLM | 磁盘大小/GB | 内存使用情况/GB | 参数/百万 | 训练数据大小/GB |
| ---- | ---- | ---- | ---- | ---- |
| BERT-Large | 1.3 | 3.3 | 340 | 20 |
| GPT-2 117M | 0.5 | 1.5 | 117 | 40 |
| GPT-2 1.5B | 6 | 16 | 1500 | 40 |
| GPT-3 175B | 700 | 2000 | 175000 | 570 |
| T5-11B | 45 | 40 | 11000 | 750 |
| RoBERTa-Large | 1.5 | 3.5 | 355 | 160 |
| ELECTRA-Large | 1.3 | 3.3 | 335 | 20 |

但规模不是一切。下面来看看LLM的一些关键特征，然后深入了解它们是如何学习读写的。

#### 1.1.2 大模型的关键特征

2017年设计的原始Transformer架构是一个序列到序列模型，有两个主要组件：

- **编码器**：任务是接收原始文本，并拆分为核心组件（稍后详细介绍），将这些组件转换为向量（类似于Word2vec过程），并使用注意力机制来理解文本的上下文。

- **解码器**：擅长通过一种修改过的注意力机制来预测下一个最佳的词元，逐字生成文本。

如图1.3所示，Transformer有许多其他子组件（不做详细讲解），这些子组件可以让模型训练得更快，具有更强的泛化能力和更好的性能。今天的LLM在很大程度上是原始Transformer的变体。像BERT和GPT这样的模型将Transformer分解为（单独的）编码器和解码器，以便在理解和生成方面（也是单独的）构建擅长的模型。

图1.3中原始的Transformer包含两个主要组件：一个擅长理解文本的编码器和一个擅长生成文本的解码器，将它们组合在一起使整个模型成为“序列到序列”模型。

![image](https://github.com/user-attachments/assets/11eaaf26-cd75-40a0-8607-0c2754faa4a2)


如前所述，一般来说，LLM可以主要分为以下三类。

（1）**自回归模型**：如GPT，根据前面的词预测句子中的下一个词。LLM在给定上下文后生成连贯的自由文本方面非常有效。

（2）**自编码模型**：如BERT，通过屏蔽一些输入的词元，并尝试从其余词元中预测被屏蔽的词元来进行双向构建。这些LLM擅长快速且大量地捕捉词元之间的上下文关系，因此成为文本分类任务的理想候选者。 

（3）**自回归和自编码的组合模型**：如T5，可以使用编码器和解码器，在生成文本时更加通用和灵活。与纯基于解码器的自回归模型相比，这种组合模型可以在不同背景下生成更多样化和创造性的文本，因为它们能够使用编码器捕获额外的上下文。

基于这三个类别，图1.4显示了LLM对关键特征的细分内容。

无论LLM是如何构建的，以及它使用的是Transformer的哪些部分，它们都考虑上下文（图1.5）。其目标是理解每个词元，因为它与输入文本中的其他词元相关。自从2013年左右引入Word2vec以来，NLP从业者和研究人员一直热衷于结合语义含义和上下文来构建嵌入的最佳方法。Transformer依靠注意力计算来实现这种结合。

![image](https://github.com/user-attachments/assets/b9751a3f-5668-4b4f-8e13-d379c110f7fd)

![image](https://github.com/user-attachments/assets/d78486a7-f680-496c-af99-c5697de4d08a)


原始序列到序列的Transformer模型
- 可以用来训练并执行自编码自回归语言建模任务。
- 例如T5模型

![image](https://github.com/user-attachments/assets/d41aa4e1-5428-45ec-88b7-7b8dda150573)


![image](https://github.com/user-attachments/assets/abef2b23-f595-42ca-8d9d-ec9a982d159c)


仅解码模型
- 训练并执行自编码语言建模任务。
- 这些模型擅长理解任务。
- 例如BERT系列

仅解码模型
- 训练并执行自回归语言建模任务。
- 这些模型擅长生成任务。
- 例如GPT系列

![image](https://github.com/user-attachments/assets/55c07253-d7e6-48e8-b237-e824f9316f1e)


仅仅选择哪种Transformer变体是不够的。仅仅选择编码器并不意味着Transformer会神奇地变得擅长理解文本。下面来看看这些LLM实际上是如何学习读写的。

图1.5中，LLM擅长理解上下文，Python这个词根据上下文可以有不同的含义，可能指一条蛇或一种非常酷的编程语言。 
