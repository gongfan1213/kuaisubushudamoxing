方法和自定义分隔符方法均可用于有重叠或无重叠的情况。相比于固定词元窗口

方法,一般自定义分隔符方法块的划分是不均匀的。

![image](https://github.com/user-attachments/assets/16378b61-47cc-4789-a468-7fe48e02ae77)


### 图2.7
- **有重叠的最大词元窗口方法**：重要文档被划分为块1、块2、块3、块4、块5，各块间存在重叠文本。
- **没有重叠的自然空白块化**：重要文档按页划分为块1、块2、块3、块4，分别对应页1、页2、页3、页4 ，无重叠。

下面看一下课本类文档常见的空格类型(程序清单2.5)。

### 程序清单2.5：用自然分隔符方法对书进行分块

```python
# 导入Counter和re库
from collections import Counter
import re

# 找到principles_of_ds中的一处或多处空白
matches = re.findall(r'\s{1,}',principles_of_ds)

# 文档中出现频率最高的5种空白
most_common_spaces = Counter(matches).most_common(5)

# 输出这些空白和出现频率
print(most_common_spaces)

#[('', 82259),
# ('\n', 9220),
# (' ', 1592),
# ('\n\n', 333),
# ('\n ', 250)]
```
最常见的自然分隔符是双空格,即代码中的两个换行符,被笔者用来区分页面。这是有道的,因为书中最自然的空白是按页划分的。在其他情况下,也可能在段落之间找到自然的空白,当对源文档比较熟悉和了解时,自然分隔符方法非常实用。

还可以借助更多的机器学习方法,更有创意地来分割文档。


### 3. 使用语义聚类来分割文档

另一种文档分块方法是使用语义聚类来分割文档。这种方法通过组合语义上相似的小块信息来创建新文档(图2.8)。这个算法的设计需要花一些心思,因为对文档块的任何修改都会改变结果向量。例如,可以使用scikit-learn中的agglomerative clustering实例,将相似的句子或段落组合在一起,形成新的文档。

![image](https://github.com/user-attachments/assets/59d2a045-e758-488d-9ea7-1138f99754af)


### 图2.8
- **没有重叠的自然空白块化**：重要文档按页分为块1、块2、块3、块4，对应页1、页2、页3、页4 ，无重叠。
- **利用语义相似性的成组自然分块**：重要文档中，块1对应页1和页3，块2对应页2，块3对应页4 ，根据语义组合。

下面尝试把前面从书中发现的那些块(程序清单2.6)聚在一起。
### 程序清单2.6：根据语义相似度对文档页面进行聚类
```python
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
import numpy ad np

# 假设已经有一个叫作embeddings的文本嵌入列表
# 首先计算所有embeddings对的余弦相似度矩阵
cosine_sim_matrix = cosine_similarity(embeddings)

# 实例化AgglomerativeClustering模型
agg_clustering = AgglomerativeClustering(
    n_clusters = None,  # 基于数据算法将确定块的可选数目
    distance_threshold=0.1,  # 当块之间的所有成对距离都大于0.1,块就形成了
    affinity='precomputed',  # 提供一个预计算的距离矩阵(1 - similarity matrix)作为输入
    linkage = 'precomputed'
)

# 对余弦距离矩阵(1 - similarity matrix)进行适配
agg_clustering.fit(1 - cosine_sim_matrix)

# 对每个嵌入得到块标签
cluster_labels = agg_clustering.labels_

# 打印每块中的嵌入数目
unique_labels,counts = np.unique(cluster_labels, return_counts = True)
for label, count in zip(unique_labels, counts):
    print(f'cluster {label}: {count} embeddings')

#Cluster 0: 2 embeddings
#Cluster 1: 3 embeddings
#Cluster 2: 4 embeddings
#...
```
这种方法倾向于产生在语义上更具凝聚力的块,但会受到内容片段与上下文不相关的影响。当用户开始使用的块关联性不强时,即当块彼此之间更加独立时,这种方法效果很好。

### 4. 使用整个文档,不进行分块
也可以不使用分块,直接使用整个文档。这种方法可能是最简便的选择,但当文档太长,在嵌入文本遇到上下文窗口限制时,这种方法有缺点。用户也可能会被填充无关的、完全不同的上下文的文档所干扰,由此产生的嵌入可能因试图一次性转化太多内容来生成向量嵌入而导致质量下降。对于非常大的(多页)文档,这些缺点会更加严重。

在选择文档嵌入方法时,选择不同的文档分块方法或直接采用整个文档,会最终影响嵌入的效果(表2.1)。当切分方法和嵌入方法确定之后,我们需要为文档块转化后的向量提供存储位置。如果只是在本地部署,可以直接采用矩阵计算的方式提供简易的检索服务。不过本书将介绍如何在云端部署服务,下面将介绍可用的数据库服务。

### 表2.1 不同的文档分块方法及其优缺点

|分块类型|描述|优点|缺点|
| ---- | ---- | ---- | ---- |
|无重叠的最大词元窗口分块|文档被分割成固定大小的块,每个块代表一个单独的文档段|简单且易于实施|可能会切断块之间的上下文,从而导致信息丢失|
|有重叠的最大词元窗口分块|文档被分割成固定大小的、有重叠的块|简单且易于实施|可能会导致不同块之间出现冗余信息|
|依据自然分隔符进行分块|文档中的自然空白用于确定每个块的边界|可以生成与文档中的自然分割相吻合的更有意义的分块|找到正确的分隔符可能会很耗时|
|语义聚类生成文档块|相似的文档块被组合起来,形成更大的语义文档|可以生成更有意义的文档,捕捉文档的整体含义|需要更多的计算资源,并且实现起来可能更复杂|
|不分块,使用整个文档|整个文档被视为一个块|简单且易于实施|可能会受到上下文窗口长度限制的影响,从而产生低质量且无关的上下文,影响嵌入生成的质量|

### 2.4.3 向量数据库
向量数据库是一种数据存储系统,专门用于高性能的向量存储和向量检索。这类数据库对于存储由LLM生成的嵌入向量非常有用,LLM可以很好地对文档或文档块进行编码,形成嵌入向量。通过在向量数据库中存储嵌入,我们可以有效地执行最近邻搜索,根据语义含义检索相似的文本片段。

### 2.4.4 Pinecone
Pinecone是一个向量数据库,专为中小型数据集设计(通常适用于少于100万条)。Pinecone通常是免费使用的,但它也有一个定价方案来提供额外的功能和更大规模的向量存储。Pinecone针对快速向量搜索和检索进行了优化,使其成为低延迟搜索应用程序(如推荐系统、搜索引擎和聊天机器人)的理想选择。

### 2.4.5 开源替代方案
Pinecone的几个开源替代品可用于构建用于LLM嵌入的向量数据库,其中一个替代品是Pgvector,它是PostgreSQL的扩展,增加了向量数据类型的支持,并提供快速的向量操作。另一个选择是Weaviate,它是一个为机器学习应用程序设计的云原生开源向量数据库。Weaviate支持语义搜索,可以与TensorFlow和PyTorch等机器学习工具集成。ANNOY是一个开源的近似最近邻搜索库,针对大规模数据集进行优化,可以用于构建针对特定用例定制的自定义向量数据库。

### 2.4.6 检索结果重排
使用相似度指标(例如余弦相似度)从给定查询的向量数据库中检索潜在结果后,通常需要对它们进行重新排列,以确保向用户呈现最相关的结果。如图2.9所示,交叉编码器接收两段文本,输出一个相似度得分,此处并不用于产生文本的向量表示。双编码器预先将一堆文本进行嵌入向量表示,然后在给定查询(例如,查找“我是一名数据科学家”)的情况下进行实时检索。对结果进行重新排列的一种方法是使用交叉编码器,这是一种Transformer模型,输入是两个成对序列,输出是预测分数,用于计算两个序列的相关性。通过使用交叉编码器对搜索结果进行重新排序,可以考虑整个搜索词的上下文,而不局限于单个关键字。当然,这会增加一些开销和时延,但是有助于提高检索精度。在后面的讲解中,将比较使用和不使用交叉编码器时检索结果的相关性,用来判断是否使用较差编码器。

### 图2.9
- **交叉编码器**：输入“我是一名数据科学家”和“我从事数据科学家的工作” ，输出相似度判断。
- **双编码器**：输入“我是一名数据科学家” ，与“我从事数据科学家的工作”“我是一名生物学家”“我是一名数据分析员”“我是一名化学家”分别计算相似度得分，如[0.19, 0.12, 0.75, 0.025…]等。

![image](https://github.com/user-attachments/assets/267af3fe-4d55-4458-bb95-29ea03446405)


一个流行的跨编码器模型来源是Sentence Transformers库,我们之前就是在这个库中找到了双编码器。还可以在特定任务的数据集上微调预训练的跨编码器模型,以提高搜索结果的相关性,并提供更准确的推荐。

重新对搜索结果进行排序的另一种选择是使用传统的检索模型,如BM25,该模型根据文档中查询术语的频率对结果进行排名,并考虑术语的接近度和逆文档频率。虽然BM25没有考虑整个查询上下文,但它仍然是一种对搜索结果进行重新排序并提高结果整体相关性的有用方法。


### 2.4.7 API
我们现在需要一个地方来放置所有这些组件,以便用户能够快速、安全、轻松地访问文档,为此需要设计被调用的API。

FastAPI是一个快速构建Python API的Web框架。它既容易上手又易于调试,使其成为语义搜索API的最佳选择。FastAPI使用Pydantic数据验证库来验证请求和响应数据；它还使用了高性能的ASGI服务器uvicorn。

设置FastAPI项目很简单,只需要很少的配置。FastAPI使用OpenAPI标准提供的自动文档生成功能,这使得构建API文档和客户端库变得容易。程序清单2.7是该文件的大致内容。

### 程序清单2.7：FastAPI框架代码
```python
import hashlib
import os
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

openai.api_key = os.environ.get('OPENAI_API_KEY', '')
pinecone_key = os.environ.get('PINEACONE_KEY', '')

# 用必要的属性在Pinecone中创建一个索引
def my_hash(s):
    # 返回输入字符串的MD5哈希值作为十六进制字符串
    return hashlib.md5(s.encode()).hexdigest()

class DocumentInputRequest(BaseModel):
    # 为/document/ingest定义输入

class DocumentInputResponse(BaseModel):
    # 从/document/ingest定义输出

class DocumentRetrieveRequest(BaseModel):
    # 为/document/retrieve定义输入

class DocumentRetrieveResponse(BaseModel):
    # 从/document/retrieve定义输出

# 导入文档的API路径
@app.post("/document/ingest",response_model = DocumentInputResponse)
async def document_ingest(request: DocumentInputRequest):
    # 对输入数据进行语义分析并且分块
    # 对每个块创建嵌入文本和元数据
    # 把嵌入文本和元数据插入Pinecone中
    # 返回插入块的数目
    return DocumentInputResponse(chunks_count = num_chunks)

# 返回文档的API路径
@app.post("/document/retrieve",response_model = DocumentRetrieveResponse)
async def document_retrieve(request: DocumentRetrieveRequest):
    # 对查询数据进行语义分析并检索Pinecone中匹配的嵌入

    # 返回文档回复列表
    return DocumentRetrieveResponse(documents = documents)

if __name__ == "__main__":
    uvicorn.run("api:app", host = "0.0.0.0", port = 8000, reload = True)
```

如需完整源码,请查看本书的代码库。


### 2.5 完整方案

现在有了一个适用于所有组件的解决方案。下面来看看解决方案的步骤。粗体字部分是相对于本章之前的文本检索方案新增的内容。

1. **第一部分：嵌入文档**
    - 收集用于嵌入的文档——将所有用于检索引擎的文档分块,使其更易于管理。
    - 创建文本嵌入以编码语义信息——OpenAI的嵌入。
    - 将嵌入向量存储在数据库中,以便在给定搜索词后进行检索——Pinecone。
2. **第二部分：检索文档**
    - 检索系统接收清洗过或预处理过的用户查询信息——FastAPI。
    - 检索候选文档——OpenAI的嵌入+Pinecone。
    - 必要时候选文档进行重新排序——交叉编码器。
    - 返回最终搜索结果——FastAPI。


准备好所有这些部件后,下面来看看图2.10中的最终系统架构。图2.10中使用了两个闭源系统(OpenAI和Pinecone)和开源API框架(FastAPI)来构成完整语义搜索架构。

现在有一个完整的端到端语义检索方案用于语义搜索,下面看看该系统在验证集上的表现如何。

前面已经讲述了语义搜索问题的解决方案,下面测试这些不同组件如何协同工作。为此,使用一个常见的数据集来进行测试：BoolQ数据集——一个用于回答“是/否”问题的问答数据集,包含近16 000个样本。该数据集包含问题和回答二元对,对于给定的问题,回答该段落是否是该问题的最佳段落。

表2.2概述了笔者为本书的代码实现和运行进行的一些试验。笔者使用了嵌入器、重排解决方案和小规模微调的组合,以观察系统在以下两个方面表现如何。

![image](https://github.com/user-attachments/assets/81c3083e-3485-4b79-82c1-b44a6c18aa12)


### 图2.10
- **加入文档**：原始文档使用OpenAI的嵌入产品生成嵌入向量，存入向量数据库Pinecone。
- **用户查询**：用户查询如“密码重置” ，使用与文档语料相同的嵌入器生成查询词嵌入向量，Pinecone获取最相似文档，FastAPI对结果重排后向用户返回相关结果并评分。

### 表2.2 各种组合对BoolQ验证集的性能结果

|嵌入器|重排方法|排名前几的结果准确率|运行时间评估（使用Pinecone）|备注|
| ---- | ---- | ---- | ---- | ---- |
|OpenAI（闭源）|无|0.85229|18min|到目前为止最易运行|
|OpenAI（闭源）|交叉编译器/mmarco-mMini-LMv2-L12-H384-v1(开源)|0.83731|27min|引入重排方法带来了50%的性能损失|
|OpenAI（闭源）|交叉编译器/ms-marco-MiniLM-L-12-v2(开源)|0.84190|27min|表现稍好,但仍不及OpenAI|
|OpenAI（闭源）|交叉编译器/ms-marco-MiniLM-L-12-v2(开源,并且在BoolQ数据集上训练了2轮)|0.84954|27min|表现更好,但仍不及OpenAI|
|Sentence-transformers/multi-qa-mpnet-base-cos-v1(开源)|无|0.85260|16min|勉强击败OpenAI,无须对双编码器进行微调,本地运行也无须调用API,因此运行速度也较快|
|Sentence-transformers/multi-qa-mpnet-base-cos-v1(开源)|交叉编译器/ms-marco-MiniLM-L-12-v2(开源,并且在BoolQ数据集上训练了2轮)|0.84343|25min|微调交叉编码器并没有提升效果|

1. **性能**：如排名前几的回答的准确率。对于BoolQ验证集(3270个样本)中的所问问题回答二元对,我们测试了检索系统返回的最佳结果是否与预期的文档段落相符合。当然,我们也可以使用其他指标,Sentence_Transformers库还包括排名评估、相关性评估等。

2. **延迟**：使用Pinecone运行这些样本需要多长时间。对于每个嵌入器,笔者重置了索引,上传了新的向量,并使用笔记本电脑在内存中交叉编码,以保持简单和标准化。以分钟为单位测量了运行BoolQ数据集验证集的延迟。

笔者没有尝试的一些试验内容如下。

1. 微调交叉编码器来训练更多轮次,并花费更多时间寻找最优学习参数(如权重衰减、学习率调度器)。

2. 使用其他的OpenAI嵌入引擎。

3. 在训练集上微调开源双编码器。


请注意,笔者用于交叉编码器和双向编码器的模型都是以类似于非对称语义搜索的方式在数据上预先训练过的。这很重要,因为我们希望嵌入器为短文本查询和长文档分别生成嵌入向量,并当它们内容相关时,嵌入向量会在同一空间靠近。

如果从简单的角度考虑,那么我们只使用OpenAI嵌入器,而且在应用程序中不进行重新排序(第1行)。我们现在应该考虑使用FastAPI、Pinecone和OpenAI进行文本嵌入的相关成本。



### 2.6 闭源组件的成本
本方案使用了多个组件,但并非所有组件都是免费的。幸运的是,Fast

API是一个开源框架,不需要任何许可费用。使用FastAPI的成本是在托管部署的服务器上,而生成服务的代码是免费的。笔者更喜欢Render,它有一个免费版本,但也有一个从7美元/月的起步版本,保证100%正常运行时间的收费版本。在撰写本文时,Pinecone提供一个免费版本,限制为100 000个嵌入和最多3个索引；超过这个规模,依据使用的嵌入和索引的数量收费。Pinecone的标准收费为每月49美元,最多可提供100万个嵌入和10个索引。 

OpenAI提供其文本嵌入引擎(Ada - 002)的免费版本,但每月限制为100 000个请求。但OpenAI对用户使用的嵌入服务,每1000个词元收取0.0004美元。假设每份文件平均有500个词元,那么每份文件的成本将是0.0002美元。例如,如果想嵌入100万份文件,那么大约需要200美元。 

如果想构建一个包含100万个嵌入的系统,并且希望每月使用全新的嵌入更新一次索引,那么每月的成本是如下： 
- Pinecone成本 = 49美元 
- OpenAI成本 = 200美元 
- FastAPI成本 = 7美元 
- 总费用 = 49美元 + 200美元 + 7美元 = 每月256美元 

随着系统的扩展,这些成本会迅速增加。研究开源替代品或其他降低成本的策略也是很有意义的,例如使用开源双编码器进行嵌入或使用Pgvector作为向量数据库。 

### 2.7 本章小结 
如果考虑每个组件的开销,检索系统的成本将会增加很多,但其实每一步都有替代方案,这部分就留给读者来完成。在阅读完本书所附的代码库中检索引擎的完整代码和部署说明后,其中代码包含一个可以运行的FastAPI的应用程序,读者可以尝试自己新的检索引擎。读者可以多多尝试本章的检索引擎方案,并用在自己特定领域的数据上。 

第3章将在此API的基础上,构建基于GPT - 4的聊天机器人和检索系统。 
