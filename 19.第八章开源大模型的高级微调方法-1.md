### 第8章 开源大模型的高级微调方法

在特定的任务上，较小的开源模型通过适当的数据微调也能达到和GPT - 4一样好的效果。相对于采用API来调用闭源大模型，微调模型具有一定的优势。虽然直接采用API来调用闭源大模型拥有不可忽视的优势，但是闭源模型并不是总能产生符合用户预期的结果，此时需要采用用户的私有数据进行微调来弥补这一缺陷。

本章旨在帮助读者挖掘开源大模型的巨大潜力，通过采用本章中介绍的技术和策略，读者将能够根据自己的特定需求对这些开源模型进行改进，以获得与闭源大模型相媲美的效果。

作为一名机器学习工程师，笔者认为微调的魅力在于它的灵活性和适应性，用户能够根据个性化的需求来定制大模型。无论用户的目标是开发一个复杂的聊天机器人、一个简单的分类器，还是一个可以生成创造性内容的工具，微调都能够确保模型与项目的目标保持一致。

对于本章的学习，需要保持严谨的态度、创造力、解决问题的能力以及对机器学习基本原理的透彻理解。但请放心，付出的努力是值得的。让我们开始本章的学习吧。


#### 8.1 案例研究：采用BERT对动漫进行多标签分类
本章将沿用第6章中的动漫数据集来构建一个类型预测的引擎。在第6章中，使用生成的描述作为动漫标题的基本特征来构建推荐引擎，其中采用的特征之一是动漫的类型列表。假设新的目标是帮助人们在给定其他特征的情况下标记动漫的流派列表。如图8.1所示，有42个独特的类型。在多标签动漫类型分类任务中，有42个类型需要分类。

##### 8.1.1 采用Jaccard相似分来评估动漫标题多标签分类的效果

这里采用Jaccard评分作为评估类型预测模型的效果指标，这是一种衡量样本集之间相似性的指标。Jaccard评分适用于多标签流派预测任务，以评估模型在预测每个动漫标题的正确类型方面的准确性。

程序清单8.1展示了Trainer中如何自定义指标函数的方式，此处给出四种常用指标的定义。

![image](https://github.com/user-attachments/assets/4be8a0d9-ad0f-4c24-8c93-1b811b87a75e)


- **Jaccard相似分**：类似于在第6章中使用的Jaccard分数，它可以衡量此案例中样本集的相似性和多样性。一般而言，较高的Jaccard分数表明模型的预测与实际标签更接近。
- **F1分数**：F1分数是衡量模型在数据集上的准确度的指标。常应用于评估二元分类系统，该系统将案例的分类分为“正”或“负”。F1分数是精确度和召回率的调和平均；取1时达到最佳，取0时达到最差。 
- **ROC/AUC**：受试者工作特征（ROC）是一种概率曲线；曲线下的面积（AUC）表示可分性的程度或度量。AUC表明模型区分类别的程度：AUC越高，模型在预测0为假和1为真方面的分辨度越好。 
- **准确率**：准确率量化预测标签与真实标签完全匹配的概率。虽然很容易解释，但这个指标对于分布不平衡的数据集可能会产生误导，在这种情况下，模型可以通过只预测主要类别来提高准确率。

**程序清单8.1：为多标签流派预测自定义指标**

```python
# 定义函数计算几个多标签指标
def multi_label_metrics(predictions, labels, threshold=0.5):
    # 初始化sigmoid函数，用来转化原始预测数值
    sigmoid = torch.nn.sigmoid()
    # 为预测应用sigmoid函数
    probs = sigmoid(torch.Tensor(predictions))
    # 基于门限值创建一个二进制预测数组
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= threshold)] = 1
    # 使用实际的标签作为y_true
    y_true = labels
    # 计算F1分、ROC/AUC、准确率和Jaccard相似分
    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
    roc_auc = roc_auc_score(y_true, y_pred, average='micro')
    accuracy = accuracy_score(y_true, y_pred)
    jaccard = jaccard_score(y_true, y_pred)
    # 把得分打包进字典并返回
    metrics = {
        'f1': f1_micro_average,
        'roc_auc': roc_auc,
        'accuracy': accuracy,
        'jaccard': jaccard
    }
    return metrics
# 为预测定义一个计算指标的函数
def compute_metrics(p: EvalPrediction):
    # 从EvalPrediction对象中提取预测值
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    # 为预测和实际标签计算多标签指标
    result = multi_label_metrics(predictions=preds, labels=p.label_ids)
    # 返回结果
    return result
```

##### 8.1.2 简单的微调模型训练流程
为了实现模型的微调，将设置多个组件，每个组件在个性化微调过程中都起着至关重要的作用。
- **数据集**：这里沿用第6章采用的MyAnimeList数据集的训练集和测试集。该数据集是整个微调过程的基础，因为它包含模型学习预测的输入数据（剧情简介）和目标标签（类型）。将数据集合理地划分为训练集和测试集，这对于评估微调模型在未见数据上的效果非常重要。 
- **数据整理器**：数据整理器负责处理和准备模型的输入数据。它接受原始输入数据，如文本，通过标记化（Tokenization）、填充（Padding）和批处理（Batching），将输入数据转换为模型可以理解的格式。通过使用数据整理器，确保输入数据格式的正确性，并在训练期间高效地传入训练模型。 
- **训练参数**：训练参数是Hugging Face提供的一个可配置对象，它允许为训练过程指定各种超参数。这些参数包括学习率、批大小、训练轮数等。通过设置训练参数，可以调整训练过程，以实现特定任务的最佳性能。 
- **权重和偏差以及训练器**：权重和偏差（WandB）是一个库，可以促进训练过程的跟踪和可视化。通过集成WandB，可以监控损失和准确率等关键指标，并了解模型随时间推移的表现。训练器是Hugging Face提供的一个管理微调过程的实用工具。它处理诸如加载数据、更新模型权重和评估模型性能等任务。通过设置训练器，可以简化微调过程，并确保模型有效地开展训练任务。

图8.2展示了使用Hugging Face内置微调组件的基本深度学习训练循环。

![image](https://github.com/user-attachments/assets/1d77a3d4-26f8-490d-acce-325befaa7773)


##### 8.1.3 通用的开源大模型微调技巧

本节重点介绍一些适用于普适任务的LLM微调技巧。

1. **数据准备和特征工程**

在机器学习中，数据准备和特征工程非常重要。就大模型微调而言，可从原始特征构造新的复合特征。例如，第6章创建了“生成的描述”特征，包含动漫剧情简介、类型、制作人信息等，为模型提供背景。本例将创建类似描述，但不含类型，因在输入中包含类型会让类型预测任务作弊。

回顾数据去重重要性。尽管本例数据集无重复动漫，但语义层面仍可考虑去重。基于相同素材或情节的动漫可能使模型混淆。程序清单8.2定义了一个简单函数，使用双编码器对动漫描述进行编码，并删除与其他动漫在语义上过于相似的动漫（这里的相似是指余弦相似度）。


**程序清单8.2：使用双编码器对语料库进行语义去重**
```python
# 导入必要的库
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
# 初始化模型，通过编码使语义上相似的文本彼此邻近
# 'paraphrase - distilroberta - base - v1'是一个语义文本相似度预训练模型
downsample_model = SentenceTransformer('paraphrase - distilroberta - base - v1')
def filter_semantically_similar_texts(texts, similarity_threshold=0.8):
    # 为所有文本产生嵌入，这些嵌入是数字化的representations of the text that encode meaning to a high - dimensional space
    embeddings = downsample_model.encode(texts)
    # 所有文本嵌入对之间的余弦相似度结果是一个矩阵，矩阵中是对应行列的嵌入文本
    similarity_matrix = cosine_similarity(embeddings)
    # 将余弦相似度矩阵的对角线元素设为0，因为这些元素代表每个文本与自己的相似度，值总是1
    np.fill_diagonal(similarity_matrix, 0)
    # 初始化一个空列表，用来存储不太相似的文本
    filtered_texts = []
    # 存储特别相似的文本指标集
    excluded_indices = set()
    for i, text in enumerate(texts):
        # 如果当前文本与其他文本均不相似
        if i not in excluded_indices:
            # 将其加入不太相似文本列表
            filtered_texts.append(text)
            # 找到与当前文本非常相似的文本索引
            similar_texts_indices = np.where(similarity_matrix[i] > similarity_threshold)[0]
            # 提取这些文本
            excluded_indices.update(similar_texts_indices)
    return filtered_texts
# 测试函数的样例文本列表
texts = [
    "This is a sample text.",
    "This is another sample text.",
    "This is a similar text.",
    "This is a completely different text.",
    "This text is quite alike."
]
# 使用函数过滤语义相似文本
filtered_texts = filter_semantically_similar_texts(texts, similarity_threshold=0.9)
# 打印传给语义相似过滤器的文本
filtered_texts == [
    'This is a sample text.',
    'This is a similar text.',
    'This is a completely different text.', 'This text is quite alike.'
]
```
需注意，此过程有丢失高价值信息风险。一部动漫语义相似不意味着类型相同，虽不是微调主要障碍，但仍需留意。这里采用的语义相似性去重方法是数据预处理一部分，用于删除相似文档的阈值（程序清单8.2中的similarity_threshold变量）可视为超参数，如训练轮数或学习率。 

2. **调整批大小和梯度累积**

寻找最优批大小对保障微调过程中模型内存占用与运行稳定性很重要。更大批大小意味着训练期间模型能处理更多数据样本，可提供更准确梯度估计，但需更多计算资源。

当内存受限，梯度累积是好方案。它允许将较大批处理拆分为多个反向传递来训练，减少每次传递所需内存，能用更少内存、更稳定梯度训练。 

3. **动态填充**

动态填充技术可在处理大量可变长度序列（如文本数据）时大幅降低计算资源浪费。传统等长填充常将序列填充到数据集中最长序列长度，若序列长度差异大，会浪费大量计算资源。

动态填充独立调整每批填充量，平均使用较少填充，使计算更高效。

执行动态填充可使用Transformers包中的DataCollatorWithPadding。程序清单8.3展示了更改后使用DataCollatorWithPadding的快速示例代码，本书代码库提供完整示例。

图8.3中，深灰色为实际词元；浅灰色为填充词元。均匀填充（顶部）将数据集中所有序列填充到最长序列长度，计算低效。动态填充（底部）将每个批次的序列填充到该批中最长序列长度。

![image](https://github.com/user-attachments/assets/296edacf-c720-4929-ac5d-65667ff85003)


**程序清单8.3：使用DataCollatorWithPadding执行动态填充**
```python
# 导入DataCollatorWithPadding包
from transformers import DataCollatorWithPadding
model = AutoModelForSequenceClassification.from_pretrained(
   ... # 导入预训练模型参数来做模型参数初始化，例如采用BERT来初始化GPT - 2
)
# 确认好分词器和填充方式后，即可定义collator模块
# 最大填充是默认的填充方式，将每一个批次的句子序列都填充到最大长度
# 将数据集中的文本进行分词（暂不做填充），方便在训练或测试时collator模块能够动态填充
# 假设在处理过程中，已经拥有原始数据的训练数据和测试数据
train = raw_train.map(lambda x: tokenizer(x["text"], truncation=True, batched=True))
test = raw_test.map(lambda x: tokenizer(x["text"], truncation=True, batched=True))
collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")
trainer = Trainer(
    model=model,
    train_dataset=train,
    eval_dataset=test,
    tokenizer=tokenizer,
    args=training_args,
    data_collator=collate_fn,  # 设置采集器（默认设置，使用标准的无填充方式）
)
... # 余下是模型训练代码
```

动态填充是多数模型训练流程中，显著减少内存使用、提升训练速度的有效手段。 

4. **混合精度训练**

混合精度训练可显著提高模型训练效率，尤其在GPU上训练时。新一代GPU中，较低精度（16位浮点格式，FP16）比标准32位格式（FP32）执行某些操作更快。

混合精度训练利用FP16和FP32优势，FP16用于提高计算速度，权重以FP32格式存储保证精度，避免下溢和上溢等数值问题。

并非所有GPU上FP16计算都更快，此方法特别适合有TensorCore的GPU，它们能按FP16精度更快执行操作。 

5. **结合PyTorch 2.0**

PyTorch更新引入训练模型的内置优化，可编译模型提升生产力。可调用torch.compile(model)编译模型，相关示例在本书代码库。但PyTorch 2.0编译在系统编译环境有局限，如Windows系统、Python 3.11环境及无GPU时不适用。即便不用PyTorch 2.0，也需关注训练管道变化对模型训练时间和内存使用的影响。

图8.4展示了使用BERT作为基础模型训练简单分类任务时采用的优化手段与训练/内存权衡图表。

寻找训练参数最佳组合不易，需多次迭代，可能训练失败几次才能找到。图中最后柱状图采用四种技术，显著降低速度、减少内存使用，通常表示达到参数最佳组合。 


![image](https://github.com/user-attachments/assets/7307becb-0763-40e0-ac4f-88df55a4fc1f)


6. **模型冻结**

加速微调预训练模型常用方法是冻结模型权重，训练时预训练模型参数或权重不变，防止被迭代更新，保留预学习特征。

模型冻结原理源于深度学习模型学习表示方式。模型较低层（初始嵌入附近）学习一般特征（如图像分类的边缘轮廓、自然语言处理的低级别单词语义），较高层（如注意力机制末尾网络层）学习复杂、特定任务特征。冻结较低层权重，保留一般特征，只需微调特定任务的较高层。

如将BERT模型用于下游任务时，可冻结部分或全部层，保留对通用语言理解能力，只训练特定任务的少数层。比如冻结BERT模型中最后三层之前的所有权重，下游任务训练阶段只更新最后三层，其他层权重不变。处理小数据集时，该技术可降低过拟合风险，还能减少计算资源需求，加快模型训练。

冻结模型权重时，最好冻结模型开始附近的较低层网络权重。图8.5展示了一个只有六个编码层的模型，选项1（顶部）不冻结任何内容；选项2（中间）部分冻结一些较低权重；选项3（底部）冻结除额外添加的附加层外的整个模型。 



![image](https://github.com/user-attachments/assets/623c5a64-95d7-4a1e-b0ed-4f86597c98c5)


![image](https://github.com/user-attachments/assets/90068cfd-2fee-4e7a-9306-9870d4c59f73)


![image](https://github.com/user-attachments/assets/3475030f-7948-495d-ab76-9819b859fde2)

